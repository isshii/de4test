{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "DeepXplore_kaggle_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshii/de4test/blob/master/DeepXplore/DeepXplore_kaggle_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVFjZpUIRF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 共通で使うパスなどの定義\n",
        "# 共通の変数設定\n",
        "# 共通フォルダパス\n",
        "data_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data\"\n",
        "data_imagenet = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet\"\n",
        "data_imagenet_seeds = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet/seeds\"\n",
        "model_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/model\"\n",
        "output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output\"\n",
        "tmp_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/tmp\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4lP4POGn5Xj",
        "colab_type": "code",
        "outputId": "8a748363-29cf-4a57-9167-870a8ed825f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Goggle Drive つなぐ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3d3Suv3Q5Db",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50ec5f37-fe5d-434c-c834-2630040c15e2"
      },
      "source": [
        "# outputフォルダ内容物のクリーンアップ削除\n",
        "!rm \"$output_dir\"/*"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/ColabNotebooks/test4ai/output/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDY8sDVBqxmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.layers import Input\n",
        "import imageio\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiGGwQVElJNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tensorflow.python.client import device_lib\n",
        "# device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQ2M_8glJNM",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "c28de40a-3779-4256-afcd-372d0a2373f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "# DeepXplore のパラメータ設定部\n",
        "# read the parameter\n",
        "# argument parsing\n",
        "parser = argparse.ArgumentParser(description='Main function for difference-inducing input generation in MNIST dataset')\n",
        "parser.add_argument('transformation', help=\"realistic transformation type\", choices=['light', 'occl', 'blackout'])\n",
        "parser.add_argument('weight_diff', help=\"weight hyperparm to control differential behavior\", type=float)\n",
        "parser.add_argument('weight_nc', help=\"weight hyperparm to control neuron coverage\", type=float)\n",
        "parser.add_argument('step', help=\"step size of gradient descent\", type=float)\n",
        "parser.add_argument('seeds', help=\"number of seeds of input\", type=int)\n",
        "parser.add_argument('grad_iterations', help=\"number of iterations of gradient descent\", type=int)\n",
        "parser.add_argument('threshold', help=\"threshold for determining neuron activated\", type=float)\n",
        "parser.add_argument('-t', '--target_model', help=\"target model that we want it predicts differently\",\n",
        "                    choices=[0, 1, 2], default=0, type=int)\n",
        "parser.add_argument('-sp', '--start_point', help=\"occlusion upper left corner coordinate\", default=(0, 0), type=tuple)\n",
        "parser.add_argument('-occl_size', '--occlusion_size', help=\"occlusion size\", default=(10, 10), type=tuple)\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-occl_size', '--occlusion_size'], dest='occlusion_size', nargs=None, const=None, default=(10, 10), type=<class 'tuple'>, choices=None, help='occlusion size', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Csx_IXrMdMj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93KW-VGjMzun",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title ノイズのタイプ light / occl / blackout\n",
        "#@body {light,occl,blackout} weight_diff weight_nc step seeds\n",
        "#                   grad_iterations threshold\n",
        "noise_type = \"light\" #@param [\"light\", \"occl\", \"blackout\"]\n",
        "weight_diff = \"0.1\" #@param {type:\"string\"}\n",
        "weight_nc = \"0.1\" #@param {type:\"string\"}\n",
        "step = \"1\" #@param {type:\"string\"}\n",
        "seeds = \"10\" #@param {type:\"string\"}\n",
        "grad_iterations = \"100\" #@param {type:\"string\"}\n",
        "threshold = \"0.1\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk80fCuPts9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = parser.parse_args([noise_type, weight_diff, weight_nc, step, seeds, grad_iterations, threshold])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tTYrllplJNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "import os, re\n",
        "\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    input_img_data = image.img_to_array(img)\n",
        "    input_img_data = np.expand_dims(input_img_data, axis=0)\n",
        "    input_img_data = preprocess_input(input_img_data)  # final input shape = (1,224,224,3)\n",
        "    return input_img_data\n",
        "\n",
        "def preprocess_image2(img_path):\n",
        "    img = image.load_img(img_path, target_size=(128, 128))\n",
        "    input_img_data = image.img_to_array(img)\n",
        "    input_img_data = np.expand_dims(input_img_data, axis=0)\n",
        "    input_img_data = preprocess_input(input_img_data)  # final input shape = (1,224,224,3)\n",
        "    return input_img_data\n",
        "\n",
        "\n",
        "def deprocess_image(x):\n",
        "    x = x.reshape((224, 224, 3))\n",
        "    # Remove zero-center by mean pixel\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    # 'BGR'->'RGB'\n",
        "    x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "\n",
        "def decode_label(pred):\n",
        "    return decode_predictions(pred)[0][0][1]\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "def constraint_occl(gradients, start_point, rect_shape):\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def constraint_light(gradients):\n",
        "    new_grads = np.ones_like(gradients)\n",
        "    grad_mean = 1e4 * np.mean(gradients)\n",
        "    return grad_mean * new_grads\n",
        "\n",
        "\n",
        "def constraint_black(gradients, rect_shape=(10, 10)):\n",
        "    start_point = (\n",
        "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    if np.mean(patch) < 0:\n",
        "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def init_coverage_tables(model1, model2, model3):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    model_layer_dict2 = defaultdict(bool)\n",
        "    model_layer_dict3 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    init_dict(model2, model_layer_dict2)\n",
        "    init_dict(model3, model_layer_dict3)\n",
        "    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n",
        "\n",
        "\n",
        "def init_dict(model, model_layer_dict):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_dict[(layer.name, index)] = False\n",
        "\n",
        "\n",
        "def neuron_to_cover(model_layer_dict):\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in list(model_layer_dict.items()) if not v]\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "    else:\n",
        "        layer_name, index = random.choice(list(model_layer_dict.keys()))\n",
        "    return layer_name, index\n",
        "\n",
        "\n",
        "def neuron_covered(model_layer_dict):\n",
        "    covered_neurons = len([v for v in list(model_layer_dict.values()) if v])\n",
        "    total_neurons = len(model_layer_dict)\n",
        "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n",
        "\n",
        "\n",
        "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
        "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
        "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
        "    X_scaled = X_std * (rmax - rmin) + rmin\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def update_coverage(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n",
        "                model_layer_dict[(layer_names[i], num_neuron)] = True\n",
        "    print(\"update coverage finished\")\n",
        "\n",
        "\n",
        "def full_coverage(model_layer_dict):\n",
        "    if False in list(model_layer_dict.values()):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def fired(model, layer_name, index, input_data, threshold=0):\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
        "    scaled = scale(intermediate_layer_output)\n",
        "    if np.mean(scaled[..., index]) > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def diverged(predictions1, predictions2, predictions3, target):\n",
        "    if not predictions1 == predictions2 == predictions3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "#add\n",
        "def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n",
        "    return [os.path.join(root, f)\n",
        "            for root, _, files in os.walk(directory) for f in files\n",
        "            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42vpA36BB_7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########以下、変更"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjFtoxqvB__1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras.models import model_from_json\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg19 import VGG19"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgPdVk7yCtot",
        "colab_type": "code",
        "outputId": "a517b249-2508-4e49-ed3b-01a6bd5b3976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# モデル構築\n",
        "# 参照元 : https://www.kaggle.com/rohit1277/cat-dog-classifier-using-vgg16-transfer-learning\n",
        "# model名 : good_vgg16.h5\n",
        "# Dropbox保存先 : https://www.dropbox.com/home/TOPSE/model\n",
        "# Gdrive保存先 : /content/gdrive/My Drive/ColabNotebooks/test4ai/model\n",
        "\n",
        "img_rows, img_cols = 224, 224\n",
        "input_shape = (img_rows, img_cols, 3)\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "model1 = VGG16(weights='imagenet',input_tensor=input_tensor,include_top=True)\n",
        "\n",
        "for layers in (vggmodel1.layers)[:19]:\n",
        "    print(layers)\n",
        "    layers.trainable = False\n",
        "X= vggmodel1.layers[-2].output\n",
        "predictions1 = Dense(2, activation=\"softmax\")(X)\n",
        "model1 = Model(input = vggmodel1.input, output = predictions1)\n",
        "model1.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
        "# model1.summary()\n",
        "\n",
        "\n",
        "vgg16_weights = model_dir + '/ImageNet/good_vgg16.h5'\n",
        "model1.load_weights(vgg16_weights)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.engine.input_layer.InputLayer object at 0x7fe2f1d855c0>\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe2f1d85588>\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe2f1d85898>\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fe2f1d88748>\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe2f1d0bf60>\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe2f1d10f60>\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fe2f1d17dd8>\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe2f1d25d30>\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe2f1d2c588>\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe2f1d322e8>\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fe2f1d39da0>\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe2f1d44c18>\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe2f1ccc470>\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe2f1cd21d0>\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fe2f1cd8c88>\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe2f1ce3b38>\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe2f1cee390>\n",
            "<keras.layers.convolutional.Conv2D object at 0x7fe2f1cf1208>\n",
            "<keras.layers.pooling.MaxPooling2D object at 0x7fe2f1cf8ba8>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdSBSbnYG53_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "4371496c-6aeb-4e41-9737-1b05ae8e52a5"
      },
      "source": [
        "# モデル構築\n",
        "# 参照元 : https://www.kaggle.com/jmourad100/dogsvscats-transfer-learning-with-resnet50\n",
        "# input_shape が VGG と異なる可能性があるため注意（summary参照）\n",
        "# model名 : BestModel_from_jmourad100.hdf5\n",
        "# Dropbox保存先 : https://www.dropbox.com/home/TOPSE/model\n",
        "# Gdrive保存先 : /content/gdrive/My Drive/ColabNotebooks/test4ai/model\n",
        "%%time\n",
        "NUM_CLASSES = 2\n",
        "RESNET_WEIGHTS_PATH = model_dir + '/ImageNet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(ResNet50(include_top = False,\n",
        "                   pooling = 'max',\n",
        "                   weights = RESNET_WEIGHTS_PATH))\n",
        "model2.add(Dense(NUM_CLASSES, activation= 'softmax'))\n",
        "model2.layers[0].trainable = False\n",
        "\n",
        "# Compile the Transfer Learning Model\n",
        "sgd = optimizers.SGD(lr = 1e-3, decay = 1e-6, momentum = 0.1, nesterov=True)\n",
        "model2.compile(optimizer= sgd, loss = 'categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "model2.summary()\n",
        "\n",
        "# モデルの重みをロードする\n",
        "resnet_weights = model_dir + '/ImageNet/BestModel_from_jmourad100.hdf5'\n",
        "model2.load_weights(resnet_weights)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50 (Model)             (None, 2048)              23587712  \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 2)                 4098      \n",
            "=================================================================\n",
            "Total params: 23,591,810\n",
            "Trainable params: 4,098\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n",
            "CPU times: user 35.4 s, sys: 696 ms, total: 36.1 s\n",
            "Wall time: 36.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLjC_ConH8vh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "ce7b6af4-a76f-4f72-e565-bc15663e48f8"
      },
      "source": [
        "# モデル構築\n",
        "# 参照元 : https://www.kaggle.com/sajaldeb25/dog-and-cat-by-cnn-karnel-2/output\n",
        "# input_shape が VGG と異なる可能性があるため注意（summary参照）\n",
        "# model名 : model_from_sajaldeb25.h5\n",
        "# Dropbox保存先 : https://www.dropbox.com/home/TOPSE/model\n",
        "# Gdrive保存先 : /content/gdrive/My Drive/ColabNotebooks/test4ai/model\n",
        "%%time\n",
        "\n",
        "IMAGE_WIDTH=128\n",
        "IMAGE_HEIGHT=128\n",
        "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "IMAGE_CHANNELS=3\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
        "\n",
        "model3 = Sequential()\n",
        "\n",
        "model3.add(Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model3.add(Dropout(0.25))\n",
        "\n",
        "model3.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model3.add(Dropout(0.25))\n",
        "\n",
        "model3.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model3.add(Dropout(0.25))\n",
        "\n",
        "model3.add(Flatten())\n",
        "model3.add(Dense(512, activation='relu'))\n",
        "model3.add(BatchNormalization())\n",
        "model3.add(Dropout(0.5))\n",
        "model3.add(Dense(2, activation='softmax')) # 2 because we have cat and dog classes\n",
        "\n",
        "model3.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "model3.summary()\n",
        "\n",
        "\n",
        "#モデル変えます\n",
        "kaggle_weights = model_dir + '/ImageNet/model_from_sajaldeb25.h5'\n",
        "model3.load_weights(kaggle_weights)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 126, 126, 32)      896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 126, 126, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 63, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 63, 63, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 61, 61, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 61, 61, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 28, 28, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 28, 28, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 512)               12845568  \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 12,942,786\n",
            "Trainable params: 12,941,314\n",
            "Non-trainable params: 1,472\n",
            "_________________________________________________________________\n",
            "CPU times: user 11 s, sys: 251 ms, total: 11.3 s\n",
            "Wall time: 11.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b-WPGHyJERg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#摂動を加える犬猫画像のパスに変えたいです。\n",
        "img_paths = list_pictures(data_imagenet_seeds+\"/\")\n",
        "\n",
        "K.set_learning_phase(0)\n",
        "model_layer_dict1, model_layer_dict2, model_layer_dict3 = init_coverage_tables(model1, model2, model3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOuYV7OwB_-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55367980-c3e5-4e63-cb8c-e649a94f285e"
      },
      "source": [
        "%%time\n",
        "\n",
        "for _ in range(args.seeds):\n",
        "    gen_img = preprocess_image(random.choice(img_paths))\n",
        "    gen_img2 = preprocess_image2(random.choice(img_paths))\n",
        "    orig_img = gen_img.copy()\n",
        "    # first check if input already induces differences\n",
        "    pred1, pred2, pred3 = model1.predict(gen_img), model2.predict(gen_img), model3.predict(gen_img2)\n",
        "    label1, label2, label3 = np.argmax(pred1[0]), np.argmax(pred2[0]), np.argmax(pred3[0])\n",
        "    if not label1 == label2 == label3:\n",
        "      ######################## 摂動を加えなくても予測が3つのモデルで異なる場合の処理 Start ########################\n",
        "      print('input already causes different outputs: {}, {}, {}'.format(label1, label2 ,label3) + bcolors.ENDC) \n",
        "\n",
        "      update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "      print(\"updatting coverage for model1\")\n",
        "      update_coverage(gen_img2, model3, model_layer_dict3, args.threshold)\n",
        "      print(\"updatting coverage for model3\")\n",
        "      update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "      print(\"updatting coverage for model2\")\n",
        "\n",
        "      print(bcolors.OKGREEN + 'covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'\n",
        "      #        print('covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'\n",
        "            % (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "      #                 neuron_covered(model_layer_dict3)[2]))\n",
        "\n",
        "      averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                      neuron_covered(model_layer_dict3)[0]) / float(\n",
        "          neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "          neuron_covered(model_layer_dict3)[\n",
        "              1])\n",
        "      print(bcolors.OKGREEN + 'averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n",
        "      print('averaged covered neurons %.3f' % averaged_nc)\n",
        "\n",
        "      gen_img_deprocessed = deprocess_image(gen_img)\n",
        "\n",
        "      # save the result to disk\n",
        "      #imsave('./generated_inputs/' + 'already_differ_' + decode_label(pred1) + '_' + decode_label(\n",
        "      imageio.imwrite('./gdrive/My Drive/deepxplore/ImageNet/generated_inputs/' + 'already_differ_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png',\n",
        "              gen_img_deprocessed)\n",
        "      continue\n",
        "      ######################## 摂動を加えなくても予測が3つのモデルで異なる場合の処理END ########################\n",
        "\n",
        "    ######################## 予測が3つのモデルで同じ場合の処理 Start ########################\n",
        "    # if all label agrees\n",
        "    orig_label = label1\n",
        "    layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n",
        "    layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n",
        "    layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n",
        "    # construct joint loss function\n",
        "    if args.target_model == 0:\n",
        "        loss1 = -args.weight_diff * K.mean(model1.get_layer('dense_7').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('dense_8').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('dense_9').output[..., orig_label])\n",
        "    elif args.target_model == 1:\n",
        "        loss1 = K.mean(model1.get_layer('dense_7').output[..., orig_label])\n",
        "        loss2 = -args.weight_diff * K.mean(model2.get_layer('dense_8').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('dense_9').output[..., orig_label])\n",
        "    elif args.target_model == 2:\n",
        "        loss1 = K.mean(model1.get_layer('dense_7').output[..., label1])\n",
        "        loss2 = K.mean(model2.get_layer('dense_8').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('dense_9').output[..., orig_label])\n",
        "    loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n",
        "    loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n",
        "    loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n",
        "    layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron)    \n",
        "\n",
        "    # for adversarial image generation\n",
        "    final_loss = K.mean(layer_output)\n",
        "\n",
        "    # we compute the gradient of the input picture wrt this loss\n",
        "    grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "\n",
        "    # this function returns the loss and grads given the input picture\n",
        "    iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n",
        "\n",
        "    # we run gradient ascent for 20 steps\n",
        "    for iters in range(args.grad_iterations):\n",
        "        loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = iterate(\n",
        "            [gen_img])\n",
        "        if args.transformation == 'light':\n",
        "            grads_value = constraint_light(grads_value)  # constraint the gradients value\n",
        "        elif args.transformation == 'occl':\n",
        "            grads_value = constraint_occl(grads_value, args.start_point,\n",
        "                                          args.occlusion_size)  # constraint the gradients value\n",
        "        elif args.transformation == 'blackout':\n",
        "            grads_value = constraint_black(grads_value)  # constraint the gradients value\n",
        "\n",
        "        gen_img += grads_value * args.step\n",
        "        pred1, pred2, pred3 = model1.predict(gen_img), model2.predict(gen_img), model3.predict(gen_img)\n",
        "        label1, label2, label3 = np.argmax(pred1[0]), np.argmax(pred2[0]), np.argmax(pred3[0])\n",
        "\n",
        "        if not label1 == label2 == label3:\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "            print(bcolors.OKGREEN + 'covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'\n",
        "                  % (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                      neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                      neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                            neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[\n",
        "                    1])\n",
        "            print(bcolors.OKGREEN + 'averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n",
        "            print('averaged covered neurons %.3f' % averaged_nc)\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #imsave('./generated_inputs/' + args.transformation + '_' + decode_label(pred1) + '_' + decode_label(\n",
        "            imageio.imwrite('./gdrive/My Drive/deepxplore/ImageNet/generated_inputs/' + args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png', gen_img_deprocessed)\n",
        "            #imsave('./generated_inputs/' + args.transformation + '_' + decode_label(pred1) + '_' + decode_label(\n",
        "            imageio.imwrite('./gdrive/My Drive/deepxplore/ImageNet/generated_inputs/' + args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '_orig.png', orig_img_deprocessed)\n",
        "            break"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input already causes different outputs: 0, 1, 0\u001b[0m\n",
            "update coverage finished\n",
            "updatting coverage for model1\n",
            "update coverage finished\n",
            "updatting coverage for model3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-e740a0d9bb58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\nfor _ in range(args.seeds):\\n    gen_img = preprocess_image(random.choice(img_paths))\\n    gen_img2 = preprocess_image2(random.choice(img_paths))\\n    orig_img = gen_img.copy()\\n    # first check if input already induces differences\\n    pred1, pred2, pred3 = model1.predict(gen_img), model2.predict(gen_img), model3.predict(gen_img2)\\n    label1, label2, label3 = np.argmax(pred1[0]), np.argmax(pred2[0]), np.argmax(pred3[0])\\n    if not label1 == label2 == label3:\\n      ######################## 摂動を加えなくても予測が3つのモデルで異なる場合の処理 Start ########################\\n      print(\\'input already causes different outputs: {}, {}, {}\\'.format(label1, label2 ,label3) + bcolors.ENDC) \\n\\n      update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\\n      print(\"updatting coverage for model1\")\\n      update_coverage(gen_img2, model3, model_layer_dict3, args.threshold)\\n      print(\"updatting coverage for model3\")\\n      update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\\n      print(\"updatting coverage for model2\")\\n\\n      print(bcolors.OKGREEN + \\'covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f\\'\\n      #        print(\\'covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f\\'\\n            % (len(model_layer_dict1), neuron_covered(model_la...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-17f7216d9756>\u001b[0m in \u001b[0;36mupdate_coverage\u001b[0;34m(input_data, model, model_layer_dict, threshold)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     intermediate_layer_model = Model(inputs=model.input,\n\u001b[0;32m--> 118\u001b[0;31m                                      outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mintermediate_layer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_layer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-17f7216d9756>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     intermediate_layer_model = Model(inputs=model.input,\n\u001b[0;32m--> 118\u001b[0;31m                                      outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mintermediate_layer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_layer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             raise AttributeError('Layer ' + self.name +\n\u001b[0;32m--> 807\u001b[0;31m                                  \u001b[0;34m' has multiple inbound nodes, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m                                  \u001b[0;34m'hence the notion of \"layer output\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                                  \u001b[0;34m'is ill-defined. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Layer resnet50 has multiple inbound nodes, hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead."
          ]
        }
      ]
    }
  ]
}