{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "DeepXplore_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshii/de4test/blob/test_ozawa/DeepXplore/DeepXplore_03_09_df_connected\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVFjZpUIRF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 共通で使うパスなどの定義\n",
        "# 共通の変数設定\n",
        "# 共通フォルダパス\n",
        "\n",
        "import datetime\n",
        "import pytz\n",
        "dt_now = datetime.datetime.now(pytz.timezone('Asia/Tokyo'))\n",
        "dt_str = str(dt_now.strftime('%Y%m%d_%H%M'))\n",
        "\n",
        "data_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data\"\n",
        "#data_imagenet = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet\"\n",
        "data_mnist = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/MNIST\"\n",
        "#data_imagenet_seeds = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet/seeds\"\n",
        "model_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/model\"\n",
        "#output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output\"\n",
        "output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output/\" + dt_str\n",
        "tmp_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/tmp\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hv53U_OdAFYb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4lP4POGn5Xj",
        "colab_type": "code",
        "outputId": "98a8d786-2f19-4c23-bd51-01917f98adef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "# Goggle Drive つなぐ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3d3Suv3Q5Db",
        "colab_type": "code",
        "outputId": "06448f3f-98de-4f0f-d5bd-a9870fe19302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# outputフォルダ内容物のクリーンアップ削除\n",
        "!mkdir \"$output_dir\"\n",
        "!rm \"$output_dir\"/*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200123_1756/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdHYnLT0Nbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST')\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDY8sDVBqxmK",
        "colab_type": "code",
        "outputId": "babf23a3-4f80-4000-e413-26f5e592545d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input\n",
        "import imageio\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "import os\n",
        "\n",
        "# TensorFlowでGPUを使っているかのチェック：\n",
        "# \"device_type: \"GPU\" \" があればOK\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 5513938901525867866, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 1945639401982692596\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 7947406368295867070\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14912199066\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 15566115145974287296\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQ2M_8glJNM",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "3a5b5225-128e-4425-e2e7-54edbf615748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "# DeepXplore のパラメータ設定部\n",
        "# read the parameter\n",
        "# argument parsing\n",
        "parser = argparse.ArgumentParser(description='Main function for difference-inducing input generation in MNIST dataset')\n",
        "parser.add_argument('transformation', help=\"realistic transformation type\", choices=['light', 'occl', 'blackout'])\n",
        "parser.add_argument('weight_diff', help=\"weight hyperparm to control differential behavior\", type=float)\n",
        "parser.add_argument('weight_nc', help=\"weight hyperparm to control neuron coverage\", type=float)\n",
        "parser.add_argument('step', help=\"step size of gradient descent\", type=float)\n",
        "parser.add_argument('seeds', help=\"number of seeds of input\", type=int)\n",
        "parser.add_argument('grad_iterations', help=\"number of iterations of gradient descent\", type=int)\n",
        "parser.add_argument('threshold', help=\"threshold for determining neuron activated\", type=float)\n",
        "parser.add_argument('-t', '--target_model', help=\"target model that we want it predicts differently\",\n",
        "                    choices=[0, 1, 2], default=0, type=int)\n",
        "parser.add_argument('-sp', '--start_point', help=\"occlusion upper left corner coordinate\", default=(0, 0), type=tuple)\n",
        "parser.add_argument('-occl_size', '--occlusion_size', help=\"occlusion size\", default=(10, 10), type=tuple)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-occl_size', '--occlusion_size'], dest='occlusion_size', nargs=None, const=None, default=(10, 10), type=<class 'tuple'>, choices=None, help='occlusion size', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93KW-VGjMzun",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title ノイズのタイプ light / occl / blackout\n",
        "#@body {light,occl,blackout} weight_diff weight_nc step seeds\n",
        "#                   grad_iterations threshold\n",
        "noise_type = \"light\" #@param [\"light\", \"occl\", \"blackout\"]\n",
        "weight_diff = \"0.1\" #@param {type:\"string\"}\n",
        "weight_nc = \"0.1\" #@param {type:\"string\"}\n",
        "step = \"20\" #@param {type:\"string\"}　#50だとほぼ白飛び\n",
        "seeds = \"5\" #@param {type:\"string\"}\n",
        "grad_iterations = \"10\" #@param {type:\"string\"}\n",
        "threshold = \"0.1\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk80fCuPts9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = parser.parse_args([noise_type, weight_diff, weight_nc, step, seeds, grad_iterations, threshold])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tTYrllplJNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x.reshape(x.shape[1], x.shape[2])  # original shape (1,img_rows, img_cols,1)\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "def constraint_occl(gradients, start_point, rect_shape):\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def constraint_light(gradients):\n",
        "    new_grads = np.ones_like(gradients)\n",
        "    grad_mean = np.mean(gradients)\n",
        "    return grad_mean * new_grads\n",
        "\n",
        "\n",
        "def constraint_black(gradients, rect_shape=(6, 6)):\n",
        "    start_point = (\n",
        "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    if np.mean(patch) < 0:\n",
        "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def init_coverage_tables(model1, model2, model3):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    model_layer_dict2 = defaultdict(bool)\n",
        "    model_layer_dict3 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    init_dict(model2, model_layer_dict2)\n",
        "    init_dict(model3, model_layer_dict3)\n",
        "    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n",
        "\n",
        "\n",
        "def init_dict(model, model_layer_dict):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_dict[(layer.name, index)] = False\n",
        "\n",
        "\n",
        "def neuron_to_cover(model_layer_dict):\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "    else:\n",
        "        layer_name, index = random.choice(model_layer_dict.keys())\n",
        "    return layer_name, index\n",
        "\n",
        "\n",
        "def neuron_covered(model_layer_dict):\n",
        "    covered_neurons = len([v for v in model_layer_dict.values() if v])\n",
        "    total_neurons = len(model_layer_dict)\n",
        "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n",
        "\n",
        "\n",
        "def update_coverage(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n",
        "                model_layer_dict[(layer_names[i], num_neuron)] = True\n",
        "\n",
        "\n",
        "def full_coverage(model_layer_dict):\n",
        "    if False in model_layer_dict.values():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
        "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
        "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
        "    X_scaled = X_std * (rmax - rmin) + rmin\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def fired(model, layer_name, index, input_data, threshold=0):\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
        "    scaled = scale(intermediate_layer_output)\n",
        "    if np.mean(scaled[..., index]) > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def diverged(predictions1, predictions2, predictions3, target):\n",
        "    #     if predictions2 == predictions3 == target and predictions1 != target:\n",
        "    if not predictions1 == predictions2 == predictions3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "#add\n",
        "def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n",
        "    return [os.path.join(root, f)\n",
        "            for root, _, files in os.walk(directory) for f in files\n",
        "            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDDT47Er6u23",
        "colab_type": "code",
        "outputId": "afa02bf8-1575-4ff2-f82d-c20e8f09d97c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        }
      },
      "source": [
        "'''\n",
        "LeNet-1\n",
        "'''\n",
        "def Model1(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        print(x_train.shape)\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(4, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(12, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "#        model.save_weights('./Model1.h5')\n",
        "        model.save_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "#        model.load_weights('./Model1.h5')\n",
        "        model.load_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        print(bcolors.OKBLUE + 'Model1 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model1(train=True)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.4390 - acc: 0.8685 - val_loss: 0.1246 - val_acc: 0.9623\n",
            "\n",
            "\n",
            "Overall Test score: 0.12461089631170034\n",
            "Overall Test accuracy: 0.9623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU88awZf68JO",
        "colab_type": "code",
        "outputId": "68ee7e50-895e-4454-b7ac-03e94241249b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "'''\n",
        "LeNet-4\n",
        "'''\n",
        "def Model2(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(84, activation='relu', name='fc1')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model2.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model2.h5')\n",
        "        print(bcolors.OKBLUE + 'Model2 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model2(train=True)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.3804 - acc: 0.8816 - val_loss: 0.1271 - val_acc: 0.9574\n",
            "\n",
            "\n",
            "Overall Test score: 0.12705045675449073\n",
            "Overall Test accuracy: 0.9574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufs67uRN7KYL",
        "colab_type": "code",
        "outputId": "de4b0f43-7b8c-43d8-e968-a4909f632cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "'''\n",
        "LeNet-5\n",
        "'''\n",
        "def Model3(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(120, activation='relu', name='fc1')(x)\n",
        "    x = Dense(84, activation='relu', name='fc2')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model3.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model3.h5')\n",
        "        print(bcolors.OKBLUE + 'Model3 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model3(train=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.4090 - acc: 0.8723 - val_loss: 0.1318 - val_acc: 0.9560\n",
            "\n",
            "\n",
            "Overall Test score: 0.13182757645472884\n",
            "Overall Test accuracy: 0.956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9QiwlW7WWR",
        "colab_type": "code",
        "outputId": "aa62d4cc-6fae-48b8-9fda-ee99bfa0f038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# input MNIST image dimensions \n",
        "img_rows, img_cols = 28, 28\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "#define the model instance\n",
        "model1 = Model1(input_tensor=input_tensor)\n",
        "model2 = Model2(input_tensor=input_tensor)\n",
        "model3 = Model3(input_tensor=input_tensor)\n",
        "\n",
        "#define the dictionary of neuron coverage\n",
        "model_layer_dict1, model_layer_dict2, model_layer_dict3 = init_coverage_tables(model1, model2, model3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[94mModel1 loaded\u001b[0m\n",
            "\u001b[94mModel2 loaded\u001b[0m\n",
            "\u001b[94mModel3 loaded\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MS5H74Q4XA",
        "colab_type": "text"
      },
      "source": [
        "### 入力するデータの選別(1/2)\n",
        "\n",
        "０～９の数字をそれぞれ束ねてデータを準備する\n",
        "[00...011...1......99...9]。\n",
        "\n",
        "* test_per_fig_x: \n",
        "    各数字に対する画像データを格納\n",
        "* test_per_fig_y: \n",
        "    各数字に対する教師ラベルを格納\n",
        "* tests_x: \n",
        "    test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: \n",
        "    tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brFIdSfL8nHP",
        "colab_type": "code",
        "outputId": "5ea5fcb1-cd55-473e-9a25-db579d103d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# load the MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "num_fig = 10\n",
        "test_per_fig_x = np.array([])\n",
        "test_per_fig_y = np.array([])\n",
        "tests_x = np.array([])\n",
        "tests_y = np.array([])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "#x_train = x_train.astype('float')\n",
        "#x_test = x_test.astype('float')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "length = int(args.seeds)\n",
        "#length = int(args.seeds)\n",
        "for i in range(num_fig):\n",
        "  cond = [(x==i) for x in y_test]\n",
        "  test_per_fig_x = x_test[cond]\n",
        "  test_per_fig_y = y_test[cond]\n",
        "#  np.set_printoptions(formatter={'int': '{:07d}'.format})\n",
        "  print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "  tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "  tests_y = np.append(tests_y, test_per_fig_y[:length])\n",
        "#  conds = [conds, cond]\n",
        "print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "\n",
        "###### MNIST data, shuffled and split by train and test sets\n",
        "#####(_, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "###### modify the numpy data for the Keras model\n",
        "#####x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "#####input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "#tests_x.shape\n",
        "tests_x = tests_x.astype('float32')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure: 0 , shape: (980, 28, 28) , deviation: -20\n",
            "figure: 1 , shape: (1135, 28, 28) , deviation: 135\n",
            "figure: 2 , shape: (1032, 28, 28) , deviation: 32\n",
            "figure: 3 , shape: (1010, 28, 28) , deviation: 10\n",
            "figure: 4 , shape: (982, 28, 28) , deviation: -18\n",
            "figure: 5 , shape: (892, 28, 28) , deviation: -108\n",
            "figure: 6 , shape: (958, 28, 28) , deviation: -42\n",
            "figure: 7 , shape: (1028, 28, 28) , deviation: 28\n",
            "figure: 8 , shape: (974, 28, 28) , deviation: -26\n",
            "figure: 9 , shape: (1009, 28, 28) , deviation: 9\n",
            "check! 5.0 = 5 equal?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUOK_EjJW2KT",
        "colab_type": "text"
      },
      "source": [
        "## 入力するデータの選別(2/2)\n",
        "０～９の数字をそれぞれ束ねてデータを準備する [00...000]。\n",
        "\n",
        "* test_per_fig_x: 各数字に対する画像データを格納\n",
        "* test_per_fig_y: 各数字に対する教師ラベルを格納\n",
        "* tests_x: test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "id": "SLee__9UAD1Y",
        "colab": {}
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "#load the MNIST \n",
        "\n",
        "def create_data(start_fig, num_fi, length):\n",
        "  (datax_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  test_per_fig_x = np.array([])\n",
        "  test_per_fig_y = np.array([])\n",
        "  tests_x = np.array([])\n",
        "  tests_y = np.array([])\n",
        "\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test  = x_test.astype('float32')\n",
        "  x_train = x_train.astype('float')\n",
        "  x_test = x_test.astype('float')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  #length = int(args.seeds/num_fig)\n",
        "  #length = int(args.seeds)\n",
        "  for i in range(start_fig, start_fig+num_fig):\n",
        "  #i=0\n",
        "    cond = [(x==i) for x in y_test]\n",
        "    test_per_fig_x = x_test[cond]\n",
        "    test_per_fig_y = y_test[cond]\n",
        "    print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "    tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "    tests_y = np.append(tests_y, test_per_fig_y[:length])  \n",
        "\n",
        "  #conds = [conds, cond]\n",
        "  print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "  tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "  tests_x.shape\n",
        "  tests_x = tests_x.astype('float32')\n",
        "  return tests_x, tests_y, length\n",
        "  tests_x, tests_y, length = create_data(0, 10, int(args.seeds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyo2SSSz8vkT",
        "colab_type": "code",
        "outputId": "0d247e82-cb37-4531-af71-9624437ed051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "print(tests_x.shape)\n",
        "print(tests_y)\n",
        "for i in range(0,1):\n",
        "  print(i)\n",
        "print(length)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 28, 28, 1)\n",
            "[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 4. 4. 4. 4.\n",
            " 4. 5. 5. 5. 5. 5. 6. 6. 6. 6. 6. 7. 7. 7. 7. 7. 8. 8. 8. 8. 8. 9. 9. 9.\n",
            " 9. 9.]\n",
            "0\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLlKIMxG9kEA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfdTZi4mUPf",
        "colab_type": "code",
        "outputId": "24974356-2c0d-423b-b163-de72569a9aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "def neuron_output(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "    \n",
        "    out_list = []\n",
        "    out_list_scale = []\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            out_list.append(np.mean(intermediate_layer_output[..., num_neuron]))\n",
        "            out_list_scale.append(np.mean(scaled[..., num_neuron]))\n",
        "\n",
        "    return out_list, out_list_scale\n",
        "\n",
        "\n",
        "def deepXplore(model_layer_dict1, model_layer_dict2, model_layer_dict3, tests_x, model1, model2, model3, num_fig, start_fig, length):\n",
        "  #print(\"test01\")\n",
        "  count_already = 0\n",
        "  count_found = 0\n",
        "  count_not_found = 0\n",
        "  temp_per_nc1 = np.array([])\n",
        "  temp_per_nc2 = np.array([])\n",
        "  temp_per_nc3 = np.array([])\n",
        "  temp_num_nc1 = np.array([])\n",
        "  temp_num_nc2 = np.array([])\n",
        "  temp_num_nc3 = np.array([])\n",
        "  #print(\"test10\")\n",
        "\n",
        "  #for each neuron\n",
        "  num_neurons1 = neuron_covered(model_layer_dict1)[1]\n",
        "  num_neurons2 = neuron_covered(model_layer_dict2)[1]\n",
        "  num_neurons3 = neuron_covered(model_layer_dict3)[1]\n",
        "  #print(\"test11\")\n",
        "\n",
        "  column_tmp1 = list(model_layer_dict1.keys())\n",
        "  column_tmp2 = list(model_layer_dict2.keys())\n",
        "  column_tmp3 = list(model_layer_dict3.keys())\n",
        "  #print(\"test12\")\n",
        "  df1 = pd.DataFrame(columns=column_tmp1)\n",
        "  df2 = pd.DataFrame(columns=column_tmp2)\n",
        "  df3 = pd.DataFrame(columns=column_tmp3)\n",
        "  df1_scale = pd.DataFrame(columns=column_tmp1)\n",
        "  df2_scale = pd.DataFrame(columns=column_tmp2)\n",
        "  df3_scale = pd.DataFrame(columns=column_tmp3)\n",
        "  bug_result1 = pd.DataFrame(columns=[\"already_diff\", \"found\", \"not_found\", \"layer1\", \"index1\", \"layer2\", \"index2\", \"layer3\", \"index3\"])\n",
        "  trial = 1\n",
        "  #print(\"test13\")\n",
        "  #print(\"test02\")\n",
        "  for index_fig in range(num_fig):\n",
        "    index_fig = index_fig + start_fig\n",
        "    print(\"figure\"+str(index_fig))\n",
        "    for _ in range(length):\n",
        "      #gen_img = np.expand_dims(random.choice(tests_x), axis=0)\n",
        "      gen_img = np.expand_dims(tests_x[(length*index_fig + _)], axis=0)\n",
        "      orig_img = gen_img.copy()\n",
        "      # first check if input already induces differences\n",
        "      label1, label2, label3 = np.argmax(model1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "      if not label1 == label2 == label3:\n",
        "          count_already += 1\n",
        "          print(bcolors.OKGREEN + '   {}/{}. input already causes different outputs ({},{},{}) at({}, {}, {}): '.format(_, length, label1, label2, label3, count_already, count_found, count_not_found) + bcolors.ENDC)        \n",
        "\n",
        "          update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "          temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "          temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "          temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "          temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "          temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "          temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "                               \n",
        "          print(bcolors.OKGREEN + '     covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'% (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  neuron_covered(model_layer_dict2)[2], len(model_layer_dict3), neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "          averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                       neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +neuron_covered(model_layer_dict3)[1])\n",
        "          print(bcolors.OKGREEN + '     averaged covered neurons %.3f' % (averaged_nc) + bcolors.ENDC)\n",
        "\n",
        "          gen_img_deprocessed = deprocess_image(gen_img)\n",
        "\n",
        "          # save the result to disk\n",
        "          outputfilepath0 = os.path.join(output_dir, 'already_differ_' + str(label1) + '_' + str(label2) + '_' + str(label3) +'_['+ str(_) +  '].png')\n",
        "          imageio.imwrite(outputfilepath0, gen_img_deprocessed)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          temp = pd.Series(temp, index=df1.columns, name=str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(trial))\n",
        "          df1 = df1.append(temp)\n",
        "          df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          temp = pd.Series(temp, index=df2.columns, name=str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(trial))\n",
        "          df2 = df2.append(temp)\n",
        "          df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "          temp = pd.Series(temp, index=df3.columns, name=str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(trial))\n",
        "          df3 = df3.append(temp)\n",
        "          df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "          trial += 1\n",
        "          continue\n",
        "\n",
        "      # if all label agrees\n",
        "      orig_label = label1\n",
        "      layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n",
        "      layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n",
        "      layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n",
        "\n",
        "      # construct joint loss function\n",
        "      if args.target_model == 0:\n",
        "        loss1 = -args.weight_diff * K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 1:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = -args.weight_diff * K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 2:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n",
        "      loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n",
        "      loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n",
        "      layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron)\n",
        "\n",
        "      # for adversarial image generation\n",
        "      final_loss = K.mean(layer_output)\n",
        "\n",
        "      # we compute the gradient of the input picture wrt this loss\n",
        "      grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "\n",
        "      # this function returns the loss and grads given the input picture\n",
        "      iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n",
        "\n",
        "      #print(\"test03\")\n",
        "      # we run gradient ascent for some steps\n",
        "      for iters in range(args.grad_iterations):\n",
        "        loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = iterate([gen_img])\n",
        "        if args.transformation == 'light':\n",
        "            grads_value = constraint_light(grads_value)  # constraint the gradients value\n",
        "        elif args.transformation == 'occl':\n",
        "            grads_value = constraint_occl(grads_value, args.start_point,\n",
        "                                          args.occlusion_size)  # constraint the gradients value\n",
        "        elif args.transformation == 'blackout':\n",
        "            grads_value = constraint_black(grads_value)  # constraint the gradients value\n",
        "\n",
        "        gen_img += grads_value * args.step\n",
        "        predictions1 = np.argmax(model1.predict(gen_img)[0])\n",
        "        predictions2 = np.argmax(model2.predict(gen_img)[0])\n",
        "        predictions3 = np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "        #print(\"test04\")\n",
        "        if not predictions1 == predictions2 == predictions3:\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            count_found += 1\n",
        "            print(bcolors.OKBLUE + '%4d/%d. found at %d! covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f at (%d, %d, %d)'\n",
        "#                  % (_, args.seeds, iters, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  % (_, length, iters + 1, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                     neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                     neuron_covered(model_layer_dict3)[2], count_already, count_found, count_not_found) + bcolors.ENDC)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            print(bcolors.OKBLUE + '     averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  '].png')\n",
        "            #print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "#            outputfilepath2 = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '_orig.png')\n",
        "            outputfilepath2 = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  ']_orig.png')\n",
        "            #print(outputfilepath2)\n",
        "            imageio.imwrite(outputfilepath2, orig_img_deprocessed)\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "            trial += 1\n",
        "            break\n",
        "          \n",
        "          #add\n",
        "        #print(\"test05\")\n",
        "        if iters == (args.grad_iterations-1):\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            count_not_found += 1\n",
        "#              print('%4d/%d. test suite was not found: averaged covered neurons %.3f at %d/%d' % (_, args.seeds, averaged_nc, count_not_found, count_already + count_found + count_not_found))\n",
        "            print('%4d/%d. test suite was not found: averaged covered neurons %.3f at (%d, %d, %d)' % (_, length, averaged_nc, count_already, count_found, count_not_found))\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            #orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, 'not_found_' + str(label1)+'_['+ str(_) + '].png')\n",
        "#           print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "\n",
        "            \n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "            trial += 1\n",
        "            \n",
        "            #break\n",
        "\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "  temp_per_nc1=temp_per_nc1.reshape(num_fig, length)\n",
        "  temp_per_nc2=temp_per_nc2.reshape(num_fig, length)\n",
        "  temp_per_nc3=temp_per_nc3.reshape(num_fig, length)\n",
        "  temp_num_nc1=temp_num_nc1.reshape(num_fig, length)\n",
        "  temp_num_nc2=temp_num_nc2.reshape(num_fig, length)\n",
        "  temp_num_nc3=temp_num_nc3.reshape(num_fig, length)\n",
        "\n",
        "  return df1, df2, df3, df1_scale, df2_scale, df3_scale"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
            "Wall time: 6.68 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHi1ffJuWH5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "20f2c658-6e9e-40db-b5ab-c64403918c90"
      },
      "source": [
        "print(length, num_fig)\n",
        "print(output_dir)\n",
        "column_tmp1 = list(model_layer_dict1.keys())\n",
        "df1 = pd.DataFrame(columns=column_tmp1)\n",
        "df1.head()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 1\n",
            "/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200123_1756\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(block1_conv1, 0)</th>\n",
              "      <th>(block1_conv1, 1)</th>\n",
              "      <th>(block1_conv1, 2)</th>\n",
              "      <th>(block1_conv1, 3)</th>\n",
              "      <th>(block1_pool1, 0)</th>\n",
              "      <th>(block1_pool1, 1)</th>\n",
              "      <th>(block1_pool1, 2)</th>\n",
              "      <th>(block1_pool1, 3)</th>\n",
              "      <th>(block2_conv1, 0)</th>\n",
              "      <th>(block2_conv1, 1)</th>\n",
              "      <th>(block2_conv1, 2)</th>\n",
              "      <th>(block2_conv1, 3)</th>\n",
              "      <th>(block2_conv1, 4)</th>\n",
              "      <th>(block2_conv1, 5)</th>\n",
              "      <th>(block2_conv1, 6)</th>\n",
              "      <th>(block2_conv1, 7)</th>\n",
              "      <th>(block2_conv1, 8)</th>\n",
              "      <th>(block2_conv1, 9)</th>\n",
              "      <th>(block2_conv1, 10)</th>\n",
              "      <th>(block2_conv1, 11)</th>\n",
              "      <th>(block2_pool1, 0)</th>\n",
              "      <th>(block2_pool1, 1)</th>\n",
              "      <th>(block2_pool1, 2)</th>\n",
              "      <th>(block2_pool1, 3)</th>\n",
              "      <th>(block2_pool1, 4)</th>\n",
              "      <th>(block2_pool1, 5)</th>\n",
              "      <th>(block2_pool1, 6)</th>\n",
              "      <th>(block2_pool1, 7)</th>\n",
              "      <th>(block2_pool1, 8)</th>\n",
              "      <th>(block2_pool1, 9)</th>\n",
              "      <th>(block2_pool1, 10)</th>\n",
              "      <th>(block2_pool1, 11)</th>\n",
              "      <th>(before_softmax, 0)</th>\n",
              "      <th>(before_softmax, 1)</th>\n",
              "      <th>(before_softmax, 2)</th>\n",
              "      <th>(before_softmax, 3)</th>\n",
              "      <th>(before_softmax, 4)</th>\n",
              "      <th>(before_softmax, 5)</th>\n",
              "      <th>(before_softmax, 6)</th>\n",
              "      <th>(before_softmax, 7)</th>\n",
              "      <th>(before_softmax, 8)</th>\n",
              "      <th>(before_softmax, 9)</th>\n",
              "      <th>(predictions, 0)</th>\n",
              "      <th>(predictions, 1)</th>\n",
              "      <th>(predictions, 2)</th>\n",
              "      <th>(predictions, 3)</th>\n",
              "      <th>(predictions, 4)</th>\n",
              "      <th>(predictions, 5)</th>\n",
              "      <th>(predictions, 6)</th>\n",
              "      <th>(predictions, 7)</th>\n",
              "      <th>(predictions, 8)</th>\n",
              "      <th>(predictions, 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [(block1_conv1, 0), (block1_conv1, 1), (block1_conv1, 2), (block1_conv1, 3), (block1_pool1, 0), (block1_pool1, 1), (block1_pool1, 2), (block1_pool1, 3), (block2_conv1, 0), (block2_conv1, 1), (block2_conv1, 2), (block2_conv1, 3), (block2_conv1, 4), (block2_conv1, 5), (block2_conv1, 6), (block2_conv1, 7), (block2_conv1, 8), (block2_conv1, 9), (block2_conv1, 10), (block2_conv1, 11), (block2_pool1, 0), (block2_pool1, 1), (block2_pool1, 2), (block2_pool1, 3), (block2_pool1, 4), (block2_pool1, 5), (block2_pool1, 6), (block2_pool1, 7), (block2_pool1, 8), (block2_pool1, 9), (block2_pool1, 10), (block2_pool1, 11), (before_softmax, 0), (before_softmax, 1), (before_softmax, 2), (before_softmax, 3), (before_softmax, 4), (before_softmax, 5), (before_softmax, 6), (before_softmax, 7), (before_softmax, 8), (before_softmax, 9), (predictions, 0), (predictions, 1), (predictions, 2), (predictions, 3), (predictions, 4), (predictions, 5), (predictions, 6), (predictions, 7), (predictions, 8), (predictions, 9)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSSBfoAZreWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d6d5cc29-cc22-48ed-f6c8-b71e21f6bd5c"
      },
      "source": [
        "index_fig = 0\n",
        "trial = 0\n",
        "gen_img = np.expand_dims(tests_x[(length*index_fig + 0)], axis=0)\n",
        "temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "print(type(temp_scale))\n",
        "temp_scale.name"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0_0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yymgs3ldNiDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "51f22c92-1d8e-48d7-ceaa-67b8f607e221"
      },
      "source": [
        "#num_fig = 1\n",
        "#start_fig = 0\n",
        "#deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "\n",
        "num_fig = 1\n",
        "for start_fig in range(10):\n",
        "  df1, df2, df3, df1_scale, df2_scale, df3_scale = deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "  df1_scale.to_csv(output_dir+ \"/../fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure0\n",
            "\u001b[94m   0/5. found at 3! covered neurons percentage 52 neurons 0.577, 148 neurons 0.527, 268 neurons 0.455 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.491\u001b[0m\n",
            "\u001b[94m   1/5. found at 3! covered neurons percentage 52 neurons 0.577, 148 neurons 0.534, 268 neurons 0.478 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.506\u001b[0m\n",
            "\u001b[94m   2/5. found at 4! covered neurons percentage 52 neurons 0.577, 148 neurons 0.534, 268 neurons 0.485 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.511\u001b[0m\n",
            "\u001b[94m   3/5. found at 6! covered neurons percentage 52 neurons 0.596, 148 neurons 0.547, 268 neurons 0.515 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.534\u001b[0m\n",
            "\u001b[94m   4/5. found at 3! covered neurons percentage 52 neurons 0.596, 148 neurons 0.547, 268 neurons 0.515 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.534\u001b[0m\n",
            "figure1\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.827, 148 neurons 0.723, 268 neurons 0.660 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.699\u001b[0m\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.846, 148 neurons 0.750, 268 neurons 0.720 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.744\u001b[0m\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.846, 148 neurons 0.750, 268 neurons 0.720 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.744\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.846, 148 neurons 0.764, 268 neurons 0.724 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.750\u001b[0m\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.846, 148 neurons 0.777, 268 neurons 0.746 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.767\u001b[0m\n",
            "figure2\n",
            "   0/5. test suite was not found: averaged covered neurons 0.769 at (0, 0, 1)\n",
            "   1/5. test suite was not found: averaged covered neurons 0.774 at (0, 0, 2)\n",
            "\u001b[94m   2/5. found at 4! covered neurons percentage 52 neurons 0.865, 148 neurons 0.791, 268 neurons 0.746 at (0, 1, 2)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.774\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.885, 148 neurons 0.804, 268 neurons 0.757 at (0, 2, 2)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.786\u001b[0m\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.885, 148 neurons 0.804, 268 neurons 0.757 at (0, 3, 2)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.786\u001b[0m\n",
            "figure3\n",
            "\u001b[94m   0/5. found at 4! covered neurons percentage 52 neurons 0.885, 148 neurons 0.811, 268 neurons 0.765 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.793\u001b[0m\n",
            "\u001b[94m   1/5. found at 3! covered neurons percentage 52 neurons 0.885, 148 neurons 0.811, 268 neurons 0.769 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.795\u001b[0m\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.811, 268 neurons 0.769 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.795\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.885, 148 neurons 0.811, 268 neurons 0.772 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.797\u001b[0m\n",
            "\u001b[94m   4/5. found at 4! covered neurons percentage 52 neurons 0.885, 148 neurons 0.811, 268 neurons 0.776 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.799\u001b[0m\n",
            "figure4\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.885, 148 neurons 0.831, 268 neurons 0.806 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.823\u001b[0m\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.831, 268 neurons 0.813 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.829\u001b[0m\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.831, 268 neurons 0.813 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.829\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.845, 268 neurons 0.825 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.840\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.845, 268 neurons 0.825 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.840\u001b[0m\n",
            "figure5\n",
            "\u001b[94m   0/5. found at 3! covered neurons percentage 52 neurons 0.904, 148 neurons 0.851, 268 neurons 0.825 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.842\u001b[0m\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.851, 268 neurons 0.825 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.842\u001b[0m\n",
            "\u001b[94m   2/5. found at 3! covered neurons percentage 52 neurons 0.904, 148 neurons 0.851, 268 neurons 0.825 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.842\u001b[0m\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.851, 268 neurons 0.825 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.842\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.851, 268 neurons 0.825 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.842\u001b[0m\n",
            "figure6\n",
            "\u001b[94m   0/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.851, 268 neurons 0.825 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.842\u001b[0m\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.851, 268 neurons 0.825 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.844\u001b[0m\n",
            "\u001b[94m   2/5. found at 6! covered neurons percentage 52 neurons 0.923, 148 neurons 0.851, 268 neurons 0.825 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.844\u001b[0m\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.858, 268 neurons 0.832 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.850\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.858, 268 neurons 0.832 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.850\u001b[0m\n",
            "figure7\n",
            "\u001b[94m   0/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.858, 268 neurons 0.832 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.850\u001b[0m\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.858, 268 neurons 0.836 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.853\u001b[0m\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.858, 268 neurons 0.843 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.857\u001b[0m\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.858, 268 neurons 0.843 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.857\u001b[0m\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.865, 268 neurons 0.854 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.865\u001b[0m\n",
            "figure8\n",
            "\u001b[94m   0/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.865, 268 neurons 0.854 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.865\u001b[0m\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.865, 268 neurons 0.854 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.865\u001b[0m\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.865, 268 neurons 0.858 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.868\u001b[0m\n",
            "\u001b[94m   3/5. found at 9! covered neurons percentage 52 neurons 0.923, 148 neurons 0.865, 268 neurons 0.858 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.868\u001b[0m\n",
            "   4/5. test suite was not found: averaged covered neurons 0.870 at (0, 4, 1)\n",
            "figure9\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.865, 268 neurons 0.866 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.872\u001b[0m\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.865, 268 neurons 0.866 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.872\u001b[0m\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.865, 268 neurons 0.866 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.872\u001b[0m\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.872, 268 neurons 0.873 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.878\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.872, 268 neurons 0.873 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.878\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpsI1IOM-hUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "58ff0c23-66aa-4051-dd16-db6e0248cc07"
      },
      "source": [
        "df2_scale = pd.DataFrame()\n",
        "for start_fig in range(10):\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")\n",
        "  df2_scale = pd.concat([df2_scale, df1_scale])\n",
        "\n",
        "df2_scale.head()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>('block1_conv1', 0)</th>\n",
              "      <th>('block1_conv1', 1)</th>\n",
              "      <th>('block1_conv1', 2)</th>\n",
              "      <th>('block1_conv1', 3)</th>\n",
              "      <th>('block1_pool1', 0)</th>\n",
              "      <th>('block1_pool1', 1)</th>\n",
              "      <th>('block1_pool1', 2)</th>\n",
              "      <th>('block1_pool1', 3)</th>\n",
              "      <th>('block2_conv1', 0)</th>\n",
              "      <th>('block2_conv1', 1)</th>\n",
              "      <th>('block2_conv1', 2)</th>\n",
              "      <th>('block2_conv1', 3)</th>\n",
              "      <th>('block2_conv1', 4)</th>\n",
              "      <th>('block2_conv1', 5)</th>\n",
              "      <th>('block2_conv1', 6)</th>\n",
              "      <th>('block2_conv1', 7)</th>\n",
              "      <th>('block2_conv1', 8)</th>\n",
              "      <th>('block2_conv1', 9)</th>\n",
              "      <th>('block2_conv1', 10)</th>\n",
              "      <th>('block2_conv1', 11)</th>\n",
              "      <th>('block2_pool1', 0)</th>\n",
              "      <th>('block2_pool1', 1)</th>\n",
              "      <th>('block2_pool1', 2)</th>\n",
              "      <th>('block2_pool1', 3)</th>\n",
              "      <th>('block2_pool1', 4)</th>\n",
              "      <th>('block2_pool1', 5)</th>\n",
              "      <th>('block2_pool1', 6)</th>\n",
              "      <th>('block2_pool1', 7)</th>\n",
              "      <th>('block2_pool1', 8)</th>\n",
              "      <th>('block2_pool1', 9)</th>\n",
              "      <th>('block2_pool1', 10)</th>\n",
              "      <th>('block2_pool1', 11)</th>\n",
              "      <th>('before_softmax', 0)</th>\n",
              "      <th>('before_softmax', 1)</th>\n",
              "      <th>('before_softmax', 2)</th>\n",
              "      <th>('before_softmax', 3)</th>\n",
              "      <th>('before_softmax', 4)</th>\n",
              "      <th>('before_softmax', 5)</th>\n",
              "      <th>('before_softmax', 6)</th>\n",
              "      <th>('before_softmax', 7)</th>\n",
              "      <th>('before_softmax', 8)</th>\n",
              "      <th>('before_softmax', 9)</th>\n",
              "      <th>('predictions', 0)</th>\n",
              "      <th>('predictions', 1)</th>\n",
              "      <th>('predictions', 2)</th>\n",
              "      <th>('predictions', 3)</th>\n",
              "      <th>('predictions', 4)</th>\n",
              "      <th>('predictions', 5)</th>\n",
              "      <th>('predictions', 6)</th>\n",
              "      <th>('predictions', 7)</th>\n",
              "      <th>('predictions', 8)</th>\n",
              "      <th>('predictions', 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0_1</td>\n",
              "      <td>0.686</td>\n",
              "      <td>0.696</td>\n",
              "      <td>0.601</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.727</td>\n",
              "      <td>0.732</td>\n",
              "      <td>0.640</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.301</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.311</td>\n",
              "      <td>0.286</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.203</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.931</td>\n",
              "      <td>0.909</td>\n",
              "      <td>0.537</td>\n",
              "      <td>0.695</td>\n",
              "      <td>0.630</td>\n",
              "      <td>0.854</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.999</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0_2</td>\n",
              "      <td>0.720</td>\n",
              "      <td>0.731</td>\n",
              "      <td>0.631</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.767</td>\n",
              "      <td>0.672</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.229</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.256</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.293</td>\n",
              "      <td>0.272</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.838</td>\n",
              "      <td>0.776</td>\n",
              "      <td>0.843</td>\n",
              "      <td>0.543</td>\n",
              "      <td>0.601</td>\n",
              "      <td>0.587</td>\n",
              "      <td>0.738</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0_3</td>\n",
              "      <td>0.704</td>\n",
              "      <td>0.715</td>\n",
              "      <td>0.617</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.744</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.657</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.224</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.279</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.293</td>\n",
              "      <td>0.258</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.873</td>\n",
              "      <td>0.834</td>\n",
              "      <td>0.766</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.664</td>\n",
              "      <td>0.557</td>\n",
              "      <td>0.733</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0_4</td>\n",
              "      <td>0.637</td>\n",
              "      <td>0.646</td>\n",
              "      <td>0.558</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.688</td>\n",
              "      <td>0.606</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.284</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.314</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.212</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.511</td>\n",
              "      <td>0.825</td>\n",
              "      <td>0.362</td>\n",
              "      <td>0.619</td>\n",
              "      <td>0.519</td>\n",
              "      <td>0.832</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.921</td>\n",
              "      <td>0.002</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0_5</td>\n",
              "      <td>0.669</td>\n",
              "      <td>0.679</td>\n",
              "      <td>0.586</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.715</td>\n",
              "      <td>0.629</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.284</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.301</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.197</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.865</td>\n",
              "      <td>0.910</td>\n",
              "      <td>0.559</td>\n",
              "      <td>0.558</td>\n",
              "      <td>0.567</td>\n",
              "      <td>0.662</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.983</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Unnamed: 0  ('block1_conv1', 0)  ...  ('predictions', 8)  ('predictions', 9)\n",
              "0        0_1                0.686  ...               0.001               0.000\n",
              "1        0_2                0.720  ...               1.000               0.000\n",
              "2        0_3                0.704  ...               1.000               0.000\n",
              "3        0_4                0.637  ...               0.000               0.000\n",
              "4        0_5                0.669  ...               0.000               0.000\n",
              "\n",
              "[5 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QtyLAZiPeKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bools_sum_layer =  pd.DataFrame(columns=(\"layer\", \"count\", \"index\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiVj9Wjd-rVv",
        "colab_type": "text"
      },
      "source": [
        "以下が、df1_scaleに格納されているニューロンの出力値から発火/非発火の集計をとるアルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B19EsPFQrak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "thres = 0.1\n",
        "bools = df1_scale > thres\n",
        "\n",
        "for i, count in enumerate(bools.sum()):\n",
        "#    print(bools.sum().index[i][0], bools.sum().index[i][1], count)\n",
        "    cont = [bools.sum().index[i][0], count, bools.sum().index[i][1]]\n",
        "    cont = pd.Series(cont, index=bools_sum_layer.columns, name=str(i))\n",
        "    bools_sum_layer = bools_sum_layer.append(cont)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGmSlrMm7g6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "9b06095e-6218-4ef9-ddbd-af3fced89c77"
      },
      "source": [
        "bools.head(20)\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(block1_conv1, 0)</th>\n",
              "      <th>(block1_conv1, 1)</th>\n",
              "      <th>(block1_conv1, 2)</th>\n",
              "      <th>(block1_conv1, 3)</th>\n",
              "      <th>(block1_pool1, 0)</th>\n",
              "      <th>(block1_pool1, 1)</th>\n",
              "      <th>(block1_pool1, 2)</th>\n",
              "      <th>(block1_pool1, 3)</th>\n",
              "      <th>(block2_conv1, 0)</th>\n",
              "      <th>(block2_conv1, 1)</th>\n",
              "      <th>(block2_conv1, 2)</th>\n",
              "      <th>(block2_conv1, 3)</th>\n",
              "      <th>(block2_conv1, 4)</th>\n",
              "      <th>(block2_conv1, 5)</th>\n",
              "      <th>(block2_conv1, 6)</th>\n",
              "      <th>(block2_conv1, 7)</th>\n",
              "      <th>(block2_conv1, 8)</th>\n",
              "      <th>(block2_conv1, 9)</th>\n",
              "      <th>(block2_conv1, 10)</th>\n",
              "      <th>(block2_conv1, 11)</th>\n",
              "      <th>(block2_pool1, 0)</th>\n",
              "      <th>(block2_pool1, 1)</th>\n",
              "      <th>(block2_pool1, 2)</th>\n",
              "      <th>(block2_pool1, 3)</th>\n",
              "      <th>(block2_pool1, 4)</th>\n",
              "      <th>(block2_pool1, 5)</th>\n",
              "      <th>(block2_pool1, 6)</th>\n",
              "      <th>(block2_pool1, 7)</th>\n",
              "      <th>(block2_pool1, 8)</th>\n",
              "      <th>(block2_pool1, 9)</th>\n",
              "      <th>(block2_pool1, 10)</th>\n",
              "      <th>(block2_pool1, 11)</th>\n",
              "      <th>(before_softmax, 0)</th>\n",
              "      <th>(before_softmax, 1)</th>\n",
              "      <th>(before_softmax, 2)</th>\n",
              "      <th>(before_softmax, 3)</th>\n",
              "      <th>(before_softmax, 4)</th>\n",
              "      <th>(before_softmax, 5)</th>\n",
              "      <th>(before_softmax, 6)</th>\n",
              "      <th>(before_softmax, 7)</th>\n",
              "      <th>(before_softmax, 8)</th>\n",
              "      <th>(before_softmax, 9)</th>\n",
              "      <th>(predictions, 0)</th>\n",
              "      <th>(predictions, 1)</th>\n",
              "      <th>(predictions, 2)</th>\n",
              "      <th>(predictions, 3)</th>\n",
              "      <th>(predictions, 4)</th>\n",
              "      <th>(predictions, 5)</th>\n",
              "      <th>(predictions, 6)</th>\n",
              "      <th>(predictions, 7)</th>\n",
              "      <th>(predictions, 8)</th>\n",
              "      <th>(predictions, 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9_1</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_2</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_3</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_4</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_5</th>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     (block1_conv1, 0)  (block1_conv1, 1)  ...  (predictions, 8)  (predictions, 9)\n",
              "9_1               True               True  ...              True             False\n",
              "9_2               True               True  ...              True             False\n",
              "9_3              False              False  ...             False             False\n",
              "9_4              False              False  ...             False             False\n",
              "9_5              False              False  ...             False             False\n",
              "\n",
              "[5 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU2yDijr_SYr",
        "colab_type": "text"
      },
      "source": [
        "[参考]bools.sum()、bools_sum_layerの中身"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W4ySii4Txkb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "3fb3fb1c-524d-4617-be54-cca7cc0c349d"
      },
      "source": [
        "#bools_sum_layer.columns\n",
        "bools.sum()[:10]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(block1_conv1, 0)    2\n",
              "(block1_conv1, 1)    2\n",
              "(block1_conv1, 2)    2\n",
              "(block1_conv1, 3)    0\n",
              "(block1_pool1, 0)    2\n",
              "(block1_pool1, 1)    2\n",
              "(block1_pool1, 2)    2\n",
              "(block1_pool1, 3)    3\n",
              "(block2_conv1, 0)    0\n",
              "(block2_conv1, 1)    3\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqW-Pk3iQoSv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "09f1555a-65a4-47e9-cbd9-f2984bcfb1de"
      },
      "source": [
        "bools_sum_layer.head(40)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>layer</th>\n",
              "      <th>count</th>\n",
              "      <th>index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>block1_conv1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>block1_conv1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>block1_conv1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>block1_conv1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>block1_pool1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>block1_pool1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>block1_pool1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>block1_pool1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>block2_conv1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>block2_conv1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>block2_conv1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>block2_conv1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>block2_conv1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>block2_conv1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>block2_conv1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>block2_conv1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>block2_conv1</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>block2_conv1</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>block2_conv1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>block2_conv1</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>block2_pool1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>block2_pool1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>block2_pool1</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>block2_pool1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>block2_pool1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>block2_pool1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>block2_pool1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>block2_pool1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>block2_pool1</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>block2_pool1</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>block2_pool1</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>block2_pool1</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>before_softmax</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>before_softmax</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>before_softmax</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>before_softmax</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>before_softmax</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>before_softmax</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>before_softmax</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>before_softmax</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             layer count index\n",
              "0     block1_conv1     2     0\n",
              "1     block1_conv1     2     1\n",
              "2     block1_conv1     2     2\n",
              "3     block1_conv1     0     3\n",
              "4     block1_pool1     2     0\n",
              "5     block1_pool1     2     1\n",
              "6     block1_pool1     2     2\n",
              "7     block1_pool1     3     3\n",
              "8     block2_conv1     0     0\n",
              "9     block2_conv1     3     1\n",
              "10    block2_conv1     2     2\n",
              "11    block2_conv1     2     3\n",
              "12    block2_conv1     0     4\n",
              "13    block2_conv1     4     5\n",
              "14    block2_conv1     2     6\n",
              "15    block2_conv1     0     7\n",
              "16    block2_conv1     2     8\n",
              "17    block2_conv1     2     9\n",
              "18    block2_conv1     0    10\n",
              "19    block2_conv1     2    11\n",
              "20    block2_pool1     2     0\n",
              "21    block2_pool1     3     1\n",
              "22    block2_pool1     4     2\n",
              "23    block2_pool1     2     3\n",
              "24    block2_pool1     1     4\n",
              "25    block2_pool1     5     5\n",
              "26    block2_pool1     2     6\n",
              "27    block2_pool1     0     7\n",
              "28    block2_pool1     2     8\n",
              "29    block2_pool1     2     9\n",
              "30    block2_pool1     0    10\n",
              "31    block2_pool1     2    11\n",
              "32  before_softmax     4     0\n",
              "33  before_softmax     4     1\n",
              "34  before_softmax     5     2\n",
              "35  before_softmax     5     3\n",
              "36  before_softmax     2     4\n",
              "37  before_softmax     5     5\n",
              "38  before_softmax     5     6\n",
              "39  before_softmax     4     7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNk4OB6Pqq7v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "bb4de609-4a0d-4a3a-bc11-bbf3fbe78ea0"
      },
      "source": [
        "import pylab as pl\n",
        "\n",
        "#thres = 0.1\n",
        "#bools = df1_scale > thres\n",
        "bools.sum().hist()\n",
        "pl.xlabel(\"activation time\")\n",
        "pl.ylabel(\"number of neurons\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'number of neurons')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/j+KIVr8p5w3lQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9i776jSEOTn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "41d9bbf4-e169-4449-d35f-184e28c04f82"
      },
      "source": [
        "test = bools.sum()\n",
        "print(type(test))\n",
        "print(test[0])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRhR6N3GEi9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "ed9b643f-3deb-4acb-9513-a1d8fc19b322"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "print(test.index[0],test.index[0][0])\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(12, 8))\n",
        "test.plot(kind=\"bar\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('block1_conv1', 0) block1_conv1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f87581e8630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArkAAAIqCAYAAAAgrFJZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDPvC0JSHVpj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "49907366-13df-4da2-93a8-0aaa597ea585"
      },
      "source": [
        "xs = test.index\n",
        "xs"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([  ('block1_conv1', 0),   ('block1_conv1', 1),   ('block1_conv1', 2),\n",
              "         ('block1_conv1', 3),   ('block1_pool1', 0),   ('block1_pool1', 1),\n",
              "         ('block1_pool1', 2),   ('block1_pool1', 3),   ('block2_conv1', 0),\n",
              "         ('block2_conv1', 1),   ('block2_conv1', 2),   ('block2_conv1', 3),\n",
              "         ('block2_conv1', 4),   ('block2_conv1', 5),   ('block2_conv1', 6),\n",
              "         ('block2_conv1', 7),   ('block2_conv1', 8),   ('block2_conv1', 9),\n",
              "        ('block2_conv1', 10),  ('block2_conv1', 11),   ('block2_pool1', 0),\n",
              "         ('block2_pool1', 1),   ('block2_pool1', 2),   ('block2_pool1', 3),\n",
              "         ('block2_pool1', 4),   ('block2_pool1', 5),   ('block2_pool1', 6),\n",
              "         ('block2_pool1', 7),   ('block2_pool1', 8),   ('block2_pool1', 9),\n",
              "        ('block2_pool1', 10),  ('block2_pool1', 11), ('before_softmax', 0),\n",
              "       ('before_softmax', 1), ('before_softmax', 2), ('before_softmax', 3),\n",
              "       ('before_softmax', 4), ('before_softmax', 5), ('before_softmax', 6),\n",
              "       ('before_softmax', 7), ('before_softmax', 8), ('before_softmax', 9),\n",
              "          ('predictions', 0),    ('predictions', 1),    ('predictions', 2),\n",
              "          ('predictions', 3),    ('predictions', 4),    ('predictions', 5),\n",
              "          ('predictions', 6),    ('predictions', 7),    ('predictions', 8),\n",
              "          ('predictions', 9)],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo-WxIcnOzEz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "9f4444b7-85e5-4adb-9ed9-eb91104c0c66"
      },
      "source": [
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "df1_scale"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(block1_conv1, 0)</th>\n",
              "      <th>(block1_conv1, 1)</th>\n",
              "      <th>(block1_conv1, 2)</th>\n",
              "      <th>(block1_conv1, 3)</th>\n",
              "      <th>(block1_pool1, 0)</th>\n",
              "      <th>(block1_pool1, 1)</th>\n",
              "      <th>(block1_pool1, 2)</th>\n",
              "      <th>(block1_pool1, 3)</th>\n",
              "      <th>(block2_conv1, 0)</th>\n",
              "      <th>(block2_conv1, 1)</th>\n",
              "      <th>(block2_conv1, 2)</th>\n",
              "      <th>(block2_conv1, 3)</th>\n",
              "      <th>(block2_conv1, 4)</th>\n",
              "      <th>(block2_conv1, 5)</th>\n",
              "      <th>(block2_conv1, 6)</th>\n",
              "      <th>(block2_conv1, 7)</th>\n",
              "      <th>(block2_conv1, 8)</th>\n",
              "      <th>(block2_conv1, 9)</th>\n",
              "      <th>(block2_conv1, 10)</th>\n",
              "      <th>(block2_conv1, 11)</th>\n",
              "      <th>(block2_pool1, 0)</th>\n",
              "      <th>(block2_pool1, 1)</th>\n",
              "      <th>(block2_pool1, 2)</th>\n",
              "      <th>(block2_pool1, 3)</th>\n",
              "      <th>(block2_pool1, 4)</th>\n",
              "      <th>(block2_pool1, 5)</th>\n",
              "      <th>(block2_pool1, 6)</th>\n",
              "      <th>(block2_pool1, 7)</th>\n",
              "      <th>(block2_pool1, 8)</th>\n",
              "      <th>(block2_pool1, 9)</th>\n",
              "      <th>(block2_pool1, 10)</th>\n",
              "      <th>(block2_pool1, 11)</th>\n",
              "      <th>(before_softmax, 0)</th>\n",
              "      <th>(before_softmax, 1)</th>\n",
              "      <th>(before_softmax, 2)</th>\n",
              "      <th>(before_softmax, 3)</th>\n",
              "      <th>(before_softmax, 4)</th>\n",
              "      <th>(before_softmax, 5)</th>\n",
              "      <th>(before_softmax, 6)</th>\n",
              "      <th>(before_softmax, 7)</th>\n",
              "      <th>(before_softmax, 8)</th>\n",
              "      <th>(before_softmax, 9)</th>\n",
              "      <th>(predictions, 0)</th>\n",
              "      <th>(predictions, 1)</th>\n",
              "      <th>(predictions, 2)</th>\n",
              "      <th>(predictions, 3)</th>\n",
              "      <th>(predictions, 4)</th>\n",
              "      <th>(predictions, 5)</th>\n",
              "      <th>(predictions, 6)</th>\n",
              "      <th>(predictions, 7)</th>\n",
              "      <th>(predictions, 8)</th>\n",
              "      <th>(predictions, 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9_1</th>\n",
              "      <td>0.344</td>\n",
              "      <td>0.349</td>\n",
              "      <td>0.301</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.382</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.195</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.259</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.919</td>\n",
              "      <td>0.671</td>\n",
              "      <td>0.523</td>\n",
              "      <td>0.989</td>\n",
              "      <td>0.646</td>\n",
              "      <td>0.528</td>\n",
              "      <td>0.145</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.656</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_2</th>\n",
              "      <td>0.618</td>\n",
              "      <td>0.627</td>\n",
              "      <td>0.541</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.659</td>\n",
              "      <td>0.661</td>\n",
              "      <td>0.579</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.197</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.224</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.512</td>\n",
              "      <td>0.743</td>\n",
              "      <td>0.688</td>\n",
              "      <td>0.371</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.502</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_3</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.389</td>\n",
              "      <td>0.152</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.009</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.704</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.883</td>\n",
              "      <td>0.332</td>\n",
              "      <td>0.716</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.586</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_4</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.126</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.756</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.715</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_5</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.288</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.804</td>\n",
              "      <td>0.746</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.785</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.608</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     (block1_conv1, 0)  (block1_conv1, 1)  ...  (predictions, 8)  (predictions, 9)\n",
              "9_1              0.344              0.349  ...             1.000             0.000\n",
              "9_2              0.618              0.627  ...             1.000             0.000\n",
              "9_3              0.000              0.000  ...             0.000             0.000\n",
              "9_4              0.000              0.000  ...             0.000             0.000\n",
              "9_5              0.000              0.000  ...             0.000             0.000\n",
              "\n",
              "[5 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E4092HQvN73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}