{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "DeepXplore_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshii/de4test/blob/test_ozawa/DeepXplore/DeepXplore_03_05_coverage_dependence_per_neuron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHbF5GaCQlkP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVFjZpUIRF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 共通で使うパスなどの定義\n",
        "# 共通の変数設定\n",
        "# 共通フォルダパス\n",
        "data_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data\"\n",
        "#data_imagenet = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet\"\n",
        "data_mnist = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/MNIST\"\n",
        "#data_imagenet_seeds = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet/seeds\"\n",
        "model_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/model\"\n",
        "output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output\"\n",
        "tmp_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/tmp\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4lP4POGn5Xj",
        "colab_type": "code",
        "outputId": "6375c71d-8227-475d-c3a5-7b4e9db9e081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Goggle Drive つなぐ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3d3Suv3Q5Db",
        "colab_type": "code",
        "outputId": "0b70308e-a889-4d6f-fd26-8ea987ff5cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# outputフォルダ内容物のクリーンアップ削除\n",
        "!rm \"$output_dir\"/*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20191213_2030': Is a directory\n",
            "rm: cannot remove '/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20191213_2045': Is a directory\n",
            "rm: cannot remove '/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20191214_1430': Is a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdHYnLT0Nbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST')\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDY8sDVBqxmK",
        "colab_type": "code",
        "outputId": "3b810357-3cf9-4cda-c113-f13afe87bd63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input\n",
        "import imageio\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "import os"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiGGwQVElJNH",
        "colab_type": "code",
        "outputId": "e8124845-82a7-4bfc-f419-7ce095673ec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 15221544471963002701, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 8433521872464817470\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 11242747213186908294\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15956161332\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 14078255420682904377\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQ2M_8glJNM",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "fbcbb2e3-e0bb-4f40-c2a3-aa669bd1080c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "# DeepXplore のパラメータ設定部\n",
        "# read the parameter\n",
        "# argument parsing\n",
        "parser = argparse.ArgumentParser(description='Main function for difference-inducing input generation in MNIST dataset')\n",
        "parser.add_argument('transformation', help=\"realistic transformation type\", choices=['light', 'occl', 'blackout'])\n",
        "parser.add_argument('weight_diff', help=\"weight hyperparm to control differential behavior\", type=float)\n",
        "parser.add_argument('weight_nc', help=\"weight hyperparm to control neuron coverage\", type=float)\n",
        "parser.add_argument('step', help=\"step size of gradient descent\", type=float)\n",
        "parser.add_argument('seeds', help=\"number of seeds of input\", type=int)\n",
        "parser.add_argument('grad_iterations', help=\"number of iterations of gradient descent\", type=int)\n",
        "parser.add_argument('threshold', help=\"threshold for determining neuron activated\", type=float)\n",
        "parser.add_argument('-t', '--target_model', help=\"target model that we want it predicts differently\",\n",
        "                    choices=[0, 1, 2], default=0, type=int)\n",
        "parser.add_argument('-sp', '--start_point', help=\"occlusion upper left corner coordinate\", default=(0, 0), type=tuple)\n",
        "parser.add_argument('-occl_size', '--occlusion_size', help=\"occlusion size\", default=(10, 10), type=tuple)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-occl_size', '--occlusion_size'], dest='occlusion_size', nargs=None, const=None, default=(10, 10), type=<class 'tuple'>, choices=None, help='occlusion size', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Csx_IXrMdMj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93KW-VGjMzun",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title ノイズのタイプ light / occl / blackout\n",
        "#@body {light,occl,blackout} weight_diff weight_nc step seeds\n",
        "#                   grad_iterations threshold\n",
        "noise_type = \"light\" #@param [\"light\", \"occl\", \"blackout\"]\n",
        "weight_diff = \"0.1\" #@param {type:\"string\"}\n",
        "weight_nc = \"0.1\" #@param {type:\"string\"}\n",
        "step = \"20\" #@param {type:\"string\"}　#50だとほぼ白飛び\n",
        "seeds = \"100\" #@param {type:\"string\"}\n",
        "grad_iterations = \"10\" #@param {type:\"string\"}\n",
        "threshold = \"0.1\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk80fCuPts9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = parser.parse_args([noise_type, weight_diff, weight_nc, step, seeds, grad_iterations, threshold])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tTYrllplJNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x.reshape(x.shape[1], x.shape[2])  # original shape (1,img_rows, img_cols,1)\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "def constraint_occl(gradients, start_point, rect_shape):\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def constraint_light(gradients):\n",
        "    new_grads = np.ones_like(gradients)\n",
        "    grad_mean = np.mean(gradients)\n",
        "    return grad_mean * new_grads\n",
        "\n",
        "\n",
        "def constraint_black(gradients, rect_shape=(6, 6)):\n",
        "    start_point = (\n",
        "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    if np.mean(patch) < 0:\n",
        "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def init_coverage_tables(model1, model2, model3):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    model_layer_dict2 = defaultdict(bool)\n",
        "    model_layer_dict3 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    init_dict(model2, model_layer_dict2)\n",
        "    init_dict(model3, model_layer_dict3)\n",
        "    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n",
        "\n",
        "\n",
        "def init_dict(model, model_layer_dict):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_dict[(layer.name, index)] = False\n",
        "\n",
        "\n",
        "def neuron_to_cover(model_layer_dict):\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "    else:\n",
        "        layer_name, index = random.choice(model_layer_dict.keys())\n",
        "    return layer_name, index\n",
        "\n",
        "\n",
        "def neuron_covered(model_layer_dict):\n",
        "    covered_neurons = len([v for v in model_layer_dict.values() if v])\n",
        "    total_neurons = len(model_layer_dict)\n",
        "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n",
        "\n",
        "\n",
        "def update_coverage(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n",
        "                model_layer_dict[(layer_names[i], num_neuron)] = True\n",
        "\n",
        "\n",
        "def full_coverage(model_layer_dict):\n",
        "    if False in model_layer_dict.values():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
        "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
        "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
        "    X_scaled = X_std * (rmax - rmin) + rmin\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def fired(model, layer_name, index, input_data, threshold=0):\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
        "    scaled = scale(intermediate_layer_output)\n",
        "    if np.mean(scaled[..., index]) > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def diverged(predictions1, predictions2, predictions3, target):\n",
        "    #     if predictions2 == predictions3 == target and predictions1 != target:\n",
        "    if not predictions1 == predictions2 == predictions3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "#add\n",
        "def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n",
        "    return [os.path.join(root, f)\n",
        "            for root, _, files in os.walk(directory) for f in files\n",
        "            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoRphb5Fl0JT",
        "colab_type": "code",
        "outputId": "bbd9b5b6-64f8-4a92-9987-e0ddab7f364a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "# the data, shuffled and split between train and test sets\n",
        "(_, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "\n",
        "# define input tensor as a placeholder\n",
        "input_tensor = Input(shape=input_shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDDT47Er6u23",
        "colab_type": "code",
        "outputId": "401451b4-ceb5-429c-fe4e-a26d0a3043b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "'''\n",
        "LeNet-1\n",
        "'''\n",
        "def Model1(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        print(x_train.shape)\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(4, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(12, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "#        model.save_weights('./Model1.h5')\n",
        "        model.save_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "#        model.load_weights('./Model1.h5')\n",
        "        model.load_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        print(bcolors.OKBLUE + 'Model1 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model1(train=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.4176 - acc: 0.8724 - val_loss: 0.1280 - val_acc: 0.9614\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.1057 - acc: 0.9685 - val_loss: 0.0766 - val_acc: 0.9768\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.0793 - acc: 0.9759 - val_loss: 0.0656 - val_acc: 0.9790\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.0654 - acc: 0.9802 - val_loss: 0.0628 - val_acc: 0.9789\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.0577 - acc: 0.9823 - val_loss: 0.0521 - val_acc: 0.9826\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.0525 - acc: 0.9837 - val_loss: 0.0475 - val_acc: 0.9838\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.0479 - acc: 0.9853 - val_loss: 0.0427 - val_acc: 0.9862\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.0448 - acc: 0.9865 - val_loss: 0.0479 - val_acc: 0.9848\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 1s 19us/step - loss: 0.0419 - acc: 0.9873 - val_loss: 0.0420 - val_acc: 0.9861\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 1s 20us/step - loss: 0.0396 - acc: 0.9882 - val_loss: 0.0416 - val_acc: 0.9858\n",
            "\n",
            "\n",
            "Overall Test score: 0.04158401021489408\n",
            "Overall Test accuracy: 0.9858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU88awZf68JO",
        "colab_type": "code",
        "outputId": "e536d4ec-4dc4-48b9-a1a8-8a5044f98fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "'''\n",
        "LeNet-4\n",
        "'''\n",
        "def Model2(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(84, activation='relu', name='fc1')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model2.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model2.h5')\n",
        "        print(bcolors.OKBLUE + 'Model2 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model2(train=True)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.3743 - acc: 0.8848 - val_loss: 0.1300 - val_acc: 0.9590\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 1s 22us/step - loss: 0.0936 - acc: 0.9709 - val_loss: 0.0860 - val_acc: 0.9703\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 1s 22us/step - loss: 0.0664 - acc: 0.9789 - val_loss: 0.0590 - val_acc: 0.9813\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0520 - acc: 0.9831 - val_loss: 0.0538 - val_acc: 0.9818\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 22us/step - loss: 0.0430 - acc: 0.9867 - val_loss: 0.0466 - val_acc: 0.9842\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0360 - acc: 0.9885 - val_loss: 0.0393 - val_acc: 0.9864\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 1s 22us/step - loss: 0.0309 - acc: 0.9899 - val_loss: 0.0789 - val_acc: 0.9731\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0270 - acc: 0.9916 - val_loss: 0.0352 - val_acc: 0.9881\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0239 - acc: 0.9926 - val_loss: 0.0642 - val_acc: 0.9793\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0206 - acc: 0.9935 - val_loss: 0.0345 - val_acc: 0.9895\n",
            "\n",
            "\n",
            "Overall Test score: 0.034486668376615855\n",
            "Overall Test accuracy: 0.9895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufs67uRN7KYL",
        "colab_type": "code",
        "outputId": "4e8c71b0-42d8-4a5f-c3e2-3da0322b5fe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "'''\n",
        "LeNet-5\n",
        "'''\n",
        "def Model3(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(120, activation='relu', name='fc1')(x)\n",
        "    x = Dense(84, activation='relu', name='fc2')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model3.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model3.h5')\n",
        "        print(bcolors.OKBLUE + 'Model3 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model3(train=True)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.3994 - acc: 0.8724 - val_loss: 0.1010 - val_acc: 0.9696\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0873 - acc: 0.9728 - val_loss: 0.0664 - val_acc: 0.9797\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0606 - acc: 0.9815 - val_loss: 0.1222 - val_acc: 0.9603\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0458 - acc: 0.9858 - val_loss: 0.0463 - val_acc: 0.9861\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0379 - acc: 0.9882 - val_loss: 0.0362 - val_acc: 0.9884\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0327 - acc: 0.9899 - val_loss: 0.0303 - val_acc: 0.9908\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0264 - acc: 0.9919 - val_loss: 0.0398 - val_acc: 0.9880\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0228 - acc: 0.9927 - val_loss: 0.0555 - val_acc: 0.9813\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0196 - acc: 0.9938 - val_loss: 0.0344 - val_acc: 0.9904\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0163 - acc: 0.9952 - val_loss: 0.0352 - val_acc: 0.9896\n",
            "\n",
            "\n",
            "Overall Test score: 0.03522895187438098\n",
            "Overall Test accuracy: 0.9896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9QiwlW7WWR",
        "colab_type": "code",
        "outputId": "eae0cf17-0377-4191-abb6-077e4ab6e694",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model1 = Model1(input_tensor=input_tensor)\n",
        "model2 = Model2(input_tensor=input_tensor)\n",
        "model3 = Model3(input_tensor=input_tensor)\n",
        "model_layer_dict1, model_layer_dict2, model_layer_dict3 = init_coverage_tables(model1, model2, model3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[94mModel1 loaded\u001b[0m\n",
            "\u001b[94mModel2 loaded\u001b[0m\n",
            "\u001b[94mModel3 loaded\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD6cHroNl43Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start gen inputs\n",
        "# img_paths = list_pictures(data_imagenet_seeds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brFIdSfL8nHP",
        "colab_type": "code",
        "outputId": "9a3cf154-dff0-4a7f-8f70-7f3a1100c1ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "test_per_fig_x = np.array([])\n",
        "test_per_fig_y = np.array([])\n",
        "tests_x = np.array([])\n",
        "tests_y = np.array([])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "#x_train = x_train.astype('float')\n",
        "#x_test = x_test.astype('float')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "length = int(args.seeds/10)\n",
        "for i in range(10):\n",
        "  cond = [(x==i) for x in y_test]\n",
        "  test_per_fig_x = x_test[cond]\n",
        "  test_per_fig_y = y_test[cond]\n",
        "  print(i, test_per_fig_x.shape, test_per_fig_x.shape[0]-1000)\n",
        "  tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "  tests_y = np.append(tests_y, test_per_fig_y[:length])\n",
        "#  conds = [conds, cond]\n",
        "print(\"check!\", tests_x.shape[0]/28/28/10, \"=\", length, \"equal?\")\n",
        "tests_x = tests_x.reshape(-1,28,28,1)\n",
        "#tests_x = tests_x.reshape(-1,1,28,28,1,)\n",
        "tests_x.shape\n",
        "tests_x = tests_x.astype('float32')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 (980, 28, 28) -20\n",
            "1 (1135, 28, 28) 135\n",
            "2 (1032, 28, 28) 32\n",
            "3 (1010, 28, 28) 10\n",
            "4 (982, 28, 28) -18\n",
            "5 (892, 28, 28) -108\n",
            "6 (958, 28, 28) -42\n",
            "7 (1028, 28, 28) 28\n",
            "8 (974, 28, 28) -26\n",
            "9 (1009, 28, 28) 9\n",
            "check! 10.0 = 10 equal?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyo2SSSz8vkT",
        "colab_type": "code",
        "outputId": "4a622852-c5b7-4d85-f303-64b3c94096e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfdTZi4mUPf",
        "colab_type": "code",
        "outputId": "b5e43520-6f7a-4656-a330-4f366340b30b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "def neuron_output(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "    \n",
        "    out_list = []\n",
        "    out_list_scale = []\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            out_list.append(np.mean(intermediate_layer_output[..., num_neuron]))\n",
        "            out_list_scale.append(np.mean(scaled[..., num_neuron]))\n",
        "\n",
        "    return out_list, out_list_scale\n",
        "\n",
        "\n",
        "count_already = 0\n",
        "count_found = 0\n",
        "count_not_found = 0\n",
        "temp_per_nc1 = np.array([])\n",
        "temp_per_nc2 = np.array([])\n",
        "temp_per_nc3 = np.array([])\n",
        "temp_num_nc1 = np.array([])\n",
        "temp_num_nc2 = np.array([])\n",
        "temp_num_nc3 = np.array([])\n",
        "\n",
        "#for each neuron\n",
        "num_neurons1 = neuron_covered(model_layer_dict1)[1]\n",
        "num_neurons2 = neuron_covered(model_layer_dict2)[1]\n",
        "num_neurons3 = neuron_covered(model_layer_dict3)[1]\n",
        "\n",
        "column_tmp1 = list(model_layer_dict1.keys())\n",
        "column_tmp2 = list(model_layer_dict2.keys())\n",
        "column_tmp3 = list(model_layer_dict3.keys())\n",
        "df1 = pd.DataFrame(columns=column_tmp1)\n",
        "df2 = pd.DataFrame(columns=column_tmp2)\n",
        "df3 = pd.DataFrame(columns=column_tmp3)\n",
        "df1_scale = pd.DataFrame(columns=column_tmp1)\n",
        "df2_scale = pd.DataFrame(columns=column_tmp2)\n",
        "df3_scale = pd.DataFrame(columns=column_tmp3)\n",
        "trial = 0\n",
        "\n",
        "for index_fig in range(10):\n",
        "  print(\"figure\"+str(index_fig))\n",
        "  for _ in range(length):\n",
        "      #gen_img = np.expand_dims(random.choice(tests_x), axis=0)\n",
        "      gen_img = np.expand_dims(tests_x[(length*index_fig + _)], axis=0)\n",
        "      orig_img = gen_img.copy()\n",
        "      # first check if input already induces differences\n",
        "      label1, label2, label3 = np.argmax(model1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "      if not label1 == label2 == label3:\n",
        "          count_already += 1\n",
        "          print(bcolors.OKGREEN + '{}/{}. input already causes different outputs ({},{},{}) at{}/{}: '.format(_, length, label1, label2, label3, count_already, count_already + count_found + count_not_found) + bcolors.ENDC)        \n",
        "          #print(bcolors.OKGREEN + '{}/{}. input already causes different outputs ({},{},{}) at{}/{}: '.format(_, length, label1, label2, label3, count_already, count_already + count_found + count_not_found) + bcolors.ENDC)        \n",
        "\n",
        "          update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "          temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "          temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "          temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "          temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "          temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "          temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "                               \n",
        "          print(bcolors.OKGREEN + '     covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'% (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  neuron_covered(model_layer_dict2)[2], len(model_layer_dict3), neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "          averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                       neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +neuron_covered(model_layer_dict3)[1])\n",
        "          print(bcolors.OKGREEN + '     averaged covered neurons %.3f' % (averaged_nc) + bcolors.ENDC)\n",
        "\n",
        "          gen_img_deprocessed = deprocess_image(gen_img)\n",
        "\n",
        "          # save the result to disk\n",
        "          outputfilepath0 = os.path.join(output_dir, 'already_differ_' + str(label1) + '_' + str(label2) + '_' + str(label3) +'_['+ str(_) +  '].png')\n",
        "          imageio.imwrite(outputfilepath0, gen_img_deprocessed)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          temp = pd.Series(temp, index=df1.columns, name=str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(trial))\n",
        "          df1 = df1.append(temp)\n",
        "          df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          temp = pd.Series(temp, index=df2.columns, name=str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(trial))\n",
        "          df2 = df2.append(temp)\n",
        "          df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "          temp = pd.Series(temp, index=df3.columns, name=str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(trial))\n",
        "\n",
        "          df3 = df3.append(temp)\n",
        "          df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "          trial += 1\n",
        "          continue\n",
        "\n",
        "      # if all label agrees\n",
        "      orig_label = label1\n",
        "      layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n",
        "      layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n",
        "      layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n",
        "\n",
        "      # construct joint loss function\n",
        "      if args.target_model == 0:\n",
        "        loss1 = -args.weight_diff * K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 1:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = -args.weight_diff * K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 2:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n",
        "      loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n",
        "      loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n",
        "      layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron)\n",
        "\n",
        "      # for adversarial image generation\n",
        "      final_loss = K.mean(layer_output)\n",
        "\n",
        "      # we compute the gradient of the input picture wrt this loss\n",
        "      grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "\n",
        "      # this function returns the loss and grads given the input picture\n",
        "      iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n",
        "\n",
        "      # we run gradient ascent for some steps\n",
        "      for iters in range(args.grad_iterations):\n",
        "        loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = iterate(\n",
        "            [gen_img])\n",
        "        if args.transformation == 'light':\n",
        "            grads_value = constraint_light(grads_value)  # constraint the gradients value\n",
        "        elif args.transformation == 'occl':\n",
        "            grads_value = constraint_occl(grads_value, args.start_point,\n",
        "                                          args.occlusion_size)  # constraint the gradients value\n",
        "        elif args.transformation == 'blackout':\n",
        "            grads_value = constraint_black(grads_value)  # constraint the gradients value\n",
        "\n",
        "        gen_img += grads_value * args.step\n",
        "        predictions1 = np.argmax(model1.predict(gen_img)[0])\n",
        "        predictions2 = np.argmax(model2.predict(gen_img)[0])\n",
        "        predictions3 = np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "        if not predictions1 == predictions2 == predictions3:\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            count_found += 1\n",
        "            print(bcolors.OKBLUE + '%4d/%d. found at %d! covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f at %d/%d'\n",
        "#                  % (_, args.seeds, iters, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  % (_, length, iters, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                     neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                     neuron_covered(model_layer_dict3)[2], count_found, count_already + count_found + count_not_found) + bcolors.ENDC)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[\n",
        "                    1])\n",
        "            print(bcolors.OKBLUE + '     averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  '].png')\n",
        "            #print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "#            outputfilepath2 = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '_orig.png')\n",
        "            outputfilepath2 = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  ']_orig.png')\n",
        "            #print(outputfilepath2)\n",
        "            imageio.imwrite(outputfilepath2, orig_img_deprocessed)\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(trial))\n",
        "\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "            trial += 1\n",
        "            break\n",
        "          \n",
        "          #add\n",
        "        if iters == (args.grad_iterations-1):\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            count_not_found += 1\n",
        "#              print('%4d/%d. test suite was not found: averaged covered neurons %.3f at %d/%d' % (_, args.seeds, averaged_nc, count_not_found, count_already + count_found + count_not_found))\n",
        "            print('%4d/%d. test suite was not found: averaged covered neurons %.3f at %d/%d' % (_, length, averaged_nc, count_not_found, count_already + count_found + count_not_found))\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            #orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, 'not_found_' + str(label1)+'_['+ str(_) + '].png')\n",
        "#           print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "\n",
        "            \n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(trial))\n",
        "\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "            trial += 1\n",
        "            \n",
        "            #break\n",
        "\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "temp_per_nc1=temp_per_nc1.reshape(10, length)\n",
        "temp_per_nc2=temp_per_nc2.reshape(10, length)\n",
        "temp_per_nc3=temp_per_nc3.reshape(10, length)\n",
        "temp_num_nc1=temp_num_nc1.reshape(10, length)\n",
        "temp_num_nc2=temp_num_nc2.reshape(10, length)\n",
        "temp_num_nc3=temp_num_nc3.reshape(10, length)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure0\n",
            "\u001b[94m   0/10. found at 1! covered neurons percentage 52 neurons 0.538, 148 neurons 0.426, 268 neurons 0.403 at 1/1\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.425\u001b[0m\n",
            "\u001b[94m   1/10. found at 2! covered neurons percentage 52 neurons 0.596, 148 neurons 0.500, 268 neurons 0.459 at 2/2\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.487\u001b[0m\n",
            "\u001b[94m   2/10. found at 4! covered neurons percentage 52 neurons 0.635, 148 neurons 0.514, 268 neurons 0.481 at 3/3\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.509\u001b[0m\n",
            "\u001b[94m   3/10. found at 2! covered neurons percentage 52 neurons 0.635, 148 neurons 0.554, 268 neurons 0.515 at 4/4\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.541\u001b[0m\n",
            "\u001b[94m   4/10. found at 3! covered neurons percentage 52 neurons 0.635, 148 neurons 0.554, 268 neurons 0.515 at 5/5\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.541\u001b[0m\n",
            "\u001b[94m   5/10. found at 2! covered neurons percentage 52 neurons 0.654, 148 neurons 0.561, 268 neurons 0.526 at 6/6\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.551\u001b[0m\n",
            "\u001b[94m   6/10. found at 1! covered neurons percentage 52 neurons 0.827, 148 neurons 0.682, 268 neurons 0.746 at 7/7\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.735\u001b[0m\n",
            "\u001b[94m   7/10. found at 2! covered neurons percentage 52 neurons 0.827, 148 neurons 0.689, 268 neurons 0.746 at 8/8\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.737\u001b[0m\n",
            "\u001b[94m   8/10. found at 2! covered neurons percentage 52 neurons 0.827, 148 neurons 0.689, 268 neurons 0.746 at 9/9\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.737\u001b[0m\n",
            "\u001b[94m   9/10. found at 0! covered neurons percentage 52 neurons 0.846, 148 neurons 0.696, 268 neurons 0.754 at 10/10\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.746\u001b[0m\n",
            "figure1\n",
            "\u001b[94m   0/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.797, 268 neurons 0.810 at 11/11\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.816\u001b[0m\n",
            "\u001b[94m   1/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.811, 268 neurons 0.810 at 12/12\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.821\u001b[0m\n",
            "\u001b[94m   2/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.831, 268 neurons 0.817 at 13/13\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.831\u001b[0m\n",
            "\u001b[94m   3/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.858, 268 neurons 0.828 at 14/14\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.846\u001b[0m\n",
            "\u001b[94m   4/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.858, 268 neurons 0.828 at 15/15\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.846\u001b[0m\n",
            "\u001b[94m   5/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.858, 268 neurons 0.836 at 16/16\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.850\u001b[0m\n",
            "\u001b[94m   6/10. found at 3! covered neurons percentage 52 neurons 0.904, 148 neurons 0.858, 268 neurons 0.836 at 17/17\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.850\u001b[0m\n",
            "\u001b[94m   7/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.858, 268 neurons 0.836 at 18/18\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.850\u001b[0m\n",
            "\u001b[94m   8/10. found at 5! covered neurons percentage 52 neurons 0.904, 148 neurons 0.865, 268 neurons 0.836 at 19/19\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.853\u001b[0m\n",
            "\u001b[94m   9/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.865, 268 neurons 0.840 at 20/20\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.855\u001b[0m\n",
            "figure2\n",
            "\u001b[94m   0/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.872, 268 neurons 0.843 at 21/21\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.859\u001b[0m\n",
            "\u001b[94m   1/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.872, 268 neurons 0.847 at 22/22\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.861\u001b[0m\n",
            "\u001b[94m   2/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.872, 268 neurons 0.847 at 23/23\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.861\u001b[0m\n",
            "\u001b[94m   3/10. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.872, 268 neurons 0.858 at 24/24\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.868\u001b[0m\n",
            "\u001b[94m   4/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.872, 268 neurons 0.858 at 25/25\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.868\u001b[0m\n",
            "\u001b[94m   5/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.872, 268 neurons 0.858 at 26/26\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.868\u001b[0m\n",
            "\u001b[94m   6/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.872, 268 neurons 0.858 at 27/27\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.868\u001b[0m\n",
            "\u001b[94m   7/10. found at 6! covered neurons percentage 52 neurons 0.904, 148 neurons 0.872, 268 neurons 0.858 at 28/28\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.868\u001b[0m\n",
            "\u001b[94m   8/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.872, 268 neurons 0.858 at 29/29\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.868\u001b[0m\n",
            "\u001b[94m   9/10. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.872, 268 neurons 0.862 at 30/30\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.870\u001b[0m\n",
            "figure3\n",
            "\u001b[94m   0/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.872, 268 neurons 0.869 at 31/31\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.876\u001b[0m\n",
            "\u001b[94m   1/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.872, 268 neurons 0.869 at 32/32\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.876\u001b[0m\n",
            "\u001b[94m   2/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.872, 268 neurons 0.869 at 33/33\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.876\u001b[0m\n",
            "\u001b[94m   3/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.872, 268 neurons 0.869 at 34/34\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.876\u001b[0m\n",
            "\u001b[94m   4/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.872, 268 neurons 0.869 at 35/35\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.876\u001b[0m\n",
            "\u001b[94m   5/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.872, 268 neurons 0.877 at 36/36\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.880\u001b[0m\n",
            "\u001b[94m   6/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.872, 268 neurons 0.877 at 37/37\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.880\u001b[0m\n",
            "\u001b[94m   7/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.872, 268 neurons 0.877 at 38/38\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.880\u001b[0m\n",
            "\u001b[94m   8/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.872, 268 neurons 0.877 at 39/39\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.880\u001b[0m\n",
            "\u001b[94m   9/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.872, 268 neurons 0.877 at 40/40\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.880\u001b[0m\n",
            "figure4\n",
            "\u001b[94m   0/10. found at 4! covered neurons percentage 52 neurons 0.923, 148 neurons 0.878, 268 neurons 0.877 at 41/41\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.882\u001b[0m\n",
            "\u001b[94m   1/10. found at 7! covered neurons percentage 52 neurons 0.923, 148 neurons 0.885, 268 neurons 0.877 at 42/42\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.885\u001b[0m\n",
            "\u001b[94m   2/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.877 at 43/43\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.887\u001b[0m\n",
            "\u001b[94m   3/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.881 at 44/44\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.889\u001b[0m\n",
            "\u001b[94m   4/10. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.881 at 45/45\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.889\u001b[0m\n",
            "\u001b[94m   5/10. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.881 at 46/46\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.889\u001b[0m\n",
            "\u001b[94m   6/10. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.881 at 47/47\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.889\u001b[0m\n",
            "\u001b[94m   7/10. found at 3! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.884 at 48/48\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.891\u001b[0m\n",
            "\u001b[94m   8/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.892 at 49/49\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.895\u001b[0m\n",
            "\u001b[94m   9/10. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.892 at 50/50\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.895\u001b[0m\n",
            "figure5\n",
            "   0/10. test suite was not found: averaged covered neurons 0.900 at 1/51\n",
            "\u001b[94m   1/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.899 at 51/52\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.900\u001b[0m\n",
            "\u001b[94m   2/10. found at 6! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.899 at 52/53\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.900\u001b[0m\n",
            "\u001b[94m   3/10. found at 3! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.903 at 53/54\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.902\u001b[0m\n",
            "\u001b[94m   4/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.903 at 54/55\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.902\u001b[0m\n",
            "\u001b[94m   5/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.903 at 55/56\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.902\u001b[0m\n",
            "\u001b[94m   6/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.903 at 56/57\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.902\u001b[0m\n",
            "   7/10. test suite was not found: averaged covered neurons 0.904 at 2/58\n",
            "\u001b[94m   8/10. found at 8! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.907 at 57/59\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.904\u001b[0m\n",
            "\u001b[94m   9/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.907 at 58/60\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.904\u001b[0m\n",
            "figure6\n",
            "\u001b[94m   0/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.910 at 59/61\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.906\u001b[0m\n",
            "\u001b[94m   1/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.910 at 60/62\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.906\u001b[0m\n",
            "\u001b[94m   2/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.910 at 61/63\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.906\u001b[0m\n",
            "\u001b[94m   3/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.910 at 62/64\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.906\u001b[0m\n",
            "\u001b[94m   4/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.910 at 63/65\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.906\u001b[0m\n",
            "\u001b[94m   5/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.910 at 64/66\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.906\u001b[0m\n",
            "\u001b[94m   6/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.910 at 65/67\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.908\u001b[0m\n",
            "\u001b[94m   7/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.910 at 66/68\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.908\u001b[0m\n",
            "\u001b[94m   8/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.910 at 67/69\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.908\u001b[0m\n",
            "\u001b[94m   9/10. found at 0! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.910 at 68/70\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.908\u001b[0m\n",
            "figure7\n",
            "\u001b[94m   0/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 69/71\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   1/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 70/72\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   2/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 71/73\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   3/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 72/74\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   4/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 73/75\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   5/10. found at 0! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 74/76\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   6/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 75/77\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   7/10. found at 3! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 76/78\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   8/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 77/79\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   9/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 78/80\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "figure8\n",
            "\u001b[94m   0/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 79/81\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   1/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 80/82\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   2/10. found at 0! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 81/83\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   3/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 82/84\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   4/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 83/85\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   5/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.914 at 84/86\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   6/10. found at 0! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.918 at 85/87\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.912\u001b[0m\n",
            "\u001b[94m   7/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.918 at 86/88\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.912\u001b[0m\n",
            "\u001b[94m   8/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.918 at 87/89\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.912\u001b[0m\n",
            "\u001b[94m   9/10. found at 0! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.922 at 88/90\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.915\u001b[0m\n",
            "figure9\n",
            "\u001b[94m   0/10. found at 6! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.933 at 89/91\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.921\u001b[0m\n",
            "\u001b[94m   1/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.933 at 90/92\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.921\u001b[0m\n",
            "\u001b[94m   2/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.899, 268 neurons 0.933 at 91/93\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.921\u001b[0m\n",
            "\u001b[94m   3/10. found at 0! covered neurons percentage 52 neurons 0.923, 148 neurons 0.905, 268 neurons 0.933 at 92/94\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.923\u001b[0m\n",
            "\u001b[94m   4/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.905, 268 neurons 0.933 at 93/95\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.923\u001b[0m\n",
            "\u001b[94m   5/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.905, 268 neurons 0.933 at 94/96\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.923\u001b[0m\n",
            "\u001b[94m   6/10. found at 0! covered neurons percentage 52 neurons 0.923, 148 neurons 0.905, 268 neurons 0.933 at 95/97\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.923\u001b[0m\n",
            "\u001b[94m   7/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.905, 268 neurons 0.933 at 96/98\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.923\u001b[0m\n",
            "\u001b[94m   8/10. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.905, 268 neurons 0.933 at 97/99\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.923\u001b[0m\n",
            "\u001b[94m   9/10. found at 3! covered neurons percentage 52 neurons 0.923, 148 neurons 0.905, 268 neurons 0.933 at 98/100\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.923\u001b[0m\n",
            "CPU times: user 7min 55s, sys: 2.78 s, total: 7min 58s\n",
            "Wall time: 7min 58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo-WxIcnOzEz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "ccc99201-456f-44ea-c939-998920fd0600"
      },
      "source": [
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "df1_scale"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(block1_conv1, 0)</th>\n",
              "      <th>(block1_conv1, 1)</th>\n",
              "      <th>(block1_conv1, 2)</th>\n",
              "      <th>(block1_conv1, 3)</th>\n",
              "      <th>(block1_pool1, 0)</th>\n",
              "      <th>(block1_pool1, 1)</th>\n",
              "      <th>(block1_pool1, 2)</th>\n",
              "      <th>(block1_pool1, 3)</th>\n",
              "      <th>(block2_conv1, 0)</th>\n",
              "      <th>(block2_conv1, 1)</th>\n",
              "      <th>(block2_conv1, 2)</th>\n",
              "      <th>(block2_conv1, 3)</th>\n",
              "      <th>(block2_conv1, 4)</th>\n",
              "      <th>(block2_conv1, 5)</th>\n",
              "      <th>(block2_conv1, 6)</th>\n",
              "      <th>(block2_conv1, 7)</th>\n",
              "      <th>(block2_conv1, 8)</th>\n",
              "      <th>(block2_conv1, 9)</th>\n",
              "      <th>(block2_conv1, 10)</th>\n",
              "      <th>(block2_conv1, 11)</th>\n",
              "      <th>(block2_pool1, 0)</th>\n",
              "      <th>(block2_pool1, 1)</th>\n",
              "      <th>(block2_pool1, 2)</th>\n",
              "      <th>(block2_pool1, 3)</th>\n",
              "      <th>(block2_pool1, 4)</th>\n",
              "      <th>(block2_pool1, 5)</th>\n",
              "      <th>(block2_pool1, 6)</th>\n",
              "      <th>(block2_pool1, 7)</th>\n",
              "      <th>(block2_pool1, 8)</th>\n",
              "      <th>(block2_pool1, 9)</th>\n",
              "      <th>(block2_pool1, 10)</th>\n",
              "      <th>(block2_pool1, 11)</th>\n",
              "      <th>(before_softmax, 0)</th>\n",
              "      <th>(before_softmax, 1)</th>\n",
              "      <th>(before_softmax, 2)</th>\n",
              "      <th>(before_softmax, 3)</th>\n",
              "      <th>(before_softmax, 4)</th>\n",
              "      <th>(before_softmax, 5)</th>\n",
              "      <th>(before_softmax, 6)</th>\n",
              "      <th>(before_softmax, 7)</th>\n",
              "      <th>(before_softmax, 8)</th>\n",
              "      <th>(before_softmax, 9)</th>\n",
              "      <th>(predictions, 0)</th>\n",
              "      <th>(predictions, 1)</th>\n",
              "      <th>(predictions, 2)</th>\n",
              "      <th>(predictions, 3)</th>\n",
              "      <th>(predictions, 4)</th>\n",
              "      <th>(predictions, 5)</th>\n",
              "      <th>(predictions, 6)</th>\n",
              "      <th>(predictions, 7)</th>\n",
              "      <th>(predictions, 8)</th>\n",
              "      <th>(predictions, 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.115</td>\n",
              "      <td>0.602</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.647</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.522</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.199</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.915</td>\n",
              "      <td>0.124</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.329</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.940</td>\n",
              "      <td>0.819</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.135</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.571</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.613</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.553</td>\n",
              "      <td>0.538</td>\n",
              "      <td>0.893</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.996</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.105</td>\n",
              "      <td>0.540</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.583</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.646</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.463</td>\n",
              "      <td>0.798</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.680</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.895</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.110</td>\n",
              "      <td>0.553</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.605</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.492</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.775</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.434</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.796</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.121</td>\n",
              "      <td>0.640</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.509</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.680</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.462</td>\n",
              "      <td>0.586</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.810</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.904</td>\n",
              "      <td>0.823</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.131</td>\n",
              "      <td>0.694</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.552</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.730</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.589</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.510</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.904</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.971</td>\n",
              "      <td>0.995</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.040</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.549</td>\n",
              "      <td>0.737</td>\n",
              "      <td>0.757</td>\n",
              "      <td>0.208</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.883</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.117</td>\n",
              "      <td>0.616</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.653</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.422</td>\n",
              "      <td>0.666</td>\n",
              "      <td>0.873</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.628</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.242</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.937</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.123</td>\n",
              "      <td>0.651</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.518</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.685</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.245</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.397</td>\n",
              "      <td>0.672</td>\n",
              "      <td>0.882</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.744</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.097</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.124</td>\n",
              "      <td>0.658</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.523</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.692</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.556</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.371</td>\n",
              "      <td>0.690</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.767</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.985</td>\n",
              "      <td>0.916</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 52 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    (block1_conv1, 0)  (block1_conv1, 1)  ...  (predictions, 8)  (predictions, 9)\n",
              "0               0.115              0.602  ...             0.000             0.000\n",
              "1               0.135              0.718  ...             0.000             1.000\n",
              "2               0.105              0.540  ...             0.000             1.000\n",
              "3               0.110              0.553  ...             0.000             1.000\n",
              "4               0.121              0.640  ...             0.000             0.000\n",
              "..                ...                ...  ...               ...               ...\n",
              "95              0.131              0.694  ...             0.000             0.000\n",
              "96              0.040              0.000  ...             0.000             0.000\n",
              "97              0.117              0.616  ...             1.000             0.000\n",
              "98              0.123              0.651  ...             1.000             0.000\n",
              "99              0.124              0.658  ...             0.000             0.000\n",
              "\n",
              "[100 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLyQvSQN5bPM",
        "colab_type": "code",
        "outputId": "01cc06ca-f71a-4550-c1c5-277f8ebf297c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "data_per_x1 = np.average(temp_per_nc1, axis=1)\n",
        "data_per_x2 = np.average(temp_per_nc2, axis=1)\n",
        "data_per_x3 = np.average(temp_per_nc3, axis=1)\n",
        "std_per_x1 = np.std(temp_per_nc1, axis=1)\n",
        "std_per_x2 = np.std(temp_per_nc2, axis=1)\n",
        "std_per_x3 = np.std(temp_per_nc3, axis=1)\n",
        "\n",
        "print(data_per_x1, std_per_x1, std_per_x2, std_per_x3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.70192308 0.90384615 0.90384615 0.92307692 0.92307692 0.92307692\n",
            " 0.92307692 0.92307692 0.92307692 0.92307692] [0.110221 0.       0.       0.       0.       0.       0.       0.\n",
            " 0.       0.      ] [9.14935723e-02 2.29729730e-02 0.00000000e+00 0.00000000e+00\n",
            " 4.32643530e-03 1.11022302e-16 3.31012127e-03 1.11022302e-16\n",
            " 1.11022302e-16 3.09633493e-03] [1.33928197e-01 1.07886808e-02 6.11985801e-03 3.73134328e-03\n",
            " 5.32942420e-03 2.89028608e-03 0.00000000e+00 1.11022302e-16\n",
            " 2.50306117e-03 0.00000000e+00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIlpsRy3XY95",
        "colab_type": "code",
        "outputId": "c29cbd03-0015-4a76-a55d-420c29373931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "data_num_x1 = np.average(temp_num_nc1, axis=1)\n",
        "data_num_x2 = np.average(temp_num_nc2, axis=1)\n",
        "data_num_x3 = np.average(temp_num_nc3, axis=1)\n",
        "std_num_x1 = np.std(temp_num_nc1, axis=1)\n",
        "std_num_x2 = np.std(temp_num_nc2, axis=1)\n",
        "std_num_x3 = np.std(temp_num_nc3, axis=1)\n",
        "\n",
        "print(data_num_x1, std_num_x1, std_num_x2, std_num_x3)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[36.5 47.  47.  48.  48.  48.  48.  48.  48.  48. ] [5.73149195 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ] [13.5410487   3.4         0.          0.          0.64031242  0.\n",
            "  0.48989795  0.          0.          0.45825757] [35.89275693  2.89136646  1.64012195  1.          1.42828569  0.77459667\n",
            "  0.          0.          0.67082039  0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc6vamXyXbEM",
        "colab_type": "code",
        "outputId": "15df5d41-725e-48aa-a10d-4399207ea485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "t = np.linspace(0, 10, 10)\n",
        "fig = plt.figure(figsize=(20,7),dpi=60)\n",
        "ax = fig.add_subplot(1,2,1)\n",
        "plt.errorbar(t, data_per_x1, yerr=std_per_x1, marker=\"o\", label=\"model1\")\n",
        "plt.errorbar(t, data_per_x2, yerr=std_per_x2, marker=\"s\", label=\"model2\")\n",
        "plt.errorbar(t, data_per_x3, yerr=std_per_x3, marker=\"^\", label=\"model3\")\n",
        "ax.legend(loc=0)\n",
        "plt.legend(fontsize=18)\n",
        "plt.title(\"neuron coverage\", fontsize=18)\n",
        "ax.set_xlabel('step', fontsize=18)\n",
        "ax.set_ylabel('neuron coverage', fontsize=18)\n",
        "plt.tick_params(labelsize=16)\n",
        "\n",
        "ax = fig.add_subplot(1,2,2)\n",
        "plt.errorbar(t, data_num_x1, yerr=std_num_x1, marker=\"o\", label=\"model1:\"+str(neuron_covered(model_layer_dict1)[1]))\n",
        "plt.errorbar(t, data_num_x2, yerr=std_num_x2, marker=\"s\", label=\"model2:\"+str(neuron_covered(model_layer_dict2)[1]))\n",
        "plt.errorbar(t, data_num_x3, yerr=std_num_x3, marker=\"^\", label=\"model3:\"+str(neuron_covered(model_layer_dict3)[1]))\n",
        "ax.legend(loc=0)\n",
        "plt.legend(fontsize=18)\n",
        "plt.title(\"neuron coverage\", fontsize=18)\n",
        "ax.set_xlabel('step', fontsize=18)\n",
        "ax.set_ylabel('# covered neuron ', fontsize=18)\n",
        "plt.tick_params(labelsize=16)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9sAAAF/CAYAAABZm9oNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERERERERkQD7/3idC5YI6TSPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1200x420 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJXfVtcaXd-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}