{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "DeepXplore_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshii/de4test/blob/test_ozawa/DeepXplore/DeepXplore_03_11_result_out.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVFjZpUIRF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 共通で使うパスなどの定義\n",
        "# 共通の変数設定\n",
        "# 共通フォルダパス\n",
        "\n",
        "import datetime\n",
        "import pytz\n",
        "dt_now = datetime.datetime.now(pytz.timezone('Asia/Tokyo'))\n",
        "dt_str = str(dt_now.strftime('%Y%m%d_%H%M'))\n",
        "\n",
        "data_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data\"\n",
        "#data_imagenet = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet\"\n",
        "data_mnist = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/MNIST\"\n",
        "#data_imagenet_seeds = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet/seeds\"\n",
        "model_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/model\"\n",
        "#output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output\"\n",
        "output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output/\" + dt_str\n",
        "tmp_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/tmp\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hv53U_OdAFYb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4lP4POGn5Xj",
        "colab_type": "code",
        "outputId": "a521cb31-b699-4579-91bf-b29227ecb7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# Goggle Drive つなぐ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3d3Suv3Q5Db",
        "colab_type": "code",
        "outputId": "13976c49-d488-43e4-f41c-90649c487529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# outputフォルダ内容物のクリーンアップ削除\n",
        "!mkdir \"$output_dir\"\n",
        "!rm \"$output_dir\"/*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200123_1952/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdHYnLT0Nbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST')\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDY8sDVBqxmK",
        "colab_type": "code",
        "outputId": "426f177a-713a-4b3d-b300-0e51d9cd32fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input\n",
        "import imageio\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "import os\n",
        "\n",
        "# TensorFlowでGPUを使っているかのチェック：\n",
        "# \"device_type: \"GPU\" \" があればOK\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 10643032179482315373, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 12830089441812830657\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 6691896190850280725\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14912199066\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 17227525787459490833\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQ2M_8glJNM",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "d2b46942-8203-42b4-f7e3-533619cc54ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "# DeepXplore のパラメータ設定部\n",
        "# read the parameter\n",
        "# argument parsing\n",
        "parser = argparse.ArgumentParser(description='Main function for difference-inducing input generation in MNIST dataset')\n",
        "parser.add_argument('transformation', help=\"realistic transformation type\", choices=['light', 'occl', 'blackout'])\n",
        "parser.add_argument('weight_diff', help=\"weight hyperparm to control differential behavior\", type=float)\n",
        "parser.add_argument('weight_nc', help=\"weight hyperparm to control neuron coverage\", type=float)\n",
        "parser.add_argument('step', help=\"step size of gradient descent\", type=float)\n",
        "parser.add_argument('seeds', help=\"number of seeds of input\", type=int)\n",
        "parser.add_argument('grad_iterations', help=\"number of iterations of gradient descent\", type=int)\n",
        "parser.add_argument('threshold', help=\"threshold for determining neuron activated\", type=float)\n",
        "parser.add_argument('-t', '--target_model', help=\"target model that we want it predicts differently\",\n",
        "                    choices=[0, 1, 2], default=0, type=int)\n",
        "parser.add_argument('-sp', '--start_point', help=\"occlusion upper left corner coordinate\", default=(0, 0), type=tuple)\n",
        "parser.add_argument('-occl_size', '--occlusion_size', help=\"occlusion size\", default=(10, 10), type=tuple)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-occl_size', '--occlusion_size'], dest='occlusion_size', nargs=None, const=None, default=(10, 10), type=<class 'tuple'>, choices=None, help='occlusion size', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93KW-VGjMzun",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title ノイズのタイプ light / occl / blackout\n",
        "#@body {light,occl,blackout} weight_diff weight_nc step seeds\n",
        "#                   grad_iterations threshold\n",
        "noise_type = \"light\" #@param [\"light\", \"occl\", \"blackout\"]\n",
        "weight_diff = \"0.1\" #@param {type:\"string\"}\n",
        "weight_nc = \"0.1\" #@param {type:\"string\"}\n",
        "step = \"20\" #@param {type:\"string\"}　#50だとほぼ白飛び\n",
        "seeds = \"5\" #@param {type:\"string\"}\n",
        "grad_iterations = \"10\" #@param {type:\"string\"}\n",
        "threshold = \"0.1\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk80fCuPts9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = parser.parse_args([noise_type, weight_diff, weight_nc, step, seeds, grad_iterations, threshold])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tTYrllplJNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x.reshape(x.shape[1], x.shape[2])  # original shape (1,img_rows, img_cols,1)\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "def constraint_occl(gradients, start_point, rect_shape):\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def constraint_light(gradients):\n",
        "    new_grads = np.ones_like(gradients)\n",
        "    grad_mean = np.mean(gradients)\n",
        "    return grad_mean * new_grads\n",
        "\n",
        "\n",
        "def constraint_black(gradients, rect_shape=(6, 6)):\n",
        "    start_point = (\n",
        "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    if np.mean(patch) < 0:\n",
        "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def init_coverage_tables(model1, model2, model3):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    model_layer_dict2 = defaultdict(bool)\n",
        "    model_layer_dict3 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    init_dict(model2, model_layer_dict2)\n",
        "    init_dict(model3, model_layer_dict3)\n",
        "    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n",
        "\n",
        "\n",
        "def init_dict(model, model_layer_dict):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_dict[(layer.name, index)] = False\n",
        "\n",
        "\n",
        "def neuron_to_cover(model_layer_dict):\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "    else:\n",
        "        layer_name, index = random.choice(model_layer_dict.keys())\n",
        "    return layer_name, index\n",
        "\n",
        "\n",
        "def neuron_covered(model_layer_dict):\n",
        "    covered_neurons = len([v for v in model_layer_dict.values() if v])\n",
        "    total_neurons = len(model_layer_dict)\n",
        "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n",
        "\n",
        "\n",
        "def update_coverage(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n",
        "                model_layer_dict[(layer_names[i], num_neuron)] = True\n",
        "\n",
        "\n",
        "def full_coverage(model_layer_dict):\n",
        "    if False in model_layer_dict.values():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
        "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
        "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
        "    X_scaled = X_std * (rmax - rmin) + rmin\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def fired(model, layer_name, index, input_data, threshold=0):\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
        "    scaled = scale(intermediate_layer_output)\n",
        "    if np.mean(scaled[..., index]) > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def diverged(predictions1, predictions2, predictions3, target):\n",
        "    #     if predictions2 == predictions3 == target and predictions1 != target:\n",
        "    if not predictions1 == predictions2 == predictions3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "#add\n",
        "def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n",
        "    return [os.path.join(root, f)\n",
        "            for root, _, files in os.walk(directory) for f in files\n",
        "            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDDT47Er6u23",
        "colab_type": "code",
        "outputId": "a300f7ca-09a2-40f8-92d4-f5160f7c9779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "'''\n",
        "LeNet-1\n",
        "'''\n",
        "def Model1(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        print(x_train.shape)\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(4, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(12, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "#        model.save_weights('./Model1.h5')\n",
        "        model.save_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "#        model.load_weights('./Model1.h5')\n",
        "        model.load_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        print(bcolors.OKBLUE + 'Model1 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model1(train=True)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 4s 73us/step - loss: 0.4524 - acc: 0.8629 - val_loss: 0.1452 - val_acc: 0.9581\n",
            "\n",
            "\n",
            "Overall Test score: 0.14523715103194118\n",
            "Overall Test accuracy: 0.9581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU88awZf68JO",
        "colab_type": "code",
        "outputId": "462fc044-f9f1-41dd-8f69-4d6a04a767a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "'''\n",
        "LeNet-4\n",
        "'''\n",
        "def Model2(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(84, activation='relu', name='fc1')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model2.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model2.h5')\n",
        "        print(bcolors.OKBLUE + 'Model2 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model2(train=True)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.3506 - acc: 0.8935 - val_loss: 0.2616 - val_acc: 0.9157\n",
            "\n",
            "\n",
            "Overall Test score: 0.2616416467715055\n",
            "Overall Test accuracy: 0.9157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufs67uRN7KYL",
        "colab_type": "code",
        "outputId": "7e822e76-bf2d-42a7-c42d-f0afa0d89113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "'''\n",
        "LeNet-5\n",
        "'''\n",
        "def Model3(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(120, activation='relu', name='fc1')(x)\n",
        "    x = Dense(84, activation='relu', name='fc2')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model3.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model3.h5')\n",
        "        print(bcolors.OKBLUE + 'Model3 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model3(train=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 2s 36us/step - loss: 0.3647 - acc: 0.8838 - val_loss: 0.1585 - val_acc: 0.9509\n",
            "\n",
            "\n",
            "Overall Test score: 0.1585128423959017\n",
            "Overall Test accuracy: 0.9509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9QiwlW7WWR",
        "colab_type": "code",
        "outputId": "fa72c5a2-6f07-4c27-da7e-6d98fc1a693d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# input MNIST image dimensions \n",
        "img_rows, img_cols = 28, 28\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "#define the model instance\n",
        "model1 = Model1(input_tensor=input_tensor)\n",
        "model2 = Model2(input_tensor=input_tensor)\n",
        "model3 = Model3(input_tensor=input_tensor)\n",
        "\n",
        "#define the dictionary of neuron coverage\n",
        "model_layer_dict1, model_layer_dict2, model_layer_dict3 = init_coverage_tables(model1, model2, model3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[94mModel1 loaded\u001b[0m\n",
            "\u001b[94mModel2 loaded\u001b[0m\n",
            "\u001b[94mModel3 loaded\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MS5H74Q4XA",
        "colab_type": "text"
      },
      "source": [
        "### 入力するデータの選別(1/2)\n",
        "\n",
        "０～９の数字をそれぞれ束ねてデータを準備する\n",
        "[00...011...1......99...9]。\n",
        "\n",
        "* test_per_fig_x: \n",
        "    各数字に対する画像データを格納\n",
        "* test_per_fig_y: \n",
        "    各数字に対する教師ラベルを格納\n",
        "* tests_x: \n",
        "    test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: \n",
        "    tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brFIdSfL8nHP",
        "colab_type": "code",
        "outputId": "159149f7-76dd-4810-db4d-40b740a8a750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load the MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "num_fig = 10\n",
        "test_per_fig_x = np.array([])\n",
        "test_per_fig_y = np.array([])\n",
        "tests_x = np.array([])\n",
        "tests_y = np.array([])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "#x_train = x_train.astype('float')\n",
        "#x_test = x_test.astype('float')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "length = int(args.seeds)\n",
        "#length = int(args.seeds)\n",
        "for i in range(num_fig):\n",
        "  cond = [(x==i) for x in y_test]\n",
        "  test_per_fig_x = x_test[cond]\n",
        "  test_per_fig_y = y_test[cond]\n",
        "#  np.set_printoptions(formatter={'int': '{:07d}'.format})\n",
        "  print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "  tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "  tests_y = np.append(tests_y, test_per_fig_y[:length])\n",
        "#  conds = [conds, cond]\n",
        "print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "\n",
        "###### MNIST data, shuffled and split by train and test sets\n",
        "#####(_, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "###### modify the numpy data for the Keras model\n",
        "#####x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "#####input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "#tests_x.shape\n",
        "tests_x = tests_x.astype('float32')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure: 0 , shape: (980, 28, 28) , deviation: -20\n",
            "figure: 1 , shape: (1135, 28, 28) , deviation: 135\n",
            "figure: 2 , shape: (1032, 28, 28) , deviation: 32\n",
            "figure: 3 , shape: (1010, 28, 28) , deviation: 10\n",
            "figure: 4 , shape: (982, 28, 28) , deviation: -18\n",
            "figure: 5 , shape: (892, 28, 28) , deviation: -108\n",
            "figure: 6 , shape: (958, 28, 28) , deviation: -42\n",
            "figure: 7 , shape: (1028, 28, 28) , deviation: 28\n",
            "figure: 8 , shape: (974, 28, 28) , deviation: -26\n",
            "figure: 9 , shape: (1009, 28, 28) , deviation: 9\n",
            "check! 5.0 = 5 equal?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUOK_EjJW2KT",
        "colab_type": "text"
      },
      "source": [
        "## 入力するデータの選別(2/2)\n",
        "０～９の数字をそれぞれ束ねてデータを準備する [00...000]。\n",
        "\n",
        "* test_per_fig_x: 各数字に対する画像データを格納\n",
        "* test_per_fig_y: 各数字に対する教師ラベルを格納\n",
        "* tests_x: test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "id": "SLee__9UAD1Y",
        "colab": {}
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "#load the MNIST \n",
        "\n",
        "def create_data(start_fig, num_fi, length):\n",
        "  (datax_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  test_per_fig_x = np.array([])\n",
        "  test_per_fig_y = np.array([])\n",
        "  tests_x = np.array([])\n",
        "  tests_y = np.array([])\n",
        "\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test  = x_test.astype('float32')\n",
        "  x_train = x_train.astype('float')\n",
        "  x_test = x_test.astype('float')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  #length = int(args.seeds/num_fig)\n",
        "  #length = int(args.seeds)\n",
        "  for i in range(start_fig, start_fig+num_fig):\n",
        "  #i=0\n",
        "    cond = [(x==i) for x in y_test]\n",
        "    test_per_fig_x = x_test[cond]\n",
        "    test_per_fig_y = y_test[cond]\n",
        "    print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "    tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "    tests_y = np.append(tests_y, test_per_fig_y[:length])  \n",
        "\n",
        "  #conds = [conds, cond]\n",
        "  print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "  tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "  tests_x.shape\n",
        "  tests_x = tests_x.astype('float32')\n",
        "  return tests_x, tests_y, length\n",
        "  tests_x, tests_y, length = create_data(0, 10, int(args.seeds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyo2SSSz8vkT",
        "colab_type": "code",
        "outputId": "ca7cd3f8-6962-412d-f051-a0aa3184d389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(tests_x.shape)\n",
        "print(tests_y)\n",
        "for i in range(0,1):\n",
        "  print(i)\n",
        "print(length)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 28, 28, 1)\n",
            "[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 4. 4. 4. 4.\n",
            " 4. 5. 5. 5. 5. 5. 6. 6. 6. 6. 6. 7. 7. 7. 7. 7. 8. 8. 8. 8. 8. 9. 9. 9.\n",
            " 9. 9.]\n",
            "0\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLlKIMxG9kEA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfdTZi4mUPf",
        "colab_type": "code",
        "outputId": "c8f6915b-a6ab-4459-8d76-4faa8e50c8cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "def neuron_output(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "    \n",
        "    out_list = []\n",
        "    out_list_scale = []\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            out_list.append(np.mean(intermediate_layer_output[..., num_neuron]))\n",
        "            out_list_scale.append(np.mean(scaled[..., num_neuron]))\n",
        "\n",
        "    return out_list, out_list_scale\n",
        "\n",
        "\n",
        "def deepXplore(model_layer_dict1, model_layer_dict2, model_layer_dict3, tests_x, model1, model2, model3, num_fig, start_fig, length):\n",
        "  #print(\"test01\")\n",
        "  count_already = 0\n",
        "  count_found = 0\n",
        "  count_not_found = 0\n",
        "  temp_per_nc1 = np.array([])\n",
        "  temp_per_nc2 = np.array([])\n",
        "  temp_per_nc3 = np.array([])\n",
        "  temp_num_nc1 = np.array([])\n",
        "  temp_num_nc2 = np.array([])\n",
        "  temp_num_nc3 = np.array([])\n",
        "  #print(\"test10\")\n",
        "\n",
        "  #for each neuron\n",
        "  num_neurons1 = neuron_covered(model_layer_dict1)[1]\n",
        "  num_neurons2 = neuron_covered(model_layer_dict2)[1]\n",
        "  num_neurons3 = neuron_covered(model_layer_dict3)[1]\n",
        "  #print(\"test11\")\n",
        "\n",
        "  column_tmp1 = list(model_layer_dict1.keys())\n",
        "  column_tmp2 = list(model_layer_dict2.keys())\n",
        "  column_tmp3 = list(model_layer_dict3.keys())\n",
        "  #print(\"test12\")\n",
        "  df1 = pd.DataFrame(columns=column_tmp1)\n",
        "  df2 = pd.DataFrame(columns=column_tmp2)\n",
        "  df3 = pd.DataFrame(columns=column_tmp3)\n",
        "  df1_scale = pd.DataFrame(columns=column_tmp1)\n",
        "  df2_scale = pd.DataFrame(columns=column_tmp2)\n",
        "  df3_scale = pd.DataFrame(columns=column_tmp3)\n",
        "  tmp_list = [\"already_diff\", \"found\", \"not_found\", \"layer1\", \"index1\", \"layer2\", \"index2\", \"layer3\", \"index3\"]\n",
        "  bug_result = pd.DataFrame(columns=tmp_list)\n",
        "  trial = 1\n",
        "  #print(\"test13\")\n",
        "  #print(\"test02\")\n",
        "  for index_fig in range(num_fig):\n",
        "    index_fig = index_fig + start_fig\n",
        "    print(\"figure\"+str(index_fig))\n",
        "    for _ in range(length):\n",
        "      #gen_img = np.expand_dims(random.choice(tests_x), axis=0)\n",
        "      gen_img = np.expand_dims(tests_x[(length*index_fig + _)], axis=0)\n",
        "      orig_img = gen_img.copy()\n",
        "      # first check if input already induces differences\n",
        "      label1, label2, label3 = np.argmax(model1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "      if not label1 == label2 == label3:\n",
        "          count_already += 1\n",
        "          print(bcolors.OKGREEN + '   {}/{}. input already causes different outputs ({},{},{}) at({}, {}, {}): '.format(_, length, label1, label2, label3, count_already, count_found, count_not_found) + bcolors.ENDC)        \n",
        "\n",
        "          update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "          temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "          temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "          temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "          temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "          temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "          temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "                               \n",
        "          print(bcolors.OKGREEN + '     covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'% (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  neuron_covered(model_layer_dict2)[2], len(model_layer_dict3), neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "          averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                       neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +neuron_covered(model_layer_dict3)[1])\n",
        "          print(bcolors.OKGREEN + '     averaged covered neurons %.3f' % (averaged_nc) + bcolors.ENDC)\n",
        "\n",
        "          gen_img_deprocessed = deprocess_image(gen_img)\n",
        "\n",
        "          # save the result to disk\n",
        "          outputfilepath0 = os.path.join(output_dir, 'already_differ_' + str(label1) + '_' + str(label2) + '_' + str(label3) +'_['+ str(_) +  '].png')\n",
        "          imageio.imwrite(outputfilepath0, gen_img_deprocessed)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df1 = df1.append(temp)\n",
        "          df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df2 = df2.append(temp)\n",
        "          df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "          temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df3 = df3.append(temp)\n",
        "          df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "          #print(\"test10\")\n",
        "          temp = [1, 0, 0, None, None, None, None, None, None]\n",
        "          #print(\"test11\")\n",
        "          temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          #print(\"test12\")\n",
        "          bug_result = bug_result.append(temp)\n",
        "          #print(\"test13\")\n",
        "          trial += 1\n",
        "          continue\n",
        "\n",
        "      # if all label agrees\n",
        "      orig_label = label1\n",
        "      layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n",
        "      layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n",
        "      layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n",
        "\n",
        "      # construct joint loss function\n",
        "      if args.target_model == 0:\n",
        "        loss1 = -args.weight_diff * K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 1:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = -args.weight_diff * K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 2:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n",
        "      loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n",
        "      loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n",
        "      layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron)\n",
        "\n",
        "      # for adversarial image generation\n",
        "      final_loss = K.mean(layer_output)\n",
        "\n",
        "      # we compute the gradient of the input picture wrt this loss\n",
        "      grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "\n",
        "      # this function returns the loss and grads given the input picture\n",
        "      iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n",
        "\n",
        "      #print(\"test03\")\n",
        "      # we run gradient ascent for some steps\n",
        "      for iters in range(args.grad_iterations):\n",
        "        loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = iterate([gen_img])\n",
        "        if args.transformation == 'light':\n",
        "            grads_value = constraint_light(grads_value)  # constraint the gradients value\n",
        "        elif args.transformation == 'occl':\n",
        "            grads_value = constraint_occl(grads_value, args.start_point,\n",
        "                                          args.occlusion_size)  # constraint the gradients value\n",
        "        elif args.transformation == 'blackout':\n",
        "            grads_value = constraint_black(grads_value)  # constraint the gradients value\n",
        "\n",
        "        gen_img += grads_value * args.step\n",
        "        predictions1 = np.argmax(model1.predict(gen_img)[0])\n",
        "        predictions2 = np.argmax(model2.predict(gen_img)[0])\n",
        "        predictions3 = np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "        #print(\"test04\")\n",
        "        if not predictions1 == predictions2 == predictions3:\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            count_found += 1\n",
        "            print(bcolors.OKBLUE + '%4d/%d. found at %d! covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f at (%d, %d, %d)'\n",
        "#                  % (_, args.seeds, iters, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  % (_, length, iters + 1, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                     neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                     neuron_covered(model_layer_dict3)[2], count_already, count_found, count_not_found) + bcolors.ENDC)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            print(bcolors.OKBLUE + '     averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  '].png')\n",
        "            #print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "#            outputfilepath2 = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '_orig.png')\n",
        "            outputfilepath2 = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  ']_orig.png')\n",
        "            #print(outputfilepath2)\n",
        "            imageio.imwrite(outputfilepath2, orig_img_deprocessed)\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "            temp = [0, iters+1, 0, layer_name1, index1, layer_name2, index2,layer_name3, index3]\n",
        "            temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            bug_result = bug_result.append(temp)\n",
        "            trial += 1\n",
        "            break\n",
        "          \n",
        "          #add\n",
        "        #print(\"test05\")\n",
        "        if iters == (args.grad_iterations-1):\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            count_not_found += 1\n",
        "#              print('%4d/%d. test suite was not found: averaged covered neurons %.3f at %d/%d' % (_, args.seeds, averaged_nc, count_not_found, count_already + count_found + count_not_found))\n",
        "            print('%4d/%d. test suite was not found: averaged covered neurons %.3f at (%d, %d, %d)' % (_, length, averaged_nc, count_already, count_found, count_not_found))\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            #orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, 'not_found_' + str(label1)+'_['+ str(_) + '].png')\n",
        "#           print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "\n",
        "            \n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "\n",
        "            temp = [0, 0, 1, layer_name1, index1, layer_name2, index2,layer_name3, index3]\n",
        "            temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            bug_result = bug_result.append(temp)\n",
        "\n",
        "            trial += 1\n",
        "            \n",
        "            #break\n",
        "\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "  temp_per_nc1=temp_per_nc1.reshape(num_fig, length)\n",
        "  temp_per_nc2=temp_per_nc2.reshape(num_fig, length)\n",
        "  temp_per_nc3=temp_per_nc3.reshape(num_fig, length)\n",
        "  temp_num_nc1=temp_num_nc1.reshape(num_fig, length)\n",
        "  temp_num_nc2=temp_num_nc2.reshape(num_fig, length)\n",
        "  temp_num_nc3=temp_num_nc3.reshape(num_fig, length)\n",
        "\n",
        "  return df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 7.39 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHi1ffJuWH5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "f1721700-9749-4630-c62c-81cc3895f3b4"
      },
      "source": [
        "print(length, num_fig)\n",
        "print(output_dir)\n",
        "column_tmp1 = list(model_layer_dict1.keys())\n",
        "df1 = pd.DataFrame(columns=column_tmp1)\n",
        "df1.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 10\n",
            "/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200123_1952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(block1_conv1, 0)</th>\n",
              "      <th>(block1_conv1, 1)</th>\n",
              "      <th>(block1_conv1, 2)</th>\n",
              "      <th>(block1_conv1, 3)</th>\n",
              "      <th>(block1_pool1, 0)</th>\n",
              "      <th>(block1_pool1, 1)</th>\n",
              "      <th>(block1_pool1, 2)</th>\n",
              "      <th>(block1_pool1, 3)</th>\n",
              "      <th>(block2_conv1, 0)</th>\n",
              "      <th>(block2_conv1, 1)</th>\n",
              "      <th>(block2_conv1, 2)</th>\n",
              "      <th>(block2_conv1, 3)</th>\n",
              "      <th>(block2_conv1, 4)</th>\n",
              "      <th>(block2_conv1, 5)</th>\n",
              "      <th>(block2_conv1, 6)</th>\n",
              "      <th>(block2_conv1, 7)</th>\n",
              "      <th>(block2_conv1, 8)</th>\n",
              "      <th>(block2_conv1, 9)</th>\n",
              "      <th>(block2_conv1, 10)</th>\n",
              "      <th>(block2_conv1, 11)</th>\n",
              "      <th>(block2_pool1, 0)</th>\n",
              "      <th>(block2_pool1, 1)</th>\n",
              "      <th>(block2_pool1, 2)</th>\n",
              "      <th>(block2_pool1, 3)</th>\n",
              "      <th>(block2_pool1, 4)</th>\n",
              "      <th>(block2_pool1, 5)</th>\n",
              "      <th>(block2_pool1, 6)</th>\n",
              "      <th>(block2_pool1, 7)</th>\n",
              "      <th>(block2_pool1, 8)</th>\n",
              "      <th>(block2_pool1, 9)</th>\n",
              "      <th>(block2_pool1, 10)</th>\n",
              "      <th>(block2_pool1, 11)</th>\n",
              "      <th>(before_softmax, 0)</th>\n",
              "      <th>(before_softmax, 1)</th>\n",
              "      <th>(before_softmax, 2)</th>\n",
              "      <th>(before_softmax, 3)</th>\n",
              "      <th>(before_softmax, 4)</th>\n",
              "      <th>(before_softmax, 5)</th>\n",
              "      <th>(before_softmax, 6)</th>\n",
              "      <th>(before_softmax, 7)</th>\n",
              "      <th>(before_softmax, 8)</th>\n",
              "      <th>(before_softmax, 9)</th>\n",
              "      <th>(predictions, 0)</th>\n",
              "      <th>(predictions, 1)</th>\n",
              "      <th>(predictions, 2)</th>\n",
              "      <th>(predictions, 3)</th>\n",
              "      <th>(predictions, 4)</th>\n",
              "      <th>(predictions, 5)</th>\n",
              "      <th>(predictions, 6)</th>\n",
              "      <th>(predictions, 7)</th>\n",
              "      <th>(predictions, 8)</th>\n",
              "      <th>(predictions, 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [(block1_conv1, 0), (block1_conv1, 1), (block1_conv1, 2), (block1_conv1, 3), (block1_pool1, 0), (block1_pool1, 1), (block1_pool1, 2), (block1_pool1, 3), (block2_conv1, 0), (block2_conv1, 1), (block2_conv1, 2), (block2_conv1, 3), (block2_conv1, 4), (block2_conv1, 5), (block2_conv1, 6), (block2_conv1, 7), (block2_conv1, 8), (block2_conv1, 9), (block2_conv1, 10), (block2_conv1, 11), (block2_pool1, 0), (block2_pool1, 1), (block2_pool1, 2), (block2_pool1, 3), (block2_pool1, 4), (block2_pool1, 5), (block2_pool1, 6), (block2_pool1, 7), (block2_pool1, 8), (block2_pool1, 9), (block2_pool1, 10), (block2_pool1, 11), (before_softmax, 0), (before_softmax, 1), (before_softmax, 2), (before_softmax, 3), (before_softmax, 4), (before_softmax, 5), (before_softmax, 6), (before_softmax, 7), (before_softmax, 8), (before_softmax, 9), (predictions, 0), (predictions, 1), (predictions, 2), (predictions, 3), (predictions, 4), (predictions, 5), (predictions, 6), (predictions, 7), (predictions, 8), (predictions, 9)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSSBfoAZreWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "09413e90-8a23-4f02-e02a-a2515097d51e"
      },
      "source": [
        "index_fig = 0\n",
        "trial = 0\n",
        "gen_img = np.expand_dims(tests_x[(length*index_fig + 0)], axis=0)\n",
        "temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "print(type(temp_scale))\n",
        "temp_scale.name"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0_0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yymgs3ldNiDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "36b699fc-f9b6-4cc8-b2cc-8fbe1f251d52"
      },
      "source": [
        "#num_fig = 1\n",
        "#start_fig = 0\n",
        "#deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "\n",
        "num_fig = 1\n",
        "for start_fig in range(10):\n",
        "#for start_fig in range(2):\n",
        "  df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result= deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "  df1_scale.to_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")\n",
        "  bug_result.to_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_index.csv\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure0\n",
            "\u001b[94m   0/5. found at 9! covered neurons percentage 52 neurons 0.596, 148 neurons 0.486, 268 neurons 0.414 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.457\u001b[0m\n",
            "\u001b[94m   1/5. found at 3! covered neurons percentage 52 neurons 0.596, 148 neurons 0.554, 268 neurons 0.422 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.483\u001b[0m\n",
            "\u001b[94m   2/5. found at 4! covered neurons percentage 52 neurons 0.615, 148 neurons 0.554, 268 neurons 0.422 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.485\u001b[0m\n",
            "\u001b[94m   3/5. found at 10! covered neurons percentage 52 neurons 0.635, 148 neurons 0.568, 268 neurons 0.437 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.500\u001b[0m\n",
            "\u001b[94m   4/5. found at 4! covered neurons percentage 52 neurons 0.635, 148 neurons 0.568, 268 neurons 0.463 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.515\u001b[0m\n",
            "figure1\n",
            "   0/5. test suite was not found: averaged covered neurons 0.534 at (0, 0, 1)\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.673, 148 neurons 0.662, 268 neurons 0.634 at (0, 1, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.647\u001b[0m\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.673, 148 neurons 0.662, 268 neurons 0.675 at (0, 2, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.671\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.673, 148 neurons 0.669, 268 neurons 0.690 at (0, 3, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.682\u001b[0m\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.673, 148 neurons 0.676, 268 neurons 0.701 at (0, 4, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.690\u001b[0m\n",
            "figure2\n",
            "\u001b[94m   0/5. found at 3! covered neurons percentage 52 neurons 0.712, 148 neurons 0.703, 268 neurons 0.739 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.724\u001b[0m\n",
            "   1/5. test suite was not found: averaged covered neurons 0.724 at (0, 1, 1)\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.712, 148 neurons 0.703, 268 neurons 0.739 at (0, 2, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.724\u001b[0m\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.712, 148 neurons 0.709, 268 neurons 0.743 at (0, 3, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.729\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.712, 148 neurons 0.709, 268 neurons 0.746 at (0, 4, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.731\u001b[0m\n",
            "figure3\n",
            "\u001b[92m   0/5. input already causes different outputs (3,3,8) at(1, 0, 0): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.865, 148 neurons 0.818, 268 neurons 0.806\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.816\u001b[0m\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.865, 148 neurons 0.818, 268 neurons 0.806 at (1, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.816\u001b[0m\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.865, 148 neurons 0.818, 268 neurons 0.806 at (1, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.816\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.865, 268 neurons 0.884 at (1, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.882\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.865, 268 neurons 0.884 at (1, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.882\u001b[0m\n",
            "figure4\n",
            "\u001b[94m   0/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.865, 268 neurons 0.899 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.893\u001b[0m\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.878, 268 neurons 0.899 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.897\u001b[0m\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.885, 268 neurons 0.907 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.904\u001b[0m\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.885, 268 neurons 0.907 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.904\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.885, 268 neurons 0.907 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.904\u001b[0m\n",
            "figure5\n",
            "\u001b[94m   0/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.885, 268 neurons 0.907 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.904\u001b[0m\n",
            "\u001b[92m   1/5. input already causes different outputs (5,3,5) at(1, 1, 0): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.942, 148 neurons 0.892, 268 neurons 0.907\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.906\u001b[0m\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.892, 268 neurons 0.910 at (1, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.908\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.892, 268 neurons 0.910 at (1, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.908\u001b[0m\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.892, 268 neurons 0.910 at (1, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.908\u001b[0m\n",
            "figure6\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.892, 268 neurons 0.914 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.892, 268 neurons 0.914 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.910\u001b[0m\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.892, 268 neurons 0.918 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.912\u001b[0m\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.892, 268 neurons 0.918 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.912\u001b[0m\n",
            "\u001b[94m   4/5. found at 4! covered neurons percentage 52 neurons 0.942, 148 neurons 0.892, 268 neurons 0.918 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.912\u001b[0m\n",
            "figure7\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.899, 268 neurons 0.918 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.915\u001b[0m\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.918 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.917\u001b[0m\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.922 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.919\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.922 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.919\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.925 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.921\u001b[0m\n",
            "figure8\n",
            "\u001b[92m   0/5. input already causes different outputs (8,2,8) at(1, 0, 0): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.925\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.921\u001b[0m\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.962, 148 neurons 0.905, 268 neurons 0.929 at (1, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.925\u001b[0m\n",
            "\u001b[92m   2/5. input already causes different outputs (8,3,8) at(2, 1, 0): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.962, 148 neurons 0.905, 268 neurons 0.929\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.925\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.962, 148 neurons 0.905, 268 neurons 0.929 at (2, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.925\u001b[0m\n",
            "\u001b[92m   4/5. input already causes different outputs (8,3,8) at(3, 2, 0): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.962, 148 neurons 0.905, 268 neurons 0.929\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.925\u001b[0m\n",
            "figure9\n",
            "\u001b[94m   0/5. found at 2! covered neurons percentage 52 neurons 0.962, 148 neurons 0.905, 268 neurons 0.929 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.925\u001b[0m\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.962, 148 neurons 0.905, 268 neurons 0.929 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.925\u001b[0m\n",
            "   2/5. test suite was not found: averaged covered neurons 0.925 at (0, 2, 1)\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.962, 148 neurons 0.905, 268 neurons 0.944 at (0, 3, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.934\u001b[0m\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.962, 148 neurons 0.912, 268 neurons 0.948 at (0, 4, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.938\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpsI1IOM-hUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "cb868699-7661-4071-9220-c811046715d0"
      },
      "source": [
        "df2_scale = pd.DataFrame()\n",
        "df3_scale = pd.DataFrame()\n",
        "for start_fig in range(10):\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")\n",
        "  df2_scale = pd.concat([df2_scale, df1_scale])\n",
        "\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_index.csv\")\n",
        "  df3_scale = pd.concat([df3_scale, df1_scale])\n",
        "\n",
        "\n",
        "tmp_list = list(df2_scale.iloc[:,0])\n",
        "df2_scale.index = tmp_list\n",
        "print(df2_scale.iloc[:,1:].head())\n",
        "\n",
        "tmp_list = list(df3_scale.iloc[:,0])\n",
        "df3_scale.index = tmp_list\n",
        "print(df3_scale.iloc[:,1:].head())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     ('block1_conv1', 0)  ...  ('predictions', 9)\n",
            "0_1                0.260  ...               0.000\n",
            "0_2                0.293  ...               0.000\n",
            "0_3                0.273  ...               0.000\n",
            "0_4                0.252  ...               0.000\n",
            "0_5                0.215  ...               0.000\n",
            "\n",
            "[5 rows x 52 columns]\n",
            "     already_diff  found  not_found  ... index2  layer3  index3\n",
            "0_1         0.000  9.000      0.000  ...  7.000     fc1  84.000\n",
            "0_2         0.000  3.000      0.000  ... 21.000     fc1  35.000\n",
            "0_3         0.000  4.000      0.000  ... 33.000     fc1 107.000\n",
            "0_4         0.000 10.000      0.000  ... 48.000     fc2  60.000\n",
            "0_5         0.000  4.000      0.000  ...  6.000     fc1  17.000\n",
            "\n",
            "[5 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QtyLAZiPeKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "b7349524-5bec-4ea6-bc73-8e3c4674c947"
      },
      "source": [
        "df2_scale.iloc[0:6,1:]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>('block1_conv1', 0)</th>\n",
              "      <th>('block1_conv1', 1)</th>\n",
              "      <th>('block1_conv1', 2)</th>\n",
              "      <th>('block1_conv1', 3)</th>\n",
              "      <th>('block1_pool1', 0)</th>\n",
              "      <th>('block1_pool1', 1)</th>\n",
              "      <th>('block1_pool1', 2)</th>\n",
              "      <th>('block1_pool1', 3)</th>\n",
              "      <th>('block2_conv1', 0)</th>\n",
              "      <th>('block2_conv1', 1)</th>\n",
              "      <th>('block2_conv1', 2)</th>\n",
              "      <th>('block2_conv1', 3)</th>\n",
              "      <th>('block2_conv1', 4)</th>\n",
              "      <th>('block2_conv1', 5)</th>\n",
              "      <th>('block2_conv1', 6)</th>\n",
              "      <th>('block2_conv1', 7)</th>\n",
              "      <th>('block2_conv1', 8)</th>\n",
              "      <th>('block2_conv1', 9)</th>\n",
              "      <th>('block2_conv1', 10)</th>\n",
              "      <th>('block2_conv1', 11)</th>\n",
              "      <th>('block2_pool1', 0)</th>\n",
              "      <th>('block2_pool1', 1)</th>\n",
              "      <th>('block2_pool1', 2)</th>\n",
              "      <th>('block2_pool1', 3)</th>\n",
              "      <th>('block2_pool1', 4)</th>\n",
              "      <th>('block2_pool1', 5)</th>\n",
              "      <th>('block2_pool1', 6)</th>\n",
              "      <th>('block2_pool1', 7)</th>\n",
              "      <th>('block2_pool1', 8)</th>\n",
              "      <th>('block2_pool1', 9)</th>\n",
              "      <th>('block2_pool1', 10)</th>\n",
              "      <th>('block2_pool1', 11)</th>\n",
              "      <th>('before_softmax', 0)</th>\n",
              "      <th>('before_softmax', 1)</th>\n",
              "      <th>('before_softmax', 2)</th>\n",
              "      <th>('before_softmax', 3)</th>\n",
              "      <th>('before_softmax', 4)</th>\n",
              "      <th>('before_softmax', 5)</th>\n",
              "      <th>('before_softmax', 6)</th>\n",
              "      <th>('before_softmax', 7)</th>\n",
              "      <th>('before_softmax', 8)</th>\n",
              "      <th>('before_softmax', 9)</th>\n",
              "      <th>('predictions', 0)</th>\n",
              "      <th>('predictions', 1)</th>\n",
              "      <th>('predictions', 2)</th>\n",
              "      <th>('predictions', 3)</th>\n",
              "      <th>('predictions', 4)</th>\n",
              "      <th>('predictions', 5)</th>\n",
              "      <th>('predictions', 6)</th>\n",
              "      <th>('predictions', 7)</th>\n",
              "      <th>('predictions', 8)</th>\n",
              "      <th>('predictions', 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_1</th>\n",
              "      <td>0.260</td>\n",
              "      <td>0.652</td>\n",
              "      <td>0.688</td>\n",
              "      <td>0.518</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.694</td>\n",
              "      <td>0.730</td>\n",
              "      <td>0.554</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.419</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.306</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.513</td>\n",
              "      <td>0.358</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.359</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.934</td>\n",
              "      <td>0.296</td>\n",
              "      <td>0.664</td>\n",
              "      <td>0.471</td>\n",
              "      <td>0.542</td>\n",
              "      <td>0.527</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.200</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_2</th>\n",
              "      <td>0.293</td>\n",
              "      <td>0.735</td>\n",
              "      <td>0.775</td>\n",
              "      <td>0.583</td>\n",
              "      <td>0.306</td>\n",
              "      <td>0.773</td>\n",
              "      <td>0.814</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.320</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.412</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.532</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.361</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.464</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.802</td>\n",
              "      <td>0.360</td>\n",
              "      <td>0.713</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.497</td>\n",
              "      <td>0.509</td>\n",
              "      <td>0.702</td>\n",
              "      <td>0.118</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_3</th>\n",
              "      <td>0.273</td>\n",
              "      <td>0.685</td>\n",
              "      <td>0.723</td>\n",
              "      <td>0.543</td>\n",
              "      <td>0.291</td>\n",
              "      <td>0.724</td>\n",
              "      <td>0.764</td>\n",
              "      <td>0.580</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.446</td>\n",
              "      <td>0.319</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.321</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.416</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.373</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.367</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.258</td>\n",
              "      <td>0.472</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.844</td>\n",
              "      <td>0.308</td>\n",
              "      <td>0.681</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.508</td>\n",
              "      <td>0.658</td>\n",
              "      <td>0.127</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_4</th>\n",
              "      <td>0.252</td>\n",
              "      <td>0.634</td>\n",
              "      <td>0.669</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.680</td>\n",
              "      <td>0.715</td>\n",
              "      <td>0.544</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.419</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.404</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.519</td>\n",
              "      <td>0.366</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.364</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.947</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.669</td>\n",
              "      <td>0.411</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.741</td>\n",
              "      <td>0.154</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_5</th>\n",
              "      <td>0.215</td>\n",
              "      <td>0.541</td>\n",
              "      <td>0.571</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.584</td>\n",
              "      <td>0.617</td>\n",
              "      <td>0.473</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.326</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.298</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.155</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.643</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.357</td>\n",
              "      <td>0.488</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.915</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_1</th>\n",
              "      <td>0.286</td>\n",
              "      <td>0.717</td>\n",
              "      <td>0.757</td>\n",
              "      <td>0.569</td>\n",
              "      <td>0.293</td>\n",
              "      <td>0.747</td>\n",
              "      <td>0.788</td>\n",
              "      <td>0.595</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.494</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.451</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.583</td>\n",
              "      <td>0.399</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.392</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.721</td>\n",
              "      <td>0.443</td>\n",
              "      <td>0.720</td>\n",
              "      <td>0.553</td>\n",
              "      <td>0.529</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.708</td>\n",
              "      <td>0.113</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ('block1_conv1', 0)  ...  ('predictions', 9)\n",
              "0_1                0.260  ...               0.000\n",
              "0_2                0.293  ...               0.000\n",
              "0_3                0.273  ...               0.000\n",
              "0_4                0.252  ...               0.000\n",
              "0_5                0.215  ...               0.000\n",
              "1_1                0.286  ...               0.000\n",
              "\n",
              "[6 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ1DCqKX9lTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_list = list(df2_scale.iloc[:,0])\n",
        "#df2_scale = df2_scale.rename(index=tmp_list)\n",
        "type(tmp_list)\n",
        "df2_scale.index = tmp_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiVj9Wjd-rVv",
        "colab_type": "text"
      },
      "source": [
        "以下が、df1_scaleに格納されているニューロンの出力値から発火/非発火の集計をとるアルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B19EsPFQrak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "fe1fdbac-2529-48e7-e062-982b364f507c"
      },
      "source": [
        "import pylab as pl\n",
        "\n",
        "thres = 0.1\n",
        "bools = df2_scale.iloc[:,1:] > thres\n",
        "\n",
        "bools.sum().hist()\n",
        "pl.xlabel(\"activation time\")\n",
        "pl.ylabel(\"number of neurons\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'number of neurons')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npJ4x+CWpZwx+SeqZ/w+/M54xfwVH2wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGmSlrMm7g6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "outputId": "9a03069c-5a8a-4cad-b986-57a6cb16cefd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#print(test.index[0],test.index[0][0])\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(12, 8))\n",
        "bools.sum().plot(kind=\"bar\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f2d10146dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAIwCAYAAACP/wPjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqRo2v5IkSaqGza8kSZKqYfMrSZKkavz/k3Vib5xeBjkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPP1-qoLB_b-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}