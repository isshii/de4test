{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "DeepXplore_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshii/de4test/blob/test_ozawa/DeepXplore/DeepXplore_03_16_trial_last_result_01_20200209_0046.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVFjZpUIRF4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f29800cf-b725-4d07-fbf0-7f8dda28923e"
      },
      "source": [
        "# 共通で使うパスなどの定義\n",
        "# 共通の変数設定\n",
        "# 共通フォルダパス\n",
        "\n",
        "import datetime\n",
        "import pytz\n",
        "dt_now = datetime.datetime.now(pytz.timezone('Asia/Tokyo'))\n",
        "dt_str = str(dt_now.strftime('%Y%m%d_%H%M'))\n",
        "print(dt_str)\n",
        "\n",
        "data_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data\"\n",
        "#data_imagenet = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet\"\n",
        "data_mnist = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/MNIST\"\n",
        "#data_imagenet_seeds = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet/seeds\"\n",
        "model_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/model\"\n",
        "#output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output\"\n",
        "output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output/\" + dt_str\n",
        "tmp_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/tmp\"\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20200209_0046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hv53U_OdAFYb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4lP4POGn5Xj",
        "colab_type": "code",
        "outputId": "ef6c70d0-504e-443b-8d2d-76bcc9aa61ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# Goggle Drive つなぐ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3d3Suv3Q5Db",
        "colab_type": "code",
        "outputId": "133d83ef-a09e-42f1-ee52-96d7a8523c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# outputフォルダ内容物のクリーンアップ削除\n",
        "!mkdir \"$output_dir\"\n",
        "!rm \"$output_dir\"/*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200209_0046/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdHYnLT0Nbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST')\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDY8sDVBqxmK",
        "colab_type": "code",
        "outputId": "067ca148-19cd-44f7-8c93-c2086d9d90ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input\n",
        "import imageio\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "import os\n",
        "\n",
        "# TensorFlowでGPUを使っているかのチェック：\n",
        "# \"device_type: \"GPU\" \" があればOK\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 10009306704679868548, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 3884238468817297987\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 17762908375168712400\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15956161332\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 563096509849287504\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQ2M_8glJNM",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "d2c93060-0650-4a13-b15c-6b0e7ec94984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "# DeepXplore のパラメータ設定部\n",
        "# read the parameter\n",
        "# argument parsing\n",
        "parser = argparse.ArgumentParser(description='Main function for difference-inducing input generation in MNIST dataset')\n",
        "parser.add_argument('transformation', help=\"realistic transformation type\", choices=['light', 'occl', 'blackout'])\n",
        "parser.add_argument('weight_diff', help=\"weight hyperparm to control differential behavior\", type=float)\n",
        "parser.add_argument('weight_nc', help=\"weight hyperparm to control neuron coverage\", type=float)\n",
        "parser.add_argument('step', help=\"step size of gradient descent\", type=float)\n",
        "parser.add_argument('seeds', help=\"number of seeds of input\", type=int)\n",
        "parser.add_argument('grad_iterations', help=\"number of iterations of gradient descent\", type=int)\n",
        "parser.add_argument('threshold', help=\"threshold for determining neuron activated\", type=float)\n",
        "parser.add_argument('-t', '--target_model', help=\"target model that we want it predicts differently\",\n",
        "                    choices=[0, 1, 2], default=0, type=int)\n",
        "parser.add_argument('-sp', '--start_point', help=\"occlusion upper left corner coordinate\", default=(0, 0), type=tuple)\n",
        "parser.add_argument('-occl_size', '--occlusion_size', help=\"occlusion size\", default=(10, 10), type=tuple)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-occl_size', '--occlusion_size'], dest='occlusion_size', nargs=None, const=None, default=(10, 10), type=<class 'tuple'>, choices=None, help='occlusion size', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93KW-VGjMzun",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title ノイズのタイプ light / occl / blackout\n",
        "#@body {light,occl,blackout} weight_diff weight_nc step seeds\n",
        "#                   grad_iterations threshold\n",
        "noise_type = \"light\" #@param [\"light\", \"occl\", \"blackout\"]\n",
        "weight_diff = \"0.1\" #@param {type:\"string\"}\n",
        "weight_nc = \"0.0\" #@param {type:\"string\"}\n",
        "step = \"10.5\" #@param {type:\"string\"}　#50だとほぼ白飛び\n",
        "seeds = \"10\" #@param {type:\"string\"}\n",
        "grad_iterations = \"50\" #@param {type:\"string\"}\n",
        "threshold = \"0.1\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk80fCuPts9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = parser.parse_args([noise_type, weight_diff, weight_nc, step, seeds, grad_iterations, threshold])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tTYrllplJNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x.reshape(x.shape[1], x.shape[2])  # original shape (1,img_rows, img_cols,1)\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "def constraint_occl(gradients, start_point, rect_shape):\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def constraint_light(gradients):\n",
        "    new_grads = np.ones_like(gradients)\n",
        "    grad_mean = np.mean(gradients)\n",
        "    return grad_mean * new_grads\n",
        "\n",
        "\n",
        "def constraint_black(gradients, rect_shape=(6, 6)):\n",
        "    start_point = (\n",
        "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    if np.mean(patch) < 0:\n",
        "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def init_coverage_tables(model1, model2, model3):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    model_layer_dict2 = defaultdict(bool)\n",
        "    model_layer_dict3 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    init_dict(model2, model_layer_dict2)\n",
        "    init_dict(model3, model_layer_dict3)\n",
        "    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n",
        "\n",
        "\n",
        "def init_dict(model, model_layer_dict):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_dict[(layer.name, index)] = False\n",
        "\n",
        "\n",
        "def neuron_to_cover(model_layer_dict):\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "    else:\n",
        "        layer_name, index = random.choice(model_layer_dict.keys())\n",
        "    return layer_name, index\n",
        "\n",
        "\n",
        "def neuron_covered(model_layer_dict):\n",
        "    covered_neurons = len([v for v in model_layer_dict.values() if v])\n",
        "    total_neurons = len(model_layer_dict)\n",
        "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n",
        "\n",
        "\n",
        "def update_coverage(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n",
        "                model_layer_dict[(layer_names[i], num_neuron)] = True\n",
        "\n",
        "\n",
        "def full_coverage(model_layer_dict):\n",
        "    if False in model_layer_dict.values():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
        "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
        "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
        "    X_scaled = X_std * (rmax - rmin) + rmin\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def fired(model, layer_name, index, input_data, threshold=0):\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
        "    scaled = scale(intermediate_layer_output)\n",
        "    if np.mean(scaled[..., index]) > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def diverged(predictions1, predictions2, predictions3, target):\n",
        "    #     if predictions2 == predictions3 == target and predictions1 != target:\n",
        "    if not predictions1 == predictions2 == predictions3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "#add\n",
        "def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n",
        "    return [os.path.join(root, f)\n",
        "            for root, _, files in os.walk(directory) for f in files\n",
        "            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDDT47Er6u23",
        "colab_type": "code",
        "outputId": "123b08c5-e7bc-48cd-8cfd-7fa5f3bcec4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        }
      },
      "source": [
        "'''\n",
        "LeNet-1\n",
        "'''\n",
        "def Model1(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        print(x_train.shape)\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(4, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(12, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "#        model.save_weights('./Model1.h5')\n",
        "        model.save_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "#        model.load_weights('./Model1.h5')\n",
        "        model.load_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        print(bcolors.OKBLUE + 'Model1 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model1(train=True)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.4489 - acc: 0.8663 - val_loss: 0.1621 - val_acc: 0.9519\n",
            "\n",
            "\n",
            "Overall Test score: 0.16206311136335136\n",
            "Overall Test accuracy: 0.9519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU88awZf68JO",
        "colab_type": "code",
        "outputId": "e3d97608-8ee9-4382-851a-1d75a0e6fb75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "'''\n",
        "LeNet-4\n",
        "'''\n",
        "def Model2(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(84, activation='relu', name='fc1')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model2.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model2.h5')\n",
        "        print(bcolors.OKBLUE + 'Model2 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model2(train=True)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.3611 - acc: 0.8906 - val_loss: 0.1070 - val_acc: 0.9648\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 1s 22us/step - loss: 0.0841 - acc: 0.9743 - val_loss: 0.0569 - val_acc: 0.9802\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0586 - acc: 0.9822 - val_loss: 0.0501 - val_acc: 0.9850\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 25us/step - loss: 0.0463 - acc: 0.9857 - val_loss: 0.0378 - val_acc: 0.9875\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0374 - acc: 0.9884 - val_loss: 0.0492 - val_acc: 0.9835\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0321 - acc: 0.9899 - val_loss: 0.0364 - val_acc: 0.9865\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0282 - acc: 0.9912 - val_loss: 0.0303 - val_acc: 0.9886\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 25us/step - loss: 0.0239 - acc: 0.9925 - val_loss: 0.0295 - val_acc: 0.9896\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0209 - acc: 0.9933 - val_loss: 0.0517 - val_acc: 0.9836\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0182 - acc: 0.9942 - val_loss: 0.0325 - val_acc: 0.9895\n",
            "\n",
            "\n",
            "Overall Test score: 0.032549199012853205\n",
            "Overall Test accuracy: 0.9895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufs67uRN7KYL",
        "colab_type": "code",
        "outputId": "97c75c3e-7106-4a62-d0bf-8987ffb1262c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "'''\n",
        "LeNet-5\n",
        "'''\n",
        "def Model3(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(120, activation='relu', name='fc1')(x)\n",
        "    x = Dense(84, activation='relu', name='fc2')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model3.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model3.h5')\n",
        "        print(bcolors.OKBLUE + 'Model3 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model3(train=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.3948 - acc: 0.8747 - val_loss: 0.0904 - val_acc: 0.9711\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 25us/step - loss: 0.0824 - acc: 0.9742 - val_loss: 0.0524 - val_acc: 0.9836\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 25us/step - loss: 0.0560 - acc: 0.9824 - val_loss: 0.0707 - val_acc: 0.9756\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 25us/step - loss: 0.0428 - acc: 0.9863 - val_loss: 0.0329 - val_acc: 0.9896\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0343 - acc: 0.9892 - val_loss: 0.0325 - val_acc: 0.9894\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 0.0288 - acc: 0.9909 - val_loss: 0.0345 - val_acc: 0.9888\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0240 - acc: 0.9923 - val_loss: 0.0555 - val_acc: 0.9827\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0204 - acc: 0.9934 - val_loss: 0.0313 - val_acc: 0.9904\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0180 - acc: 0.9943 - val_loss: 0.0365 - val_acc: 0.9886\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 0.0151 - acc: 0.9951 - val_loss: 0.0400 - val_acc: 0.9883\n",
            "\n",
            "\n",
            "Overall Test score: 0.039955458272859685\n",
            "Overall Test accuracy: 0.9883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9QiwlW7WWR",
        "colab_type": "code",
        "outputId": "174f0a9d-e05c-4d52-f45e-377ad147b728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# input MNIST image dimensions \n",
        "img_rows, img_cols = 28, 28\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "#define the model instance\n",
        "model1 = Model1(input_tensor=input_tensor)\n",
        "model2 = Model2(input_tensor=input_tensor)\n",
        "model3 = Model3(input_tensor=input_tensor)\n",
        "\n",
        "#define the dictionary of neuron coverage\n",
        "model_layer_dict1, model_layer_dict2, model_layer_dict3 = init_coverage_tables(model1, model2, model3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[94mModel1 loaded\u001b[0m\n",
            "\u001b[94mModel2 loaded\u001b[0m\n",
            "\u001b[94mModel3 loaded\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MS5H74Q4XA",
        "colab_type": "text"
      },
      "source": [
        "### 入力するデータの選別(1/2)\n",
        "\n",
        "０～９の数字をそれぞれ束ねてデータを準備する\n",
        "[00...011...1......99...9]。\n",
        "\n",
        "* test_per_fig_x: \n",
        "    各数字に対する画像データを格納\n",
        "* test_per_fig_y: \n",
        "    各数字に対する教師ラベルを格納\n",
        "* tests_x: \n",
        "    test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: \n",
        "    tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brFIdSfL8nHP",
        "colab_type": "code",
        "outputId": "12bdad94-0080-4cde-fa9f-7f6fa27ce6d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# load the MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "num_fig = 10\n",
        "test_per_fig_x = np.array([])\n",
        "test_per_fig_y = np.array([])\n",
        "tests_x = np.array([])\n",
        "tests_y = np.array([])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "#x_train = x_train.astype('float')\n",
        "#x_test = x_test.astype('float')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "length = int(args.seeds)\n",
        "#length = int(args.seeds)\n",
        "for i in range(num_fig):\n",
        "  cond = [(x==i) for x in y_test]\n",
        "  test_per_fig_x = x_test[cond]\n",
        "  test_per_fig_y = y_test[cond]\n",
        "#  np.set_printoptions(formatter={'int': '{:07d}'.format})\n",
        "  print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "  tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "  tests_y = np.append(tests_y, test_per_fig_y[:length])\n",
        "#  conds = [conds, cond]\n",
        "print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "\n",
        "###### MNIST data, shuffled and split by train and test sets\n",
        "#####(_, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "###### modify the numpy data for the Keras model\n",
        "#####x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "#####input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "#tests_x.shape\n",
        "tests_x = tests_x.astype('float32')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure: 0 , shape: (980, 28, 28) , deviation: -20\n",
            "figure: 1 , shape: (1135, 28, 28) , deviation: 135\n",
            "figure: 2 , shape: (1032, 28, 28) , deviation: 32\n",
            "figure: 3 , shape: (1010, 28, 28) , deviation: 10\n",
            "figure: 4 , shape: (982, 28, 28) , deviation: -18\n",
            "figure: 5 , shape: (892, 28, 28) , deviation: -108\n",
            "figure: 6 , shape: (958, 28, 28) , deviation: -42\n",
            "figure: 7 , shape: (1028, 28, 28) , deviation: 28\n",
            "figure: 8 , shape: (974, 28, 28) , deviation: -26\n",
            "figure: 9 , shape: (1009, 28, 28) , deviation: 9\n",
            "check! 10.0 = 10 equal?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUOK_EjJW2KT",
        "colab_type": "text"
      },
      "source": [
        "## 入力するデータの選別(2/2)\n",
        "０～９の数字をそれぞれ束ねてデータを準備する [00...000]。\n",
        "\n",
        "* test_per_fig_x: 各数字に対する画像データを格納\n",
        "* test_per_fig_y: 各数字に対する教師ラベルを格納\n",
        "* tests_x: test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "id": "SLee__9UAD1Y",
        "colab": {}
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "#load the MNIST \n",
        "\n",
        "def create_data(start_fig, num_fi, length):\n",
        "  (datax_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  test_per_fig_x = np.array([])\n",
        "  test_per_fig_y = np.array([])\n",
        "  tests_x = np.array([])\n",
        "  tests_y = np.array([])\n",
        "\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test  = x_test.astype('float32')\n",
        "  x_train = x_train.astype('float')\n",
        "  x_test = x_test.astype('float')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  #length = int(args.seeds/num_fig)\n",
        "  #length = int(args.seeds)\n",
        "  for i in range(start_fig, start_fig+num_fig):\n",
        "  #i=0\n",
        "    cond = [(x==i) for x in y_test]\n",
        "    test_per_fig_x = x_test[cond]\n",
        "    test_per_fig_y = y_test[cond]\n",
        "    print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "    tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "    tests_y = np.append(tests_y, test_per_fig_y[:length])  \n",
        "\n",
        "  #conds = [conds, cond]\n",
        "  print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "  tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "  tests_x.shape\n",
        "  tests_x = tests_x.astype('float32')\n",
        "  return tests_x, tests_y, length\n",
        "  tests_x, tests_y, length = create_data(0, 10, int(args.seeds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyo2SSSz8vkT",
        "colab_type": "code",
        "outputId": "b6109fec-fd5e-4c56-a613-b10bc8320732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "print(tests_x.shape)\n",
        "print(tests_y)\n",
        "for i in range(0,1):\n",
        "  print(i)\n",
        "print(length)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 28, 28, 1)\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 4. 4. 4. 4. 4. 4. 4. 4.\n",
            " 4. 4. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 7. 7.\n",
            " 7. 7. 7. 7. 7. 7. 7. 7. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 9. 9. 9. 9. 9. 9.\n",
            " 9. 9. 9. 9.]\n",
            "0\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLlKIMxG9kEA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfdTZi4mUPf",
        "colab_type": "code",
        "outputId": "8927f854-4524-4615-8290-3fc39bbeacb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "def neuron_output(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "    \n",
        "    out_list = []\n",
        "    out_list_scale = []\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "#        scaled = scale(intermediate_layer_output[0])\n",
        "        scaled = scale(intermediate_layer_output)\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            out_list.append(np.mean(intermediate_layer_output[..., num_neuron]))\n",
        "            out_list_scale.append(np.mean(scaled[..., num_neuron]))\n",
        "\n",
        "    return out_list, out_list_scale\n",
        "\n",
        "\n",
        "def deepXplore(model_layer_dict1, model_layer_dict2, model_layer_dict3, tests_x, model1, model2, model3, num_fig, start_fig, length):\n",
        "  #print(\"test01\")\n",
        "  count_already = 0\n",
        "  count_found = 0\n",
        "  count_not_found = 0\n",
        "  temp_per_nc1 = np.array([])\n",
        "  temp_per_nc2 = np.array([])\n",
        "  temp_per_nc3 = np.array([])\n",
        "  temp_num_nc1 = np.array([])\n",
        "  temp_num_nc2 = np.array([])\n",
        "  temp_num_nc3 = np.array([])\n",
        "  #print(\"test10\")\n",
        "\n",
        "  #for each neuron\n",
        "  num_neurons1 = neuron_covered(model_layer_dict1)[1]\n",
        "  num_neurons2 = neuron_covered(model_layer_dict2)[1]\n",
        "  num_neurons3 = neuron_covered(model_layer_dict3)[1]\n",
        "  #print(\"test11\")\n",
        "\n",
        "  column_tmp1 = list(model_layer_dict1.keys())\n",
        "  column_tmp2 = list(model_layer_dict2.keys())\n",
        "  column_tmp3 = list(model_layer_dict3.keys())\n",
        "  #print(\"test12\")\n",
        "  df1 = pd.DataFrame(columns=column_tmp1)\n",
        "  df2 = pd.DataFrame(columns=column_tmp2)\n",
        "  df3 = pd.DataFrame(columns=column_tmp3)\n",
        "  df1_scale = pd.DataFrame(columns=column_tmp1)\n",
        "  df2_scale = pd.DataFrame(columns=column_tmp2)\n",
        "  df3_scale = pd.DataFrame(columns=column_tmp3)\n",
        "  df1_trace = pd.DataFrame(columns=column_tmp1)\n",
        "  df2_trace = pd.DataFrame(columns=column_tmp2)\n",
        "  df3_trace = pd.DataFrame(columns=column_tmp3)\n",
        "  df2 = pd.DataFrame(columns=column_tmp2)\n",
        "  df3 = pd.DataFrame(columns=column_tmp3)\n",
        "  tmp_list = [\"already_diff\", \"found\", \"not_found\", \"layer1\", \"index1\", \"layer2\", \"index2\", \"layer3\", \"index3\"]\n",
        "  bug_result = pd.DataFrame(columns=tmp_list)\n",
        "  trial = 1\n",
        "  #print(\"test13\")\n",
        "  #print(\"test02\")\n",
        "  for index_fig in range(num_fig):\n",
        "    index_fig = index_fig + start_fig\n",
        "    print(\"figure\"+str(index_fig))\n",
        "    for _ in range(length):\n",
        "      #gen_img = np.expand_dims(random.choice(tests_x), axis=0)\n",
        "      gen_img = np.expand_dims(tests_x[(length*index_fig + _)], axis=0)\n",
        "      orig_img = gen_img.copy()\n",
        "      # first check if input already induces differences\n",
        "      label1, label2, label3 = np.argmax(model1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "      if not label1 == label2 == label3:\n",
        "          count_already += 1\n",
        "          print(bcolors.OKGREEN + '   {}/{}. input already causes different outputs ({},{},{}) at({}, {}, {}): '.format(_, length, label1, label2, label3, count_already, count_found, count_not_found) + bcolors.ENDC)        \n",
        "\n",
        "          update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "          temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "          temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "          temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "          temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "          temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "          temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "                               \n",
        "          print(bcolors.OKGREEN + '     covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'% (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  neuron_covered(model_layer_dict2)[2], len(model_layer_dict3), neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "          averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                       neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +neuron_covered(model_layer_dict3)[1])\n",
        "          print(bcolors.OKGREEN + '     averaged covered neurons %.3f' % (averaged_nc) + bcolors.ENDC)\n",
        "\n",
        "          gen_img_deprocessed = deprocess_image(gen_img)\n",
        "\n",
        "          # save the result to disk\n",
        "          outputfilepath0 = os.path.join(output_dir, 'already_differ_' + str(label1) + '_' + str(label2) + '_' + str(label3) +'_['+ str(_) +  '].png')\n",
        "          imageio.imwrite(outputfilepath0, gen_img_deprocessed)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df1 = df1.append(temp)\n",
        "          df1_scale = df1_scale.append(temp_scale)\n",
        "##############################\n",
        "          df1_trace = df1_trace.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df2 = df2.append(temp)\n",
        "          df2_scale = df2_scale.append(temp_scale)\n",
        "##############################\n",
        "          df2_trace = df2_trace.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "          temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df3 = df3.append(temp)\n",
        "          df3_scale = df3_scale.append(temp_scale)\n",
        "##############################\n",
        "          df3_trace = df3_trace.append(temp_scale)\n",
        "\n",
        "          #print(\"test10\")\n",
        "          temp = [1, 0, 0, None, None, None, None, None, None]\n",
        "          #print(\"test11\")\n",
        "          temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          #print(\"test12\")\n",
        "          bug_result = bug_result.append(temp)\n",
        "          #print(\"test13\")\n",
        "          trial += 1\n",
        "          continue\n",
        "\n",
        "      # if all label agrees\n",
        "      orig_label = label1\n",
        "      layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n",
        "      layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n",
        "      layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n",
        "\n",
        "\n",
        "\n",
        "      # construct joint loss function\n",
        "      if args.target_model == 0:\n",
        "        loss1 = -args.weight_diff * K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 1:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = -args.weight_diff * K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 2:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n",
        "      loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n",
        "      loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n",
        "      layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron)\n",
        "\n",
        "      # for adversarial image generation\n",
        "      final_loss = K.mean(layer_output)\n",
        "\n",
        "      # we compute the gradient of the input picture wrt this loss\n",
        "      grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "\n",
        "      # this function returns the loss and grads given the input picture\n",
        "      iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n",
        "\n",
        "      #print(\"test03\")\n",
        "      # we run gradient ascent for some steps\n",
        "      for iters in range(args.grad_iterations):\n",
        "        loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = iterate([gen_img])\n",
        "        if args.transformation == 'light':\n",
        "            grads_value = constraint_light(grads_value)  # constraint the gradients value\n",
        "        elif args.transformation == 'occl':\n",
        "            grads_value = constraint_occl(grads_value, args.start_point,\n",
        "                                          args.occlusion_size)  # constraint the gradients value\n",
        "        elif args.transformation == 'blackout':\n",
        "            grads_value = constraint_black(grads_value)  # constraint the gradients value\n",
        "\n",
        "        gen_img += grads_value * args.step\n",
        "        predictions1 = np.argmax(model1.predict(gen_img)[0])\n",
        "        predictions2 = np.argmax(model2.predict(gen_img)[0])\n",
        "        predictions3 = np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "##############################\n",
        "        temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "        #print(temp)\n",
        "        temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "        temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial) + \"_\" + str(iters))\n",
        "        df1_trace = df1_trace.append(temp_scale)\n",
        "\n",
        "        temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "        temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "        temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial) + \"_\" + str(iters))\n",
        "        df2_trace = df2_trace.append(temp_scale)\n",
        "\n",
        "        temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "        temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "        temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial) + \"_\" + str(iters))\n",
        "        df3_trace = df3_trace.append(temp_scale)\n",
        "\n",
        "        #gen_img_deprocessed = deprocess_image(gen_img)\n",
        "        #outputfilepath = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  ']_' + str(iters) + '.png')\n",
        "        #imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "\n",
        "        #print(\"test04\")\n",
        "        if not predictions1 == predictions2 == predictions3:\n",
        "            #print(\"test04-01\")\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            count_found += 1\n",
        "            print(bcolors.OKBLUE + '%4d/%d. found at %d! covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f at (%d, %d, %d)'\n",
        "#                  % (_, args.seeds, iters, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  % (_, length, iters + 1, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                     neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                     neuron_covered(model_layer_dict3)[2], count_already, count_found, count_not_found) + bcolors.ENDC)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            print(bcolors.OKBLUE + '     averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n",
        "\n",
        "            #print(\"test04-02\")\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            #print(\"test04-03\")\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  '].png')\n",
        "            #print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "#            outputfilepath2 = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '_orig.png')\n",
        "            outputfilepath2 = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  ']_orig.png')\n",
        "            #print(outputfilepath2)\n",
        "            imageio.imwrite(outputfilepath2, orig_img_deprocessed)\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "            #print(\"test04-04\")\n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "     \n",
        "            #print(\"test04-05\")\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            #print(\"test04-06\")\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "            #print(\"test04-07\")\n",
        "            temp = [0, iters+1, 0, layer_name1, index1, layer_name2, index2,layer_name3, index3]\n",
        "            temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            bug_result = bug_result.append(temp)\n",
        "            trial += 1\n",
        "            break\n",
        "          \n",
        "          #add\n",
        "        #print(\"test05\")\n",
        "        if iters == (args.grad_iterations-1):\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            count_not_found += 1\n",
        "#              print('%4d/%d. test suite was not found: averaged covered neurons %.3f at %d/%d' % (_, args.seeds, averaged_nc, count_not_found, count_already + count_found + count_not_found))\n",
        "            print('%4d/%d. test suite was not found: averaged covered neurons %.3f at (%d, %d, %d)' % (_, length, averaged_nc, count_already, count_found, count_not_found))\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            #orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, 'not_found_' + str(label1)+'_['+ str(_) + '].png')\n",
        "#           print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "\n",
        "            \n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "\n",
        "            temp = [0, 0, 1, layer_name1, index1, layer_name2, index2,layer_name3, index3]\n",
        "            temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            bug_result = bug_result.append(temp)\n",
        "\n",
        "            trial += 1\n",
        "            \n",
        "            #break\n",
        "\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "  #print(\"test06\")\n",
        "  temp_per_nc1=temp_per_nc1.reshape(num_fig, length)\n",
        "  temp_per_nc2=temp_per_nc2.reshape(num_fig, length)\n",
        "  temp_per_nc3=temp_per_nc3.reshape(num_fig, length)\n",
        "  temp_num_nc1=temp_num_nc1.reshape(num_fig, length)\n",
        "  temp_num_nc2=temp_num_nc2.reshape(num_fig, length)\n",
        "  temp_num_nc3=temp_num_nc3.reshape(num_fig, length)\n",
        "\n",
        "  #print(\"test07\")\n",
        "#  return df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result\n",
        "  return df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result, df1_trace"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
            "Wall time: 11 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHi1ffJuWH5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "f8fca4ba-3733-4799-a194-89e724f28dcc"
      },
      "source": [
        "print(length, num_fig)\n",
        "print(output_dir)\n",
        "column_tmp1 = list(model_layer_dict1.keys())\n",
        "df1 = pd.DataFrame(columns=column_tmp1)\n",
        "df1.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 10\n",
            "/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200209_0046\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(block1_conv1, 0)</th>\n",
              "      <th>(block1_conv1, 1)</th>\n",
              "      <th>(block1_conv1, 2)</th>\n",
              "      <th>(block1_conv1, 3)</th>\n",
              "      <th>(block1_pool1, 0)</th>\n",
              "      <th>(block1_pool1, 1)</th>\n",
              "      <th>(block1_pool1, 2)</th>\n",
              "      <th>(block1_pool1, 3)</th>\n",
              "      <th>(block2_conv1, 0)</th>\n",
              "      <th>(block2_conv1, 1)</th>\n",
              "      <th>(block2_conv1, 2)</th>\n",
              "      <th>(block2_conv1, 3)</th>\n",
              "      <th>(block2_conv1, 4)</th>\n",
              "      <th>(block2_conv1, 5)</th>\n",
              "      <th>(block2_conv1, 6)</th>\n",
              "      <th>(block2_conv1, 7)</th>\n",
              "      <th>(block2_conv1, 8)</th>\n",
              "      <th>(block2_conv1, 9)</th>\n",
              "      <th>(block2_conv1, 10)</th>\n",
              "      <th>(block2_conv1, 11)</th>\n",
              "      <th>(block2_pool1, 0)</th>\n",
              "      <th>(block2_pool1, 1)</th>\n",
              "      <th>(block2_pool1, 2)</th>\n",
              "      <th>(block2_pool1, 3)</th>\n",
              "      <th>(block2_pool1, 4)</th>\n",
              "      <th>(block2_pool1, 5)</th>\n",
              "      <th>(block2_pool1, 6)</th>\n",
              "      <th>(block2_pool1, 7)</th>\n",
              "      <th>(block2_pool1, 8)</th>\n",
              "      <th>(block2_pool1, 9)</th>\n",
              "      <th>(block2_pool1, 10)</th>\n",
              "      <th>(block2_pool1, 11)</th>\n",
              "      <th>(before_softmax, 0)</th>\n",
              "      <th>(before_softmax, 1)</th>\n",
              "      <th>(before_softmax, 2)</th>\n",
              "      <th>(before_softmax, 3)</th>\n",
              "      <th>(before_softmax, 4)</th>\n",
              "      <th>(before_softmax, 5)</th>\n",
              "      <th>(before_softmax, 6)</th>\n",
              "      <th>(before_softmax, 7)</th>\n",
              "      <th>(before_softmax, 8)</th>\n",
              "      <th>(before_softmax, 9)</th>\n",
              "      <th>(predictions, 0)</th>\n",
              "      <th>(predictions, 1)</th>\n",
              "      <th>(predictions, 2)</th>\n",
              "      <th>(predictions, 3)</th>\n",
              "      <th>(predictions, 4)</th>\n",
              "      <th>(predictions, 5)</th>\n",
              "      <th>(predictions, 6)</th>\n",
              "      <th>(predictions, 7)</th>\n",
              "      <th>(predictions, 8)</th>\n",
              "      <th>(predictions, 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [(block1_conv1, 0), (block1_conv1, 1), (block1_conv1, 2), (block1_conv1, 3), (block1_pool1, 0), (block1_pool1, 1), (block1_pool1, 2), (block1_pool1, 3), (block2_conv1, 0), (block2_conv1, 1), (block2_conv1, 2), (block2_conv1, 3), (block2_conv1, 4), (block2_conv1, 5), (block2_conv1, 6), (block2_conv1, 7), (block2_conv1, 8), (block2_conv1, 9), (block2_conv1, 10), (block2_conv1, 11), (block2_pool1, 0), (block2_pool1, 1), (block2_pool1, 2), (block2_pool1, 3), (block2_pool1, 4), (block2_pool1, 5), (block2_pool1, 6), (block2_pool1, 7), (block2_pool1, 8), (block2_pool1, 9), (block2_pool1, 10), (block2_pool1, 11), (before_softmax, 0), (before_softmax, 1), (before_softmax, 2), (before_softmax, 3), (before_softmax, 4), (before_softmax, 5), (before_softmax, 6), (before_softmax, 7), (before_softmax, 8), (before_softmax, 9), (predictions, 0), (predictions, 1), (predictions, 2), (predictions, 3), (predictions, 4), (predictions, 5), (predictions, 6), (predictions, 7), (predictions, 8), (predictions, 9)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSSBfoAZreWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f29f5e45-c363-44e1-bd65-3779061d27c3"
      },
      "source": [
        "index_fig = 0\n",
        "trial = 0\n",
        "gen_img = np.expand_dims(tests_x[(length*index_fig + 0)], axis=0)\n",
        "temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "print(type(temp_scale))\n",
        "temp_scale.name"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0_0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yymgs3ldNiDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47c14a0e-f02d-46b9-b358-73966d21b2c3"
      },
      "source": [
        "%%time\n",
        "#num_fig = 1\n",
        "#start_fig = 0\n",
        "#deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "\n",
        "num_fig = 1\n",
        "for start_fig in range(10):\n",
        "#for start_fig in range(4,10):\n",
        "#for start_fig in range(2):\n",
        "#  df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result, df1_trace, df2_trace. df3_trace= deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "  df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result, df1_trace= deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "#  df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result= deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "  df1_scale.to_csv(output_dir+ \"/03_16_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")\n",
        "  bug_result.to_csv(output_dir+ \"/03_16_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_index.csv\")\n",
        "  df1_trace.to_csv(output_dir+ \"/03_16_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_trace.csv\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure0\n",
            "\u001b[94m   0/10. found at 4! covered neurons percentage 52 neurons 0.481, 148 neurons 0.453, 268 neurons 0.455 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.457\u001b[0m\n",
            "\u001b[94m   1/10. found at 6! covered neurons percentage 52 neurons 0.827, 148 neurons 0.642, 268 neurons 0.571 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.622\u001b[0m\n",
            "   2/10. test suite was not found: averaged covered neurons 0.652 at (0, 2, 1)\n",
            "   3/10. test suite was not found: averaged covered neurons 0.669 at (0, 2, 2)\n",
            "\u001b[94m   4/10. found at 2! covered neurons percentage 52 neurons 0.846, 148 neurons 0.689, 268 neurons 0.646 at (0, 3, 2)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.682\u001b[0m\n",
            "   5/10. test suite was not found: averaged covered neurons 0.690 at (0, 3, 3)\n",
            "   6/10. test suite was not found: averaged covered neurons 0.699 at (0, 3, 4)\n",
            "   7/10. test suite was not found: averaged covered neurons 0.699 at (0, 3, 5)\n",
            "\u001b[94m   8/10. found at 10! covered neurons percentage 52 neurons 0.865, 148 neurons 0.703, 268 neurons 0.675 at (0, 4, 5)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.705\u001b[0m\n",
            "\u001b[94m   9/10. found at 3! covered neurons percentage 52 neurons 0.865, 148 neurons 0.703, 268 neurons 0.683 at (0, 5, 5)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.709\u001b[0m\n",
            "figure1\n",
            "\u001b[94m   0/10. found at 3! covered neurons percentage 52 neurons 0.865, 148 neurons 0.791, 268 neurons 0.750 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.776\u001b[0m\n",
            "\u001b[94m   1/10. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.818, 268 neurons 0.813 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.823\u001b[0m\n",
            "\u001b[94m   2/10. found at 3! covered neurons percentage 52 neurons 0.885, 148 neurons 0.824, 268 neurons 0.828 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.833\u001b[0m\n",
            "\u001b[94m   3/10. found at 3! covered neurons percentage 52 neurons 0.885, 148 neurons 0.845, 268 neurons 0.840 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.846\u001b[0m\n",
            "\u001b[94m   4/10. found at 3! covered neurons percentage 52 neurons 0.885, 148 neurons 0.845, 268 neurons 0.847 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.850\u001b[0m\n",
            "\u001b[94m   5/10. found at 3! covered neurons percentage 52 neurons 0.885, 148 neurons 0.845, 268 neurons 0.851 at (0, 6, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.853\u001b[0m\n",
            "\u001b[94m   6/10. found at 3! covered neurons percentage 52 neurons 0.885, 148 neurons 0.845, 268 neurons 0.851 at (0, 7, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.853\u001b[0m\n",
            "\u001b[94m   7/10. found at 1! covered neurons percentage 52 neurons 0.885, 148 neurons 0.845, 268 neurons 0.854 at (0, 8, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.855\u001b[0m\n",
            "\u001b[94m   8/10. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.845, 268 neurons 0.858 at (0, 9, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.857\u001b[0m\n",
            "\u001b[94m   9/10. found at 1! covered neurons percentage 52 neurons 0.885, 148 neurons 0.845, 268 neurons 0.866 at (0, 10, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.861\u001b[0m\n",
            "figure2\n",
            "\u001b[94m   0/10. found at 1! covered neurons percentage 52 neurons 0.885, 148 neurons 0.865, 268 neurons 0.869 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.870\u001b[0m\n",
            "\u001b[94m   1/10. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.878, 268 neurons 0.869 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.880\u001b[0m\n",
            "   2/10. test suite was not found: averaged covered neurons 0.885 at (0, 2, 1)\n",
            "\u001b[94m   3/10. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.899, 268 neurons 0.892 at (0, 3, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.900\u001b[0m\n",
            "\u001b[94m   4/10. found at 3! covered neurons percentage 52 neurons 0.962, 148 neurons 0.905, 268 neurons 0.907 at (0, 4, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.912\u001b[0m\n",
            "   5/10. test suite was not found: averaged covered neurons 0.912 at (0, 4, 2)\n",
            "\u001b[94m   6/10. found at 3! covered neurons percentage 52 neurons 0.981, 148 neurons 0.905, 268 neurons 0.907 at (0, 5, 2)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.915\u001b[0m\n",
            "   7/10. test suite was not found: averaged covered neurons 0.915 at (0, 5, 3)\n",
            "\u001b[94m   8/10. found at 46! covered neurons percentage 52 neurons 0.981, 148 neurons 0.905, 268 neurons 0.907 at (0, 6, 3)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.915\u001b[0m\n",
            "   9/10. test suite was not found: averaged covered neurons 0.917 at (0, 6, 4)\n",
            "figure3\n",
            "\u001b[92m   0/10. input already causes different outputs (3,8,3) at(1, 0, 0): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.981, 148 neurons 0.905, 268 neurons 0.918\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.921\u001b[0m\n",
            "\u001b[94m   1/10. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.905, 268 neurons 0.918 at (1, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.921\u001b[0m\n",
            "\u001b[94m   2/10. found at 39! covered neurons percentage 52 neurons 0.981, 148 neurons 0.905, 268 neurons 0.918 at (1, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.921\u001b[0m\n",
            "\u001b[94m   3/10. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.905, 268 neurons 0.918 at (1, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.921\u001b[0m\n",
            "\u001b[94m   4/10. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.905, 268 neurons 0.929 at (1, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.927\u001b[0m\n",
            "\u001b[94m   5/10. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.905, 268 neurons 0.933 at (1, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.929\u001b[0m\n",
            "\u001b[94m   6/10. found at 10! covered neurons percentage 52 neurons 0.981, 148 neurons 0.905, 268 neurons 0.933 at (1, 6, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.929\u001b[0m\n",
            "\u001b[94m   7/10. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.905, 268 neurons 0.933 at (1, 7, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.929\u001b[0m\n",
            "   8/10. test suite was not found: averaged covered neurons 0.929 at (1, 7, 1)\n",
            "\u001b[94m   9/10. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.905, 268 neurons 0.933 at (1, 8, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.929\u001b[0m\n",
            "figure4\n",
            "\u001b[94m   0/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.912, 268 neurons 0.944 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.938\u001b[0m\n",
            "\u001b[94m   1/10. found at 3! covered neurons percentage 52 neurons 0.981, 148 neurons 0.912, 268 neurons 0.944 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.938\u001b[0m\n",
            "\u001b[94m   2/10. found at 3! covered neurons percentage 52 neurons 0.981, 148 neurons 0.912, 268 neurons 0.944 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.938\u001b[0m\n",
            "\u001b[94m   3/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.912, 268 neurons 0.944 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.938\u001b[0m\n",
            "\u001b[94m   4/10. found at 6! covered neurons percentage 52 neurons 0.981, 148 neurons 0.912, 268 neurons 0.944 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.938\u001b[0m\n",
            "\u001b[92m   5/10. input already causes different outputs (6,4,4) at(1, 5, 0): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   6/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (1, 6, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "   7/10. test suite was not found: averaged covered neurons 0.940 at (1, 6, 1)\n",
            "\u001b[94m   8/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (1, 7, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   9/10. found at 4! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (1, 8, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "figure5\n",
            "   0/10. test suite was not found: averaged covered neurons 0.940 at (0, 0, 1)\n",
            "\u001b[94m   1/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 1, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   2/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 2, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   3/10. found at 4! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 3, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   4/10. found at 3! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 4, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   5/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 5, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   6/10. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 6, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   7/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 7, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   8/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 8, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "   9/10. test suite was not found: averaged covered neurons 0.940 at (0, 8, 2)\n",
            "figure6\n",
            "\u001b[94m   0/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   1/10. found at 5! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   2/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   3/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   4/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   5/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 6, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   6/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 7, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   7/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 8, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   8/10. found at 5! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 9, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "\u001b[94m   9/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.919, 268 neurons 0.944 at (0, 10, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.940\u001b[0m\n",
            "figure7\n",
            "\u001b[94m   0/10. found at 16! covered neurons percentage 52 neurons 0.981, 148 neurons 0.926, 268 neurons 0.944 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.942\u001b[0m\n",
            "\u001b[94m   1/10. found at 6! covered neurons percentage 52 neurons 0.981, 148 neurons 0.926, 268 neurons 0.944 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.942\u001b[0m\n",
            "\u001b[94m   2/10. found at 3! covered neurons percentage 52 neurons 0.981, 148 neurons 0.926, 268 neurons 0.944 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.942\u001b[0m\n",
            "\u001b[94m   3/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.926, 268 neurons 0.944 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.942\u001b[0m\n",
            "\u001b[94m   4/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.926, 268 neurons 0.944 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.942\u001b[0m\n",
            "\u001b[94m   5/10. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.926, 268 neurons 0.948 at (0, 6, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.944\u001b[0m\n",
            "\u001b[94m   6/10. found at 3! covered neurons percentage 52 neurons 0.981, 148 neurons 0.926, 268 neurons 0.948 at (0, 7, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.944\u001b[0m\n",
            "\u001b[94m   7/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.926, 268 neurons 0.948 at (0, 8, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.944\u001b[0m\n",
            "\u001b[94m   8/10. found at 3! covered neurons percentage 52 neurons 0.981, 148 neurons 0.926, 268 neurons 0.948 at (0, 9, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.944\u001b[0m\n",
            "\u001b[94m   9/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 10, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "figure8\n",
            "\u001b[94m   0/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   1/10. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   2/10. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   3/10. found at 5! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   4/10. found at 18! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   5/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 6, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   6/10. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 7, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   7/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 8, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   8/10. found at 4! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 9, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[92m   9/10. input already causes different outputs (3,3,8) at(1, 9, 0): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.947\u001b[0m\n",
            "figure9\n",
            "\u001b[94m   0/10. found at 3! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   1/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   2/10. found at 3! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "   3/10. test suite was not found: averaged covered neurons 0.947 at (0, 3, 1)\n",
            "\u001b[94m   4/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 4, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   5/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 5, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   6/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 6, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   7/10. found at 3! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 7, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   8/10. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.948 at (0, 8, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.947\u001b[0m\n",
            "\u001b[94m   9/10. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.932, 268 neurons 0.951 at (0, 9, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.949\u001b[0m\n",
            "CPU times: user 40min 51s, sys: 9.71 s, total: 41min 1s\n",
            "Wall time: 40min 59s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpsI1IOM-hUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "e3e75c82-5d25-4d63-9658-a1503b3fb125"
      },
      "source": [
        "df2_scale = pd.DataFrame()\n",
        "df3_scale = pd.DataFrame()\n",
        "df4_scale = pd.DataFrame()\n",
        "for start_fig in range(10):\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")\n",
        "  df2_scale = pd.concat([df2_scale, df1_scale])\n",
        "\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_index.csv\")\n",
        "  df3_scale = pd.concat([df3_scale, df1_scale])\n",
        "\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_trace.csv\")\n",
        "  df4_scale = pd.concat([df4_scale, df1_scale])\n",
        "\n",
        "tmp_list = list(df2_scale.iloc[:,0])\n",
        "df2_scale.index = tmp_list\n",
        "print(df2_scale.iloc[:,1:].head())\n",
        "\n",
        "tmp_list = list(df3_scale.iloc[:,0])\n",
        "df3_scale.index = tmp_list\n",
        "print(df3_scale.iloc[:,1:].head())\n",
        "\n",
        "tmp_list = list(df4_scale.iloc[:,0])\n",
        "df4_scale.index = tmp_list\n",
        "print(df4_scale.iloc[:,1:].head())\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     ('block1_conv1', 0)  ...  ('predictions', 9)\n",
            "0_1                0.384  ...               0.000\n",
            "0_2                0.387  ...               0.000\n",
            "0_3                0.257  ...               0.000\n",
            "0_4                0.421  ...               0.000\n",
            "0_5                0.223  ...               0.000\n",
            "\n",
            "[5 rows x 52 columns]\n",
            "     already_diff  found  not_found  ... index2        layer3 index3\n",
            "0_1         0.000  0.000      1.000  ...  6.000  block2_pool1  0.000\n",
            "0_2         0.000  0.000      1.000  ...  7.000           fc2 83.000\n",
            "0_3         0.000  0.000      1.000  ... 76.000           fc2  9.000\n",
            "0_4         0.000  0.000      1.000  ...  3.000  block1_pool1  3.000\n",
            "0_5         0.000  0.000      1.000  ...  2.000  block2_pool1 12.000\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "       ('block1_conv1', 0)  ...  ('predictions', 9)\n",
            "0_1_0                0.307  ...               0.000\n",
            "0_1_1                0.323  ...               0.000\n",
            "0_1_2                0.342  ...               0.000\n",
            "0_1_3                0.371  ...               0.000\n",
            "0_1_4                0.385  ...               0.000\n",
            "\n",
            "[5 rows x 52 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QtyLAZiPeKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "ffb8a329-a396-4024-9279-6c41a51f750f"
      },
      "source": [
        "df2_scale.iloc[0:6,1:]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>('block1_conv1', 0)</th>\n",
              "      <th>('block1_conv1', 1)</th>\n",
              "      <th>('block1_conv1', 2)</th>\n",
              "      <th>('block1_conv1', 3)</th>\n",
              "      <th>('block1_pool1', 0)</th>\n",
              "      <th>('block1_pool1', 1)</th>\n",
              "      <th>('block1_pool1', 2)</th>\n",
              "      <th>('block1_pool1', 3)</th>\n",
              "      <th>('block2_conv1', 0)</th>\n",
              "      <th>('block2_conv1', 1)</th>\n",
              "      <th>('block2_conv1', 2)</th>\n",
              "      <th>('block2_conv1', 3)</th>\n",
              "      <th>('block2_conv1', 4)</th>\n",
              "      <th>('block2_conv1', 5)</th>\n",
              "      <th>('block2_conv1', 6)</th>\n",
              "      <th>('block2_conv1', 7)</th>\n",
              "      <th>('block2_conv1', 8)</th>\n",
              "      <th>('block2_conv1', 9)</th>\n",
              "      <th>('block2_conv1', 10)</th>\n",
              "      <th>('block2_conv1', 11)</th>\n",
              "      <th>('block2_pool1', 0)</th>\n",
              "      <th>('block2_pool1', 1)</th>\n",
              "      <th>('block2_pool1', 2)</th>\n",
              "      <th>('block2_pool1', 3)</th>\n",
              "      <th>('block2_pool1', 4)</th>\n",
              "      <th>('block2_pool1', 5)</th>\n",
              "      <th>('block2_pool1', 6)</th>\n",
              "      <th>('block2_pool1', 7)</th>\n",
              "      <th>('block2_pool1', 8)</th>\n",
              "      <th>('block2_pool1', 9)</th>\n",
              "      <th>('block2_pool1', 10)</th>\n",
              "      <th>('block2_pool1', 11)</th>\n",
              "      <th>('before_softmax', 0)</th>\n",
              "      <th>('before_softmax', 1)</th>\n",
              "      <th>('before_softmax', 2)</th>\n",
              "      <th>('before_softmax', 3)</th>\n",
              "      <th>('before_softmax', 4)</th>\n",
              "      <th>('before_softmax', 5)</th>\n",
              "      <th>('before_softmax', 6)</th>\n",
              "      <th>('before_softmax', 7)</th>\n",
              "      <th>('before_softmax', 8)</th>\n",
              "      <th>('before_softmax', 9)</th>\n",
              "      <th>('predictions', 0)</th>\n",
              "      <th>('predictions', 1)</th>\n",
              "      <th>('predictions', 2)</th>\n",
              "      <th>('predictions', 3)</th>\n",
              "      <th>('predictions', 4)</th>\n",
              "      <th>('predictions', 5)</th>\n",
              "      <th>('predictions', 6)</th>\n",
              "      <th>('predictions', 7)</th>\n",
              "      <th>('predictions', 8)</th>\n",
              "      <th>('predictions', 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_1</th>\n",
              "      <td>0.384</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.434</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.258</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.311</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.183</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.432</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.413</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.481</td>\n",
              "      <td>0.234</td>\n",
              "      <td>0.386</td>\n",
              "      <td>0.305</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_2</th>\n",
              "      <td>0.387</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.447</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.366</td>\n",
              "      <td>0.259</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.259</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.185</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.543</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.437</td>\n",
              "      <td>0.558</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.586</td>\n",
              "      <td>0.187</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_3</th>\n",
              "      <td>0.257</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.317</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.145</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.290</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.152</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.231</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.198</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.556</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.614</td>\n",
              "      <td>0.534</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_4</th>\n",
              "      <td>0.421</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.478</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.145</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.234</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.371</td>\n",
              "      <td>0.263</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.638</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.517</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.656</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.651</td>\n",
              "      <td>0.358</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_5</th>\n",
              "      <td>0.223</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.145</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.192</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.470</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.457</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.493</td>\n",
              "      <td>0.266</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_6</th>\n",
              "      <td>0.217</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.272</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.152</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.296</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.438</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.723</td>\n",
              "      <td>0.355</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ('block1_conv1', 0)  ...  ('predictions', 9)\n",
              "0_1                0.384  ...               0.000\n",
              "0_2                0.387  ...               0.000\n",
              "0_3                0.257  ...               0.000\n",
              "0_4                0.421  ...               0.000\n",
              "0_5                0.223  ...               0.000\n",
              "0_6                0.217  ...               0.000\n",
              "\n",
              "[6 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ1DCqKX9lTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_list = list(df2_scale.iloc[:,0])\n",
        "#df2_scale = df2_scale.rename(index=tmp_list)\n",
        "type(tmp_list)\n",
        "df2_scale.index = tmp_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiVj9Wjd-rVv",
        "colab_type": "text"
      },
      "source": [
        "以下が、df1_scaleに格納されているニューロンの出力値から発火/非発火の集計をとるアルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B19EsPFQrak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "a7b1c230-9639-4d9e-9863-a82f01dd21b0"
      },
      "source": [
        "import pylab as pl\n",
        "\n",
        "thres = 0.1\n",
        "bools = df2_scale.iloc[:,1:] > thres\n",
        "\n",
        "bools.sum().hist()\n",
        "pl.xlabel(\"activation time\")\n",
        "pl.ylabel(\"number of neurons\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'number of neurons')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAVhklEQVR4nO3df5BdZX3H8feXgBKyNoDYHQVrYkux\nFAqSRVGs3RXrRKRKR1q1qFC1Ga2VaHE6OP2h7dQptEWrtP6I/EpthliRCtUOLU1ZsVXA3RhZfkhR\noJWIRIoEAwwQ+PaPc1av625ycnfPve593q+ZO3vP2XvP8304Nx/OPvec50RmIkkqx179LkCS1FsG\nvyQVxuCXpMIY/JJUGINfkgqzd78LaOKggw7KFStWdPXeBx98kGXLli1sQT/h7HMZ7PPgm29/Jycn\n783Mp81cvyiCf8WKFUxMTHT13vHxcUZHRxe2oJ9w9rkM9nnwzbe/EfE/s613qEeSCmPwS1JhDH5J\nKozBL0mFMfglqTAGvyQVxuCXpMIY/JJUGINfkgqzKK7claR+WnHW5/vS7sWr25mewiN+SSqMwS9J\nhTH4JakwBr8kFcbgl6TCGPySVBiDX5IKY/BLUmEMfkkqjMEvSYUx+CWpMAa/JBXG4Jekwhj8klSY\n1oI/Ii6MiG0RcWPHugMj4qqIuK3+eUBb7UuSZtfmEf/FwOoZ684CNmXmocCmelmS1EOtBX9mXgPc\nN2P1q4D19fP1wMlttS9Jml2vx/iHM/Pu+vl3gOEety9JxYvMbG/jESuAz2XmEfXy/Zm5f8fvv5eZ\ns47zR8QaYA3A8PDwqo0bN3ZVw44dOxgaGurqvYuVfS6Dfe6dqa3be94mwMrlS+bV37GxscnMHJm5\nvtf33L0nIp6emXdHxNOBbXO9MDPXAesARkZGcnR0tKsGx8fH6fa9i5V9LoN97p3T+3jP3Tb62+uh\nniuA0+rnpwGX97h9SSpem6dzXgJ8GTgsIu6KiDcDZwO/GhG3AS+tlyVJPdTaUE9mvm6OX53QVpuS\npN3zyl1JKozBL0mFMfglqTAGvyQVxuCXpMIY/JJUGINfkgpj8EtSYQx+SSqMwS9JhTH4JakwBr8k\nFcbgl6TC9PpGLD03tXV7X26icOfZr+h5m5LUhEf8klQYg1+SCmPwS1JhDH5JKozBL0mFMfglqTAG\nvyQVxuCXpMIY/JJUGINfkgpj8EtSYQx+SSqMwS9JhTH4JakwBr8kFWa3wR8RayPip6JyQURsjoiX\nzafRiHhXRNwUETdGxCURse98tidJaq7JEf+bMvMB4GXAAcAbgLO7bTAiDgbOAEYy8whgCfDabrcn\nSdozTYI/6p8nAp/MzJs61nVrb2BpROwN7Ad8e57bkyQ1FJm56xdEXAQcDKwEjqI6Qh/PzFVdNxqx\nFng/8DDwb5l56iyvWQOsARgeHl61cePGrtradt927nm420q7d+TBy3vfaG3Hjh0MDQ31rf1+sM9l\n6Fefp7Zu73mbACuXL5lXf8fGxiYzc2Tm+ibBvxdwNHB7Zt4fEU8FDs7MG7opJCIOAD4DvAa4H/g0\ncGlm/sNc7xkZGcmJiYlumuO8DZdz7lTvby3cz3vujo+PMzo62rf2+8E+l6FffV7Rh/t2A1y8etm8\n+hsRswb/bhMxM5+IiHuAw+uhmfl6KXBHZn63Luwy4IXAnMEvSVo4uw3yiDiH6uj8ZuDxenUC13TZ\n5v8Cx0XEflRDPScA3R3OS5L2WJMj+JOBwzLzkYVoMDOvi4hLgc3ATuCrwLqF2LYkafeaBP/twD7A\nggQ/QGa+F3jvQm1PktRck+B/CNgSEZvoCP/MPKO1qiRJrWkS/FfUD0nSAGhyVs/6iHgS8PP1qlsz\n87F2y5IktaXJWT2jwHrgTqordp8ZEadlZrdn9UiS+qjJUM+5wMsy81aAiPh54BKg6yt3JUn902Su\nnn2mQx8gM/+b6iwfSdIi1OSIfyIizueHV9aeihdcSdKi1ST43wa8nWoqZYAvAh9prSJJUqt2GfwR\nsQS4sJ498wO9KUmS1KZdjvFn5uPAs+rTOSVJA6DplA3/FRFXAA9Or8xM/wKQpEWoSfB/s37sBTyl\n3XIkSW1rcuXun/aiEElSbzS5cvdqqvn3f0RmvqSViiRpDlNbt3N6n+6GNUiaDPW8u+P5vsCrqebR\nlyQtQk2GeiZnrPqviLi+pXokSS1rMtRzYMfiXlRz9CxvrSJJUquaDPVMUo3xB9UQzx3Am9ssSpLU\nniZDPSt7UYgkqTd2OztnROwXEX8UEevq5UMj4qT2S5MktaHJtMwXAY8CL6yXtwJ/3lpFkqRWNQn+\nn83MvwQeA8jMh6jG+yVJi1CT4H80IpZSX8QVET8LPNJqVZKk1jQ5q+e9wJVU99rdABwPnN5mUZKk\n9jQ5q+eqiNgMHEc1xLM2M+9tvTJJUiuaHPFDNVXD9+rXHx4RZOY17ZUlSWpLkyt3zwFeA9wEPFGv\nTsDgl6RFqMkR/8nAYZnpF7qSNACanNVzO7BP24VIknqjyRH/Q8CWiNhEx2mcmXlGt41GxP7A+cAR\nVMNGb8rML3e7PUlSc02C/4r6sZA+BFyZmafUN3Lfb4G3L0maQ5PTOdcvZIMRsRx4MfW1AJn5KNWU\nEJKkHojMH7urYrsNRhwNrANuBo6imvZ5bWY+OON1a4A1AMPDw6s2btzYVXvb7tvOPQ/Pq+RFZ+Xy\nJQwNDfW7jJ7asWNHcX0u8bM9vJSi+jzff8tjY2OTmTkyc30/gn8EuBY4PjOvi4gPAQ9k5h/P9Z6R\nkZGcmJjoqr3zNlzOuVNNL1cYDBevXsbo6Gi/y+ip8fHx4vpc4mf7zCN3FtXn+f5bjohZg3/Os3oi\n4pP1z7Vdtzq7u4C7MvO6evlS4JgFbkOSNIddnc65KiKeAbwpIg6IiAM7H902mJnfAb4VEYfVq06g\nGvaRJPXArv5m+hiwCXg21Th851TMWa/v1juADfUZPbcDvz2PbUmS9sCcwZ+ZHwY+HBEfzcy3LWSj\nmbkF+LFxJ0lS+5qczvm2iDgK+OV61TWZeUO7ZUmS2tLknrtnABuAn64fGyLiHW0XJklqR5Pzot4C\nPH/6PPt6ts4vA+e1WZgkqR1NJmkL4PGO5cfxnruStGg1OeK/CLguIv6pXj4ZuKC9kiRJbWry5e4H\nImIceFG96rcz86utViVJak2ja58zczOwueVaJEk90GSMX5I0QAx+SSrMLoM/IpZExNW9KkaS1L5d\nBn9mPg48Ud88RZI0AJp8ubsDmIqIq4Af3CxlPvfclST1T5Pgv6x+SJIGQKN77kbEUuBnMvPWHtQk\nSWpRk0nafg3YAlxZLx8dEVe0XZgkqR1NTud8H/A84H74wVz687kJiySpj5oE/2OZuX3GuifaKEaS\n1L4mX+7eFBG/BSyJiEOBM4AvtVuWJKktTY743wH8IvAIcAnwAPDONouSJLWnyVk9DwF/WN+AJTPz\n++2XJUlqS5Ozeo6NiCngBqoLub4WEavaL02S1IYmY/wXAL+bmV8EiIgXUd2c5ZfaLEyS1I4mY/yP\nT4c+QGb+J7CzvZIkSW2a84g/Io6pn34hIj5O9cVuAq8BxtsvTZLUhl0N9Zw7Y/m9Hc+zhVokST0w\nZ/Bn5lgvC5Ek9cZuv9yNiP2BNwIrOl/vtMyStDg1OavnX4BrgSmcqkGSFr0mwb9vZv7+QjccEUuA\nCWBrZp600NuXJM2uyemcn4yI34mIp0fEgdOPBWh7LXDLAmxHkrQHmgT/o8BfAV8GJuvHxHwajYhD\ngFcA589nO5KkPReZuz4zMyJuB56XmfcuWKMRlwJ/ATwFePdsQz0RsQZYAzA8PLxq48aNXbW17b7t\n3PPwPIpdhFYuX8LQ0FDP253aOnP27t7pV5/7qcTP9vBSiurzfD/XY2Njk5k5MnN9kzH+bwAPdd3y\nDBFxErAtMycjYnSu12XmOmAdwMjISI6OzvnSXTpvw+WcO9Wkm4Pj4tXL6Pa/13ycftbne97mtH71\nuZ9K/GyfeeTOovrc1ue6yX/BB4EtEXE11dTMwLxO5zweeGVEnAjsC/xURPxDZr6+y+1JkvZAk+D/\nbP1YEJn5HuA9APUR/7sNfUnqnSbz8a/vRSGSpN5ocuXuHcwyN09mzvuG65k5jhO+SVJPNRnq6fxG\neF/gN4CFOI9fktQHuz2PPzP/r+OxNTP/huocfEnSItRkqOeYjsW9qP4CKOd8KkkaME0CvHNe/p3A\nncBvtlKNJKl1Tc7qcV5+SRogTYZ6ngy8mh+fj//P2itLktSWJkM9lwPbqSZne2Q3r5Uk/YRrEvyH\nZObq1iuRJPVEk2mZvxQRR7ZeiSSpJ5oc8b8IOL2+gvcRIIDMzF9qtTJJUiuaBP/LW69CktQzTU7n\n/J9eFCJJ6o0mY/ySpAFi8EtSYQx+SSqMwS9JhTH4JakwBr8kFcbgl6TCGPySVBiDX5IKY/BLUmEM\nfkkqjMEvSYUx+CWpMAa/JBXG4Jekwhj8klQYg1+SCtPz4I+IZ0bE1RFxc0TcFBFre12DJJWsyT13\nF9pO4MzM3BwRTwEmI+KqzLy5D7VIUnF6fsSfmXdn5ub6+feBW4CDe12HJJUqMrN/jUesAK4BjsjM\nB2b8bg2wBmB4eHjVxo0bu2pj233buefh+dW52Awvpbg+r1y+hKGhoX6X0VN+tgfffD/XY2Njk5k5\nMnN934I/IoaALwDvz8zLdvXakZGRnJiY6Kqd8zZczrlT/RjR6p8zj9xZXJ8vXr2M0dHRfpfRU362\nB998P9cRMWvw9+WsnojYB/gMsGF3oS9JWlj9OKsngAuAWzLzA71uX5JK148j/uOBNwAviYgt9ePE\nPtQhSUXq+WBZZv4nEL1uV5JU8cpdSSqMwS9JhTH4JakwBr8kFcbgl6TCGPySVBiDX5IKY/BLUmEM\nfkkqjMEvSYUx+CWpMAa/JBXG4JekwpRzKxupBSvO+nzf2j7zyL41rUXOI35JKozBL0mFMfglqTAG\nvyQVxuCXpMIY/JJUGINfkgpj8EtSYQx+SSqMwS9JhTH4JakwBr8kFcbgl6TCGPySVBiDX5IK05fg\nj4jVEXFrRHwjIs7qRw2SVKqeB39ELAH+Dng5cDjwuog4vNd1SFKp+nHE/zzgG5l5e2Y+CmwEXtWH\nOiSpSJGZvW0w4hRgdWa+pV5+A/D8zPy9Ga9bA6ypFw8Dbu2yyYOAe7t872Jln8tgnwfffPv7rMx8\n2syVP7H33M3MdcC6+W4nIiYyc2QBSlo07HMZ7PPga6u//Rjq2Qo8s2P5kHqdJKkH+hH8XwEOjYiV\nEfEk4LXAFX2oQ5KK1POhnszcGRG/B/wrsAS4MDNvarHJeQ8XLUL2uQz2efC10t+ef7krSeovr9yV\npMIY/JJUmIEO/kGfGiIinhkRV0fEzRFxU0SsrdcfGBFXRcRt9c8D+l3rQouIJRHx1Yj4XL28MiKu\nq/f1p+oTBwZGROwfEZdGxNcj4paIeMGg7+eIeFf9ub4xIi6JiH0HbT9HxIURsS0ibuxYN+t+jcqH\n677fEBHHdNvuwAZ/IVND7ATOzMzDgeOAt9d9PAvYlJmHApvq5UGzFrilY/kc4IOZ+XPA94A396Wq\n9nwIuDIznwMcRdX3gd3PEXEwcAYwkplHUJ0I8loGbz9fDKyesW6u/fpy4ND6sQb4aLeNDmzwU8DU\nEJl5d2Zurp9/nyoMDqbq5/r6ZeuBk/tTYTsi4hDgFcD59XIALwEurV8yUH2OiOXAi4ELADLz0cy8\nnwHfz1RnHS6NiL2B/YC7GbD9nJnXAPfNWD3Xfn0V8PdZuRbYPyKe3k27gxz8BwPf6li+q143kCJi\nBfBc4DpgODPvrn/1HWC4T2W15W+APwCeqJefCtyfmTvr5UHb1yuB7wIX1cNb50fEMgZ4P2fmVuCv\ngf+lCvztwCSDvZ+nzbVfFyzTBjn4ixERQ8BngHdm5gOdv8vqfN2BOWc3Ik4CtmXmZL9r6aG9gWOA\nj2bmc4EHmTGsM4D7+QCqI9yVwDOAZfz4kMjAa2u/DnLwFzE1RETsQxX6GzLzsnr1PdN/AtY/t/Wr\nvhYcD7wyIu6kGr57CdX49/71kAAM3r6+C7grM6+rly+l+h/BIO/nlwJ3ZOZ3M/Mx4DKqfT/I+3na\nXPt1wTJtkIN/4KeGqMe2LwBuycwPdPzqCuC0+vlpwOW9rq0tmfmezDwkM1dQ7dP/yMxTgauBU+qX\nDVqfvwN8KyIOq1edANzMAO9nqiGe4yJiv/pzPt3ngd3PHebar1cAb6zP7jkO2N4xJLRnMnNgH8CJ\nwH8D3wT+sN/1tNC/F1H9GXgDsKV+nEg15r0JuA34d+DAftfaUv9Hgc/Vz58NXA98A/g08OR+17fA\nfT0amKj39WeBAwZ9PwN/CnwduBH4JPDkQdvPwCVU32E8RvWX3Zvn2q9AUJ2p+E1giuqMp67adcoG\nSSrMIA/1SJJmYfBLUmEMfkkqjMEvSYUx+CWpMAa/BlZEjEbECzuW3xoRb+xyW6dHxDM6ls9fiEn/\nFrJGqame33pR6qFRYAfwJYDM/Ng8tnU61fnk36639ZZ51jZtlIWrUWrEI34tKhHx2YiYrOdpX9Ox\nfnVEbI6Ir0XEpnrSurcC74qILRHxyxHxvoh4d0Q8JyKu73jvioiYqp//SUR8pZ4Dfl19leQpwAiw\nod7W0ogYj4iR+j2vi4ip+j3ndGx3R0S8v67p2oj4kUnUdlVj/fvxiPhgRExENQf/sRFxWT1P+593\nbOf1EXF9vY2P11OSS3My+LXYvCkzV1EF8RkR8dSIeBrwCeDVmXkU8BuZeSfwMaq524/OzC9ObyAz\nvw48KSJW1qteA3yqfv63mXlsVnPALwVOysxLqa6aPbXe1sPT26qHf86hmjPoaODYiJieRncZcG1d\n0zXA73R2ZFc1dng0M0fq110OvB04Aji97vsv1PUfn5lHA48Dp+7Bf08VyODXYnNGRHwNuJZqwqpD\nqW5Cc01m3gGQmTPnN5/NP1IFJvxo8I9FdYenKaow/8XdbOdYYDyrycR2Ahuo5s4HeBT4XP18EljR\noK6ZpueXmgJuyuoeDI8At1P1/wRgFfCViNhSLz+7i3ZUEMf4tWhExCjVrI0vyMyHImIc2LfLzX0K\n+HREXEY1++1tEbEv8BGqOVC+FRHvm8f2AR7LH86J8jjd/Xt7pP75RMfz6eW9qeZvWZ+Z7+m6ShXH\nI34tJsuB79Wh/xyqI32ojv5fPD10ExEH1uu/Dzxltg1l5jepwviP+eHR/nTI31vf4+CUjrfMta3r\ngV+JiIPqsfXXAV/Ygz7NWWNDm4BTIuKn4Qf3a33WPLanAhj8WkyuBPaOiFuAs6kCn8z8LtU9SC+r\nh4Gmg/yfgV+f/uJ0lu19Cng91bAPWd3O8BNUZ+/8K9XU3tMuBj42/eXu9MqspsU9i2q64K8Bk5m5\nJ1MF767GXcrMm4E/Av4tIm4ArgK6uh2fyuHsnJJUGI/4JakwBr8kFcbgl6TCGPySVBiDX5IKY/BL\nUmEMfkkqzP8DFAE+H/IDHpoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGmSlrMm7g6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "outputId": "733d6936-153d-4e4b-f4f6-45d804d5748f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#print(test.index[0],test.index[0][0])\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(12, 8))\n",
        "bools.sum().plot(kind=\"bar\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1c5db08dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsUAAAIwCAYAAABulBayAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPP1-qoLB_b-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "c46acda4-fb6f-47fd-a2e3-bf1657bc5113"
      },
      "source": [
        "bools = df3_scale.iloc[:,1:4] > 0\n",
        "bools.sum().plot(kind=\"bar\")\n",
        "#bools.sum()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1c5da71fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEpCAYAAACKmHkAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz77SyGDD2L2",
        "colab_type": "text"
      },
      "source": [
        "フィルタ処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z06piFAVDDOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "950134ce-c034-416f-97f0-e5952a59dd0a"
      },
      "source": [
        "import glob\n",
        "img_list = glob.glob(output_dir + \"/l*].png\")\n",
        "print(img_list[0][79:80])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HqwPQNxDp_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d49956de-da32-4cc0-fde1-1ee0f0f5ec1e"
      },
      "source": [
        "from PIL import Image\n",
        "im = np.array(Image.open(img_list[0]))\n",
        "np.max(im), np.min(im)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRueeE7hEUg4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "9f900801-5aaf-40ed-f434-2e459328e922"
      },
      "source": [
        "im_dev =np.array([])\n",
        "im_bug = pd.DataFrame(columns=[\"fig\"])\n",
        "im_all = pd.DataFrame(columns=[\"fig\"])\n",
        "index = 0\n",
        "\n",
        "for i in img_list:\n",
        "  im = np.array(Image.open(i))  \n",
        "  tmp = np.max(im)-np.min(im)\n",
        "  im_dev = np.append(im_dev, tmp)\n",
        "  if tmp > 50:\n",
        "    im_bug.loc[str(index)] = i[79:80]\n",
        "  im_all.loc[str(index)] = i[79:80]\n",
        "  index += 1\n",
        "\n",
        "plt.hist(im_dev)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([17.,  5.,  8.,  9., 10.,  6., 12.,  3.,  5.,  8.]),\n",
              " array([  0. ,  18.2,  36.4,  54.6,  72.8,  91. , 109.2, 127.4, 145.6,\n",
              "        163.8, 182. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUS1749kHv90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "3e8fd59b-ce1a-44a8-c7ff-2f28d455cea9"
      },
      "source": [
        "im_all[\"fig\"].value_counts().sort_index().plot(kind=\"bar\")\n",
        "#im_bug[\"fig\"].value_counts().sort_index().plot(kind=\"bar\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1c5d9464a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD1CAYAAABJE67gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUUZcEkqyoBLUlEGXJKKMuCSVNT/AZdTx4KaiLp6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ISWtgVLQTSQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "2b46b511-f182-4a33-9aca-2a80bfcf333a"
      },
      "source": [
        "im_bug[\"fig\"].value_counts().sort_index().plot(kind=\"bar\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1c5d9ed7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD1CAYAAAB5n7/BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60e_7IL1JM8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "c9d18dcb-c5b5-40c9-ec49-d5e23f8362af"
      },
      "source": [
        "im_all"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fig</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>83 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   fig\n",
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "..  ..\n",
              "78   9\n",
              "79   9\n",
              "80   9\n",
              "81   9\n",
              "82   9\n",
              "\n",
              "[83 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGHvvsEkJzXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fig = plt.figure()\n",
        "#ax = fig.add_subplot(1,1,1)\n",
        "#x1 = im_all[\"fig\"].value_counts()\n",
        "#x2 = im_bug[\"fig\"].value_counts()\n",
        "#ax.hist([x1, x2], bins=10, normed=True, color=['red', 'blue', 'green'], label=['x1', 'x2', 'x3'])\n",
        "#ax.set_title('seventh histogram $\\mu1=100,\\ \\sigma1=15,\\ \\mu2=50,\\ \\sigma2=4$')\n",
        "#ax.set_xlabel('x')\n",
        "#ax.set_ylabel('freq')\n",
        "#ax.legend(loc='upper left')\n",
        "#fig.show()\n",
        "#\n",
        "#x1, x2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxkrXvEuLOjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "47e69d1e-fcb7-4bc8-b4b9-ddaf562649fc"
      },
      "source": [
        "x1 = im_all[\"fig\"].value_counts()\n",
        "x1, type(x1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7    10\n",
              " 6    10\n",
              " 1    10\n",
              " 8     9\n",
              " 9     9\n",
              " 3     8\n",
              " 4     8\n",
              " 5     8\n",
              " 2     6\n",
              " 0     5\n",
              " Name: fig, dtype: int64, pandas.core.series.Series)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ7spEzhOy10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(x1, x2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK8sscq8LTWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5d139b1-e295-42ae-c075-0440601d8286"
      },
      "source": [
        "x1.values"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10, 10, 10,  9,  9,  8,  8,  8,  6,  5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcJirLEsbtHB",
        "colab_type": "text"
      },
      "source": [
        "以下は実際に各画像でどのくらい\"薄い\"画像があるかの分布"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAwybx1JLkn8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "d88577d2-6d97-409b-d674-db13c4aeaff0"
      },
      "source": [
        "im_dev =np.array([])\n",
        "im_bug = pd.DataFrame(columns=[\"filter\", \"org\"])\n",
        "index = 0\n",
        "\n",
        "for i in img_list:\n",
        "  im = np.array(Image.open(i))  \n",
        "  tmp = np.max(im)-np.min(im)\n",
        "  im_dev = np.append(im_dev, tmp)\n",
        "  if tmp > 50:\n",
        "    im_bug.loc[str(index)] = [i[79:80], i[79:80]]\n",
        "  else:\n",
        "    im_bug.loc[str(index)] = [None, i[79:80]]\n",
        "#  im_all.loc[str(index)] = i[79:80]\n",
        "  index += 1\n",
        "\n",
        "plt.hist(im_dev)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([17.,  5.,  8.,  9., 10.,  6., 12.,  3.,  5.,  8.]),\n",
              " array([  0. ,  18.2,  36.4,  54.6,  72.8,  91. , 109.2, 127.4, 145.6,\n",
              "        163.8, 182. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ_waU1DMtUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "dc4964f5-552b-4222-c051-8282bd918ac7"
      },
      "source": [
        "im_bug"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filter</th>\n",
              "      <th>org</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>83 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   filter org\n",
              "0    None   0\n",
              "1    None   0\n",
              "2    None   0\n",
              "3    None   0\n",
              "4       0   0\n",
              "..    ...  ..\n",
              "78      9   9\n",
              "79      9   9\n",
              "80      9   9\n",
              "81      9   9\n",
              "82      9   9\n",
              "\n",
              "[83 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwBNwplZM3UI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "51465b1d-9f16-4d14-ebf0-ecbc303f529f"
      },
      "source": [
        "print(im_bug[\"filter\"].value_counts().sort_index())\n",
        "print(im_bug[\"org\"].value_counts().sort_index())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    1\n",
            "1    8\n",
            "2    5\n",
            "3    4\n",
            "4    5\n",
            "5    7\n",
            "6    3\n",
            "7    8\n",
            "8    6\n",
            "9    9\n",
            "Name: filter, dtype: int64\n",
            "0     5\n",
            "1    10\n",
            "2     6\n",
            "3     8\n",
            "4     8\n",
            "5     8\n",
            "6    10\n",
            "7    10\n",
            "8     9\n",
            "9     9\n",
            "Name: org, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWbPrxZRNBZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8f3d569-c759-486e-8f93-f2b67a838260"
      },
      "source": [
        "type(im_all[\"fig\"])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lncXwCAINPAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#im_bug[\"filter\"].value_counts().sort_index().merge(im_bug[\"org\"].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obbro2n3ONLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "outputId": "a3ac4a0c-fc53-4f76-c8ca-524202c962fd"
      },
      "source": [
        "df4_scale.head(20)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>('block1_conv1', 0)</th>\n",
              "      <th>('block1_conv1', 1)</th>\n",
              "      <th>('block1_conv1', 2)</th>\n",
              "      <th>('block1_conv1', 3)</th>\n",
              "      <th>('block1_pool1', 0)</th>\n",
              "      <th>('block1_pool1', 1)</th>\n",
              "      <th>('block1_pool1', 2)</th>\n",
              "      <th>('block1_pool1', 3)</th>\n",
              "      <th>('block2_conv1', 0)</th>\n",
              "      <th>('block2_conv1', 1)</th>\n",
              "      <th>('block2_conv1', 2)</th>\n",
              "      <th>('block2_conv1', 3)</th>\n",
              "      <th>('block2_conv1', 4)</th>\n",
              "      <th>('block2_conv1', 5)</th>\n",
              "      <th>('block2_conv1', 6)</th>\n",
              "      <th>('block2_conv1', 7)</th>\n",
              "      <th>('block2_conv1', 8)</th>\n",
              "      <th>('block2_conv1', 9)</th>\n",
              "      <th>('block2_conv1', 10)</th>\n",
              "      <th>('block2_conv1', 11)</th>\n",
              "      <th>('block2_pool1', 0)</th>\n",
              "      <th>('block2_pool1', 1)</th>\n",
              "      <th>('block2_pool1', 2)</th>\n",
              "      <th>('block2_pool1', 3)</th>\n",
              "      <th>('block2_pool1', 4)</th>\n",
              "      <th>('block2_pool1', 5)</th>\n",
              "      <th>('block2_pool1', 6)</th>\n",
              "      <th>('block2_pool1', 7)</th>\n",
              "      <th>('block2_pool1', 8)</th>\n",
              "      <th>('block2_pool1', 9)</th>\n",
              "      <th>('block2_pool1', 10)</th>\n",
              "      <th>('block2_pool1', 11)</th>\n",
              "      <th>('before_softmax', 0)</th>\n",
              "      <th>('before_softmax', 1)</th>\n",
              "      <th>('before_softmax', 2)</th>\n",
              "      <th>('before_softmax', 3)</th>\n",
              "      <th>('before_softmax', 4)</th>\n",
              "      <th>('before_softmax', 5)</th>\n",
              "      <th>('before_softmax', 6)</th>\n",
              "      <th>('before_softmax', 7)</th>\n",
              "      <th>('before_softmax', 8)</th>\n",
              "      <th>('before_softmax', 9)</th>\n",
              "      <th>('predictions', 0)</th>\n",
              "      <th>('predictions', 1)</th>\n",
              "      <th>('predictions', 2)</th>\n",
              "      <th>('predictions', 3)</th>\n",
              "      <th>('predictions', 4)</th>\n",
              "      <th>('predictions', 5)</th>\n",
              "      <th>('predictions', 6)</th>\n",
              "      <th>('predictions', 7)</th>\n",
              "      <th>('predictions', 8)</th>\n",
              "      <th>('predictions', 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_1_0</th>\n",
              "      <td>0_1_0</td>\n",
              "      <td>0.307</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.361</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.198</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.361</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.502</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.360</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_1</th>\n",
              "      <td>0_1_1</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.313</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.178</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.261</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.195</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.427</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.370</td>\n",
              "      <td>0.371</td>\n",
              "      <td>0.498</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.343</td>\n",
              "      <td>0.349</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_2</th>\n",
              "      <td>0_1_2</td>\n",
              "      <td>0.342</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.394</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.192</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.366</td>\n",
              "      <td>0.494</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.337</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_3</th>\n",
              "      <td>0_1_3</td>\n",
              "      <td>0.371</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.245</td>\n",
              "      <td>0.423</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.343</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.199</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.263</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.187</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.357</td>\n",
              "      <td>0.485</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.317</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_4</th>\n",
              "      <td>0_1_4</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.308</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_5</th>\n",
              "      <td>0_1_5</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.312</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_6</th>\n",
              "      <td>0_1_6</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.282</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.483</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.377</td>\n",
              "      <td>0.313</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_7</th>\n",
              "      <td>0_1_7</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.185</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.308</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_8</th>\n",
              "      <td>0_1_8</td>\n",
              "      <td>0.382</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.269</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.407</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.481</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.310</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_9</th>\n",
              "      <td>0_1_9</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.284</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.311</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_10</th>\n",
              "      <td>0_1_10</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.404</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.483</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.377</td>\n",
              "      <td>0.313</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_11</th>\n",
              "      <td>0_1_11</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.185</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.308</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_12</th>\n",
              "      <td>0_1_12</td>\n",
              "      <td>0.382</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.269</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.407</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.481</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.310</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_13</th>\n",
              "      <td>0_1_13</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.284</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.311</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_14</th>\n",
              "      <td>0_1_14</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.404</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.483</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.377</td>\n",
              "      <td>0.313</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_15</th>\n",
              "      <td>0_1_15</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.185</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.308</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_16</th>\n",
              "      <td>0_1_16</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.269</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.407</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.481</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.310</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_17</th>\n",
              "      <td>0_1_17</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.284</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.311</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_18</th>\n",
              "      <td>0_1_18</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.404</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.483</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.377</td>\n",
              "      <td>0.312</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_19</th>\n",
              "      <td>0_1_19</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.185</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.432</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.308</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ('block1_conv1', 0)  ...  ('predictions', 8)  ('predictions', 9)\n",
              "0_1_0       0_1_0                0.307  ...               0.000               0.000\n",
              "0_1_1       0_1_1                0.323  ...               0.000               0.000\n",
              "0_1_2       0_1_2                0.342  ...               0.000               0.000\n",
              "0_1_3       0_1_3                0.371  ...               0.000               0.000\n",
              "0_1_4       0_1_4                0.385  ...               0.000               0.000\n",
              "0_1_5       0_1_5                0.380  ...               0.000               0.000\n",
              "0_1_6       0_1_6                0.378  ...               0.000               0.000\n",
              "0_1_7       0_1_7                0.385  ...               0.000               0.000\n",
              "0_1_8       0_1_8                0.382  ...               0.000               0.000\n",
              "0_1_9       0_1_9                0.380  ...               0.000               0.000\n",
              "0_1_10     0_1_10                0.378  ...               0.000               0.000\n",
              "0_1_11     0_1_11                0.385  ...               0.000               0.000\n",
              "0_1_12     0_1_12                0.382  ...               0.000               0.000\n",
              "0_1_13     0_1_13                0.380  ...               0.000               0.000\n",
              "0_1_14     0_1_14                0.378  ...               0.000               0.000\n",
              "0_1_15     0_1_15                0.385  ...               0.000               0.000\n",
              "0_1_16     0_1_16                0.383  ...               0.000               0.000\n",
              "0_1_17     0_1_17                0.380  ...               0.000               0.000\n",
              "0_1_18     0_1_18                0.378  ...               0.000               0.000\n",
              "0_1_19     0_1_19                0.385  ...               0.000               0.000\n",
              "\n",
              "[20 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5HVngqRZ4GY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}