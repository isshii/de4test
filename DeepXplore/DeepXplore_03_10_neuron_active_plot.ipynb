{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "DeepXplore_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshii/de4test/blob/test_ozawa/DeepXplore/DeepXplore_03_10_neuron_active_plot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVFjZpUIRF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 共通で使うパスなどの定義\n",
        "# 共通の変数設定\n",
        "# 共通フォルダパス\n",
        "\n",
        "import datetime\n",
        "import pytz\n",
        "dt_now = datetime.datetime.now(pytz.timezone('Asia/Tokyo'))\n",
        "dt_str = str(dt_now.strftime('%Y%m%d_%H%M'))\n",
        "\n",
        "data_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data\"\n",
        "#data_imagenet = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet\"\n",
        "data_mnist = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/MNIST\"\n",
        "#data_imagenet_seeds = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet/seeds\"\n",
        "model_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/model\"\n",
        "#output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output\"\n",
        "output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output/\" + dt_str\n",
        "tmp_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/tmp\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hv53U_OdAFYb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4lP4POGn5Xj",
        "colab_type": "code",
        "outputId": "69a78ab1-a375-4826-dad7-5f23fb58665f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# Goggle Drive つなぐ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3d3Suv3Q5Db",
        "colab_type": "code",
        "outputId": "1479e409-4fd4-4fb5-8583-8656fe639b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# outputフォルダ内容物のクリーンアップ削除\n",
        "!mkdir \"$output_dir\"\n",
        "!rm \"$output_dir\"/*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200123_1903/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdHYnLT0Nbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST')\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDY8sDVBqxmK",
        "colab_type": "code",
        "outputId": "25014ddc-e9bf-445c-adf2-dabaa24b5857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input\n",
        "import imageio\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "import os\n",
        "\n",
        "# TensorFlowでGPUを使っているかのチェック：\n",
        "# \"device_type: \"GPU\" \" があればOK\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 1656238292379295392, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 5677786250296122109\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 18123212032871523143\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 14912199066\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 3048903056915010580\n",
              " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQ2M_8glJNM",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "5ef1805c-33be-4fcd-815b-065e6e471729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "# DeepXplore のパラメータ設定部\n",
        "# read the parameter\n",
        "# argument parsing\n",
        "parser = argparse.ArgumentParser(description='Main function for difference-inducing input generation in MNIST dataset')\n",
        "parser.add_argument('transformation', help=\"realistic transformation type\", choices=['light', 'occl', 'blackout'])\n",
        "parser.add_argument('weight_diff', help=\"weight hyperparm to control differential behavior\", type=float)\n",
        "parser.add_argument('weight_nc', help=\"weight hyperparm to control neuron coverage\", type=float)\n",
        "parser.add_argument('step', help=\"step size of gradient descent\", type=float)\n",
        "parser.add_argument('seeds', help=\"number of seeds of input\", type=int)\n",
        "parser.add_argument('grad_iterations', help=\"number of iterations of gradient descent\", type=int)\n",
        "parser.add_argument('threshold', help=\"threshold for determining neuron activated\", type=float)\n",
        "parser.add_argument('-t', '--target_model', help=\"target model that we want it predicts differently\",\n",
        "                    choices=[0, 1, 2], default=0, type=int)\n",
        "parser.add_argument('-sp', '--start_point', help=\"occlusion upper left corner coordinate\", default=(0, 0), type=tuple)\n",
        "parser.add_argument('-occl_size', '--occlusion_size', help=\"occlusion size\", default=(10, 10), type=tuple)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-occl_size', '--occlusion_size'], dest='occlusion_size', nargs=None, const=None, default=(10, 10), type=<class 'tuple'>, choices=None, help='occlusion size', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93KW-VGjMzun",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title ノイズのタイプ light / occl / blackout\n",
        "#@body {light,occl,blackout} weight_diff weight_nc step seeds\n",
        "#                   grad_iterations threshold\n",
        "noise_type = \"light\" #@param [\"light\", \"occl\", \"blackout\"]\n",
        "weight_diff = \"0.1\" #@param {type:\"string\"}\n",
        "weight_nc = \"0.1\" #@param {type:\"string\"}\n",
        "step = \"20\" #@param {type:\"string\"}　#50だとほぼ白飛び\n",
        "seeds = \"5\" #@param {type:\"string\"}\n",
        "grad_iterations = \"10\" #@param {type:\"string\"}\n",
        "threshold = \"0.1\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk80fCuPts9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = parser.parse_args([noise_type, weight_diff, weight_nc, step, seeds, grad_iterations, threshold])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tTYrllplJNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x.reshape(x.shape[1], x.shape[2])  # original shape (1,img_rows, img_cols,1)\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "def constraint_occl(gradients, start_point, rect_shape):\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def constraint_light(gradients):\n",
        "    new_grads = np.ones_like(gradients)\n",
        "    grad_mean = np.mean(gradients)\n",
        "    return grad_mean * new_grads\n",
        "\n",
        "\n",
        "def constraint_black(gradients, rect_shape=(6, 6)):\n",
        "    start_point = (\n",
        "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    if np.mean(patch) < 0:\n",
        "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def init_coverage_tables(model1, model2, model3):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    model_layer_dict2 = defaultdict(bool)\n",
        "    model_layer_dict3 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    init_dict(model2, model_layer_dict2)\n",
        "    init_dict(model3, model_layer_dict3)\n",
        "    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n",
        "\n",
        "\n",
        "def init_dict(model, model_layer_dict):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_dict[(layer.name, index)] = False\n",
        "\n",
        "\n",
        "def neuron_to_cover(model_layer_dict):\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "    else:\n",
        "        layer_name, index = random.choice(model_layer_dict.keys())\n",
        "    return layer_name, index\n",
        "\n",
        "\n",
        "def neuron_covered(model_layer_dict):\n",
        "    covered_neurons = len([v for v in model_layer_dict.values() if v])\n",
        "    total_neurons = len(model_layer_dict)\n",
        "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n",
        "\n",
        "\n",
        "def update_coverage(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n",
        "                model_layer_dict[(layer_names[i], num_neuron)] = True\n",
        "\n",
        "\n",
        "def full_coverage(model_layer_dict):\n",
        "    if False in model_layer_dict.values():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
        "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
        "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
        "    X_scaled = X_std * (rmax - rmin) + rmin\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def fired(model, layer_name, index, input_data, threshold=0):\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
        "    scaled = scale(intermediate_layer_output)\n",
        "    if np.mean(scaled[..., index]) > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def diverged(predictions1, predictions2, predictions3, target):\n",
        "    #     if predictions2 == predictions3 == target and predictions1 != target:\n",
        "    if not predictions1 == predictions2 == predictions3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "#add\n",
        "def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n",
        "    return [os.path.join(root, f)\n",
        "            for root, _, files in os.walk(directory) for f in files\n",
        "            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDDT47Er6u23",
        "colab_type": "code",
        "outputId": "16e57931-613c-4c80-99d5-07ea6a12ce66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "'''\n",
        "LeNet-1\n",
        "'''\n",
        "def Model1(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        print(x_train.shape)\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(4, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(12, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "#        model.save_weights('./Model1.h5')\n",
        "        model.save_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "#        model.load_weights('./Model1.h5')\n",
        "        model.load_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        print(bcolors.OKBLUE + 'Model1 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model1(train=True)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.4521 - acc: 0.8627 - val_loss: 0.1436 - val_acc: 0.9568\n",
            "\n",
            "\n",
            "Overall Test score: 0.14356533419266343\n",
            "Overall Test accuracy: 0.9568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU88awZf68JO",
        "colab_type": "code",
        "outputId": "b6b37d2c-3d3b-4a10-902a-3cbab025cf58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "'''\n",
        "LeNet-4\n",
        "'''\n",
        "def Model2(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(84, activation='relu', name='fc1')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model2.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model2.h5')\n",
        "        print(bcolors.OKBLUE + 'Model2 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model2(train=True)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.3653 - acc: 0.8863 - val_loss: 0.0895 - val_acc: 0.9723\n",
            "\n",
            "\n",
            "Overall Test score: 0.08946820538565517\n",
            "Overall Test accuracy: 0.9723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufs67uRN7KYL",
        "colab_type": "code",
        "outputId": "79245943-24c2-4e22-bc3e-016873ddef95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "'''\n",
        "LeNet-5\n",
        "'''\n",
        "def Model3(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(120, activation='relu', name='fc1')(x)\n",
        "    x = Dense(84, activation='relu', name='fc2')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model3.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model3.h5')\n",
        "        print(bcolors.OKBLUE + 'Model3 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model3(train=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.3948 - acc: 0.8749 - val_loss: 0.0878 - val_acc: 0.9724\n",
            "\n",
            "\n",
            "Overall Test score: 0.08780119342524559\n",
            "Overall Test accuracy: 0.9724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9QiwlW7WWR",
        "colab_type": "code",
        "outputId": "35b51dc6-9293-4669-e9e9-d4bd0ba23b35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# input MNIST image dimensions \n",
        "img_rows, img_cols = 28, 28\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "#define the model instance\n",
        "model1 = Model1(input_tensor=input_tensor)\n",
        "model2 = Model2(input_tensor=input_tensor)\n",
        "model3 = Model3(input_tensor=input_tensor)\n",
        "\n",
        "#define the dictionary of neuron coverage\n",
        "model_layer_dict1, model_layer_dict2, model_layer_dict3 = init_coverage_tables(model1, model2, model3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[94mModel1 loaded\u001b[0m\n",
            "\u001b[94mModel2 loaded\u001b[0m\n",
            "\u001b[94mModel3 loaded\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MS5H74Q4XA",
        "colab_type": "text"
      },
      "source": [
        "### 入力するデータの選別(1/2)\n",
        "\n",
        "０～９の数字をそれぞれ束ねてデータを準備する\n",
        "[00...011...1......99...9]。\n",
        "\n",
        "* test_per_fig_x: \n",
        "    各数字に対する画像データを格納\n",
        "* test_per_fig_y: \n",
        "    各数字に対する教師ラベルを格納\n",
        "* tests_x: \n",
        "    test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: \n",
        "    tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brFIdSfL8nHP",
        "colab_type": "code",
        "outputId": "aa004920-75c9-454c-f688-8f8eacfb7040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load the MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "num_fig = 10\n",
        "test_per_fig_x = np.array([])\n",
        "test_per_fig_y = np.array([])\n",
        "tests_x = np.array([])\n",
        "tests_y = np.array([])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "#x_train = x_train.astype('float')\n",
        "#x_test = x_test.astype('float')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "length = int(args.seeds)\n",
        "#length = int(args.seeds)\n",
        "for i in range(num_fig):\n",
        "  cond = [(x==i) for x in y_test]\n",
        "  test_per_fig_x = x_test[cond]\n",
        "  test_per_fig_y = y_test[cond]\n",
        "#  np.set_printoptions(formatter={'int': '{:07d}'.format})\n",
        "  print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "  tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "  tests_y = np.append(tests_y, test_per_fig_y[:length])\n",
        "#  conds = [conds, cond]\n",
        "print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "\n",
        "###### MNIST data, shuffled and split by train and test sets\n",
        "#####(_, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "###### modify the numpy data for the Keras model\n",
        "#####x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "#####input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "#tests_x.shape\n",
        "tests_x = tests_x.astype('float32')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure: 0 , shape: (980, 28, 28) , deviation: -20\n",
            "figure: 1 , shape: (1135, 28, 28) , deviation: 135\n",
            "figure: 2 , shape: (1032, 28, 28) , deviation: 32\n",
            "figure: 3 , shape: (1010, 28, 28) , deviation: 10\n",
            "figure: 4 , shape: (982, 28, 28) , deviation: -18\n",
            "figure: 5 , shape: (892, 28, 28) , deviation: -108\n",
            "figure: 6 , shape: (958, 28, 28) , deviation: -42\n",
            "figure: 7 , shape: (1028, 28, 28) , deviation: 28\n",
            "figure: 8 , shape: (974, 28, 28) , deviation: -26\n",
            "figure: 9 , shape: (1009, 28, 28) , deviation: 9\n",
            "check! 5.0 = 5 equal?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUOK_EjJW2KT",
        "colab_type": "text"
      },
      "source": [
        "## 入力するデータの選別(2/2)\n",
        "０～９の数字をそれぞれ束ねてデータを準備する [00...000]。\n",
        "\n",
        "* test_per_fig_x: 各数字に対する画像データを格納\n",
        "* test_per_fig_y: 各数字に対する教師ラベルを格納\n",
        "* tests_x: test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "id": "SLee__9UAD1Y",
        "colab": {}
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "#load the MNIST \n",
        "\n",
        "def create_data(start_fig, num_fi, length):\n",
        "  (datax_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  test_per_fig_x = np.array([])\n",
        "  test_per_fig_y = np.array([])\n",
        "  tests_x = np.array([])\n",
        "  tests_y = np.array([])\n",
        "\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test  = x_test.astype('float32')\n",
        "  x_train = x_train.astype('float')\n",
        "  x_test = x_test.astype('float')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  #length = int(args.seeds/num_fig)\n",
        "  #length = int(args.seeds)\n",
        "  for i in range(start_fig, start_fig+num_fig):\n",
        "  #i=0\n",
        "    cond = [(x==i) for x in y_test]\n",
        "    test_per_fig_x = x_test[cond]\n",
        "    test_per_fig_y = y_test[cond]\n",
        "    print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "    tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "    tests_y = np.append(tests_y, test_per_fig_y[:length])  \n",
        "\n",
        "  #conds = [conds, cond]\n",
        "  print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "  tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "  tests_x.shape\n",
        "  tests_x = tests_x.astype('float32')\n",
        "  return tests_x, tests_y, length\n",
        "  tests_x, tests_y, length = create_data(0, 10, int(args.seeds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyo2SSSz8vkT",
        "colab_type": "code",
        "outputId": "9b284df7-0238-49e5-c8df-ebd65ef43f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(tests_x.shape)\n",
        "print(tests_y)\n",
        "for i in range(0,1):\n",
        "  print(i)\n",
        "print(length)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 28, 28, 1)\n",
            "[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 4. 4. 4. 4.\n",
            " 4. 5. 5. 5. 5. 5. 6. 6. 6. 6. 6. 7. 7. 7. 7. 7. 8. 8. 8. 8. 8. 9. 9. 9.\n",
            " 9. 9.]\n",
            "0\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLlKIMxG9kEA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfdTZi4mUPf",
        "colab_type": "code",
        "outputId": "4cac7580-03e8-4fdc-e7b7-2ea665807719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "def neuron_output(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "    \n",
        "    out_list = []\n",
        "    out_list_scale = []\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            out_list.append(np.mean(intermediate_layer_output[..., num_neuron]))\n",
        "            out_list_scale.append(np.mean(scaled[..., num_neuron]))\n",
        "\n",
        "    return out_list, out_list_scale\n",
        "\n",
        "\n",
        "def deepXplore(model_layer_dict1, model_layer_dict2, model_layer_dict3, tests_x, model1, model2, model3, num_fig, start_fig, length):\n",
        "  #print(\"test01\")\n",
        "  count_already = 0\n",
        "  count_found = 0\n",
        "  count_not_found = 0\n",
        "  temp_per_nc1 = np.array([])\n",
        "  temp_per_nc2 = np.array([])\n",
        "  temp_per_nc3 = np.array([])\n",
        "  temp_num_nc1 = np.array([])\n",
        "  temp_num_nc2 = np.array([])\n",
        "  temp_num_nc3 = np.array([])\n",
        "  #print(\"test10\")\n",
        "\n",
        "  #for each neuron\n",
        "  num_neurons1 = neuron_covered(model_layer_dict1)[1]\n",
        "  num_neurons2 = neuron_covered(model_layer_dict2)[1]\n",
        "  num_neurons3 = neuron_covered(model_layer_dict3)[1]\n",
        "  #print(\"test11\")\n",
        "\n",
        "  column_tmp1 = list(model_layer_dict1.keys())\n",
        "  column_tmp2 = list(model_layer_dict2.keys())\n",
        "  column_tmp3 = list(model_layer_dict3.keys())\n",
        "  #print(\"test12\")\n",
        "  df1 = pd.DataFrame(columns=column_tmp1)\n",
        "  df2 = pd.DataFrame(columns=column_tmp2)\n",
        "  df3 = pd.DataFrame(columns=column_tmp3)\n",
        "  df1_scale = pd.DataFrame(columns=column_tmp1)\n",
        "  df2_scale = pd.DataFrame(columns=column_tmp2)\n",
        "  df3_scale = pd.DataFrame(columns=column_tmp3)\n",
        "  tmp_list = [\"already_diff\", \"found\", \"not_found\", \"layer1\", \"index1\", \"layer2\", \"index2\", \"layer3\", \"index3\"]\n",
        "  bug_result = pd.DataFrame(columns=tmp_list)\n",
        "  trial = 1\n",
        "  #print(\"test13\")\n",
        "  #print(\"test02\")\n",
        "  for index_fig in range(num_fig):\n",
        "    index_fig = index_fig + start_fig\n",
        "    print(\"figure\"+str(index_fig))\n",
        "    for _ in range(length):\n",
        "      #gen_img = np.expand_dims(random.choice(tests_x), axis=0)\n",
        "      gen_img = np.expand_dims(tests_x[(length*index_fig + _)], axis=0)\n",
        "      orig_img = gen_img.copy()\n",
        "      # first check if input already induces differences\n",
        "      label1, label2, label3 = np.argmax(model1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "      if not label1 == label2 == label3:\n",
        "          count_already += 1\n",
        "          print(bcolors.OKGREEN + '   {}/{}. input already causes different outputs ({},{},{}) at({}, {}, {}): '.format(_, length, label1, label2, label3, count_already, count_found, count_not_found) + bcolors.ENDC)        \n",
        "\n",
        "          update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "          temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "          temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "          temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "          temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "          temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "          temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "                               \n",
        "          print(bcolors.OKGREEN + '     covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'% (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  neuron_covered(model_layer_dict2)[2], len(model_layer_dict3), neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "          averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                       neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +neuron_covered(model_layer_dict3)[1])\n",
        "          print(bcolors.OKGREEN + '     averaged covered neurons %.3f' % (averaged_nc) + bcolors.ENDC)\n",
        "\n",
        "          gen_img_deprocessed = deprocess_image(gen_img)\n",
        "\n",
        "          # save the result to disk\n",
        "          outputfilepath0 = os.path.join(output_dir, 'already_differ_' + str(label1) + '_' + str(label2) + '_' + str(label3) +'_['+ str(_) +  '].png')\n",
        "          imageio.imwrite(outputfilepath0, gen_img_deprocessed)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df1 = df1.append(temp)\n",
        "          df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df2 = df2.append(temp)\n",
        "          df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "          temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df3 = df3.append(temp)\n",
        "          df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "          #print(\"test10\")\n",
        "          temp = [1, 0, 0, None, None, None, None, None, None]\n",
        "          #print(\"test11\")\n",
        "          temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          #print(\"test12\")\n",
        "          bug_result = bug_result.append(temp)\n",
        "          #print(\"test13\")\n",
        "          trial += 1\n",
        "          continue\n",
        "\n",
        "      # if all label agrees\n",
        "      orig_label = label1\n",
        "      layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n",
        "      layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n",
        "      layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n",
        "\n",
        "      # construct joint loss function\n",
        "      if args.target_model == 0:\n",
        "        loss1 = -args.weight_diff * K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 1:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = -args.weight_diff * K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 2:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n",
        "      loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n",
        "      loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n",
        "      layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron)\n",
        "\n",
        "      # for adversarial image generation\n",
        "      final_loss = K.mean(layer_output)\n",
        "\n",
        "      # we compute the gradient of the input picture wrt this loss\n",
        "      grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "\n",
        "      # this function returns the loss and grads given the input picture\n",
        "      iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n",
        "\n",
        "      #print(\"test03\")\n",
        "      # we run gradient ascent for some steps\n",
        "      for iters in range(args.grad_iterations):\n",
        "        loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = iterate([gen_img])\n",
        "        if args.transformation == 'light':\n",
        "            grads_value = constraint_light(grads_value)  # constraint the gradients value\n",
        "        elif args.transformation == 'occl':\n",
        "            grads_value = constraint_occl(grads_value, args.start_point,\n",
        "                                          args.occlusion_size)  # constraint the gradients value\n",
        "        elif args.transformation == 'blackout':\n",
        "            grads_value = constraint_black(grads_value)  # constraint the gradients value\n",
        "\n",
        "        gen_img += grads_value * args.step\n",
        "        predictions1 = np.argmax(model1.predict(gen_img)[0])\n",
        "        predictions2 = np.argmax(model2.predict(gen_img)[0])\n",
        "        predictions3 = np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "        #print(\"test04\")\n",
        "        if not predictions1 == predictions2 == predictions3:\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            count_found += 1\n",
        "            print(bcolors.OKBLUE + '%4d/%d. found at %d! covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f at (%d, %d, %d)'\n",
        "#                  % (_, args.seeds, iters, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  % (_, length, iters + 1, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                     neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                     neuron_covered(model_layer_dict3)[2], count_already, count_found, count_not_found) + bcolors.ENDC)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            print(bcolors.OKBLUE + '     averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  '].png')\n",
        "            #print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "#            outputfilepath2 = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '_orig.png')\n",
        "            outputfilepath2 = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  ']_orig.png')\n",
        "            #print(outputfilepath2)\n",
        "            imageio.imwrite(outputfilepath2, orig_img_deprocessed)\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "            temp = [0, iters+1, 0, layer_name1, index1, layer_name2, index2,layer_name3, index3]\n",
        "            temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            bug_result = bug_result.append(temp)\n",
        "            trial += 1\n",
        "            break\n",
        "          \n",
        "          #add\n",
        "        #print(\"test05\")\n",
        "        if iters == (args.grad_iterations-1):\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            count_not_found += 1\n",
        "#              print('%4d/%d. test suite was not found: averaged covered neurons %.3f at %d/%d' % (_, args.seeds, averaged_nc, count_not_found, count_already + count_found + count_not_found))\n",
        "            print('%4d/%d. test suite was not found: averaged covered neurons %.3f at (%d, %d, %d)' % (_, length, averaged_nc, count_already, count_found, count_not_found))\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            #orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, 'not_found_' + str(label1)+'_['+ str(_) + '].png')\n",
        "#           print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "\n",
        "            \n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "\n",
        "            temp = [0, 0, 1, layer_name1, index1, layer_name2, index2,layer_name3, index3]\n",
        "            temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            bug_result = bug_result.append(temp)\n",
        "\n",
        "            trial += 1\n",
        "            \n",
        "            #break\n",
        "\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "  temp_per_nc1=temp_per_nc1.reshape(num_fig, length)\n",
        "  temp_per_nc2=temp_per_nc2.reshape(num_fig, length)\n",
        "  temp_per_nc3=temp_per_nc3.reshape(num_fig, length)\n",
        "  temp_num_nc1=temp_num_nc1.reshape(num_fig, length)\n",
        "  temp_num_nc2=temp_num_nc2.reshape(num_fig, length)\n",
        "  temp_num_nc3=temp_num_nc3.reshape(num_fig, length)\n",
        "\n",
        "  return df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
            "Wall time: 8.11 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHi1ffJuWH5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "c069bf0b-0aea-4e8b-9d6d-3926e24c421c"
      },
      "source": [
        "print(length, num_fig)\n",
        "print(output_dir)\n",
        "column_tmp1 = list(model_layer_dict1.keys())\n",
        "df1 = pd.DataFrame(columns=column_tmp1)\n",
        "df1.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 10\n",
            "/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200123_1903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(block1_conv1, 0)</th>\n",
              "      <th>(block1_conv1, 1)</th>\n",
              "      <th>(block1_conv1, 2)</th>\n",
              "      <th>(block1_conv1, 3)</th>\n",
              "      <th>(block1_pool1, 0)</th>\n",
              "      <th>(block1_pool1, 1)</th>\n",
              "      <th>(block1_pool1, 2)</th>\n",
              "      <th>(block1_pool1, 3)</th>\n",
              "      <th>(block2_conv1, 0)</th>\n",
              "      <th>(block2_conv1, 1)</th>\n",
              "      <th>(block2_conv1, 2)</th>\n",
              "      <th>(block2_conv1, 3)</th>\n",
              "      <th>(block2_conv1, 4)</th>\n",
              "      <th>(block2_conv1, 5)</th>\n",
              "      <th>(block2_conv1, 6)</th>\n",
              "      <th>(block2_conv1, 7)</th>\n",
              "      <th>(block2_conv1, 8)</th>\n",
              "      <th>(block2_conv1, 9)</th>\n",
              "      <th>(block2_conv1, 10)</th>\n",
              "      <th>(block2_conv1, 11)</th>\n",
              "      <th>(block2_pool1, 0)</th>\n",
              "      <th>(block2_pool1, 1)</th>\n",
              "      <th>(block2_pool1, 2)</th>\n",
              "      <th>(block2_pool1, 3)</th>\n",
              "      <th>(block2_pool1, 4)</th>\n",
              "      <th>(block2_pool1, 5)</th>\n",
              "      <th>(block2_pool1, 6)</th>\n",
              "      <th>(block2_pool1, 7)</th>\n",
              "      <th>(block2_pool1, 8)</th>\n",
              "      <th>(block2_pool1, 9)</th>\n",
              "      <th>(block2_pool1, 10)</th>\n",
              "      <th>(block2_pool1, 11)</th>\n",
              "      <th>(before_softmax, 0)</th>\n",
              "      <th>(before_softmax, 1)</th>\n",
              "      <th>(before_softmax, 2)</th>\n",
              "      <th>(before_softmax, 3)</th>\n",
              "      <th>(before_softmax, 4)</th>\n",
              "      <th>(before_softmax, 5)</th>\n",
              "      <th>(before_softmax, 6)</th>\n",
              "      <th>(before_softmax, 7)</th>\n",
              "      <th>(before_softmax, 8)</th>\n",
              "      <th>(before_softmax, 9)</th>\n",
              "      <th>(predictions, 0)</th>\n",
              "      <th>(predictions, 1)</th>\n",
              "      <th>(predictions, 2)</th>\n",
              "      <th>(predictions, 3)</th>\n",
              "      <th>(predictions, 4)</th>\n",
              "      <th>(predictions, 5)</th>\n",
              "      <th>(predictions, 6)</th>\n",
              "      <th>(predictions, 7)</th>\n",
              "      <th>(predictions, 8)</th>\n",
              "      <th>(predictions, 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [(block1_conv1, 0), (block1_conv1, 1), (block1_conv1, 2), (block1_conv1, 3), (block1_pool1, 0), (block1_pool1, 1), (block1_pool1, 2), (block1_pool1, 3), (block2_conv1, 0), (block2_conv1, 1), (block2_conv1, 2), (block2_conv1, 3), (block2_conv1, 4), (block2_conv1, 5), (block2_conv1, 6), (block2_conv1, 7), (block2_conv1, 8), (block2_conv1, 9), (block2_conv1, 10), (block2_conv1, 11), (block2_pool1, 0), (block2_pool1, 1), (block2_pool1, 2), (block2_pool1, 3), (block2_pool1, 4), (block2_pool1, 5), (block2_pool1, 6), (block2_pool1, 7), (block2_pool1, 8), (block2_pool1, 9), (block2_pool1, 10), (block2_pool1, 11), (before_softmax, 0), (before_softmax, 1), (before_softmax, 2), (before_softmax, 3), (before_softmax, 4), (before_softmax, 5), (before_softmax, 6), (before_softmax, 7), (before_softmax, 8), (before_softmax, 9), (predictions, 0), (predictions, 1), (predictions, 2), (predictions, 3), (predictions, 4), (predictions, 5), (predictions, 6), (predictions, 7), (predictions, 8), (predictions, 9)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSSBfoAZreWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9d65e977-3c4f-4683-fe72-f4319eca64a8"
      },
      "source": [
        "index_fig = 0\n",
        "trial = 0\n",
        "gen_img = np.expand_dims(tests_x[(length*index_fig + 0)], axis=0)\n",
        "temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "print(type(temp_scale))\n",
        "temp_scale.name"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0_0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yymgs3ldNiDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ada0f496-e917-4da6-8996-99f220b312e6"
      },
      "source": [
        "#num_fig = 1\n",
        "#start_fig = 0\n",
        "#deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "\n",
        "num_fig = 1\n",
        "for start_fig in range(10):\n",
        "#for start_fig in range(2):\n",
        "  df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result= deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "  df1_scale.to_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")\n",
        "  bug_result.to_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_index.csv\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure0\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.654, 148 neurons 0.595, 268 neurons 0.481 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.536\u001b[0m\n",
            "\u001b[94m   1/5. found at 8! covered neurons percentage 52 neurons 0.673, 148 neurons 0.622, 268 neurons 0.519 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.568\u001b[0m\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.673, 148 neurons 0.628, 268 neurons 0.522 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.573\u001b[0m\n",
            "\u001b[94m   3/5. found at 3! covered neurons percentage 52 neurons 0.673, 148 neurons 0.635, 268 neurons 0.522 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.575\u001b[0m\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.673, 148 neurons 0.649, 268 neurons 0.534 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.585\u001b[0m\n",
            "figure1\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.712, 148 neurons 0.709, 268 neurons 0.582 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.637\u001b[0m\n",
            "\u001b[94m   1/5. found at 6! covered neurons percentage 52 neurons 0.712, 148 neurons 0.716, 268 neurons 0.586 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.641\u001b[0m\n",
            "   2/5. test suite was not found: averaged covered neurons 0.641 at (0, 2, 1)\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.865, 148 neurons 0.818, 268 neurons 0.739 at (0, 3, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.778\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.865, 148 neurons 0.818, 268 neurons 0.750 at (0, 4, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.784\u001b[0m\n",
            "figure2\n",
            "\u001b[94m   0/5. found at 4! covered neurons percentage 52 neurons 0.942, 148 neurons 0.838, 268 neurons 0.772 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.812\u001b[0m\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.838, 268 neurons 0.772 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.812\u001b[0m\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.838, 268 neurons 0.772 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.812\u001b[0m\n",
            "   3/5. test suite was not found: averaged covered neurons 0.816 at (0, 3, 1)\n",
            "   4/5. test suite was not found: averaged covered neurons 0.816 at (0, 3, 2)\n",
            "figure3\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.858, 268 neurons 0.799 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.833\u001b[0m\n",
            "   1/5. test suite was not found: averaged covered neurons 0.833 at (0, 1, 1)\n",
            "   2/5. test suite was not found: averaged covered neurons 0.840 at (0, 1, 2)\n",
            "\u001b[94m   3/5. found at 4! covered neurons percentage 52 neurons 0.942, 148 neurons 0.872, 268 neurons 0.802 at (0, 2, 2)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.840\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.872, 268 neurons 0.806 at (0, 3, 2)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.842\u001b[0m\n",
            "figure4\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.878, 268 neurons 0.810 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.846\u001b[0m\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.885, 268 neurons 0.828 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.859\u001b[0m\n",
            "\u001b[94m   2/5. found at 9! covered neurons percentage 52 neurons 0.942, 148 neurons 0.885, 268 neurons 0.836 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.863\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.885, 268 neurons 0.836 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.863\u001b[0m\n",
            "\u001b[94m   4/5. found at 3! covered neurons percentage 52 neurons 0.942, 148 neurons 0.885, 268 neurons 0.836 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.863\u001b[0m\n",
            "figure5\n",
            "\u001b[94m   0/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.892, 268 neurons 0.847 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.872\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:102: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.892, 268 neurons 0.854 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.876\u001b[0m\n",
            "\u001b[94m   2/5. found at 6! covered neurons percentage 52 neurons 0.942, 148 neurons 0.892, 268 neurons 0.854 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.876\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.892, 268 neurons 0.858 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.878\u001b[0m\n",
            "\u001b[94m   4/5. found at 4! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.866 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.887\u001b[0m\n",
            "figure6\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.869 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.889\u001b[0m\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.869 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.889\u001b[0m\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.873 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.891\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.873 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.891\u001b[0m\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.873 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.891\u001b[0m\n",
            "figure7\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.881 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.895\u001b[0m\n",
            "\u001b[94m   1/5. found at 4! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.881 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.895\u001b[0m\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.881 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.895\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.881 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.895\u001b[0m\n",
            "\u001b[94m   4/5. found at 3! covered neurons percentage 52 neurons 0.942, 148 neurons 0.905, 268 neurons 0.881 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.895\u001b[0m\n",
            "figure8\n",
            "   0/5. test suite was not found: averaged covered neurons 0.895 at (0, 0, 1)\n",
            "   1/5. test suite was not found: averaged covered neurons 0.895 at (0, 0, 2)\n",
            "   2/5. test suite was not found: averaged covered neurons 0.895 at (0, 0, 3)\n",
            "   3/5. test suite was not found: averaged covered neurons 0.895 at (0, 0, 4)\n",
            "   4/5. test suite was not found: averaged covered neurons 0.895 at (0, 0, 5)\n",
            "figure9\n",
            "   0/5. test suite was not found: averaged covered neurons 0.895 at (0, 0, 1)\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.912, 268 neurons 0.881 at (0, 1, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.897\u001b[0m\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.912, 268 neurons 0.881 at (0, 2, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.897\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.912, 268 neurons 0.881 at (0, 3, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.897\u001b[0m\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.912, 268 neurons 0.881 at (0, 4, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.897\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpsI1IOM-hUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "6262eafc-f913-4608-ca82-cab479b49791"
      },
      "source": [
        "df2_scale = pd.DataFrame()\n",
        "for start_fig in range(10):\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")\n",
        "  df2_scale = pd.concat([df2_scale, df1_scale])\n",
        "\n",
        "df2_scale.head()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>('block1_conv1', 0)</th>\n",
              "      <th>('block1_conv1', 1)</th>\n",
              "      <th>('block1_conv1', 2)</th>\n",
              "      <th>('block1_conv1', 3)</th>\n",
              "      <th>('block1_pool1', 0)</th>\n",
              "      <th>('block1_pool1', 1)</th>\n",
              "      <th>('block1_pool1', 2)</th>\n",
              "      <th>('block1_pool1', 3)</th>\n",
              "      <th>('block2_conv1', 0)</th>\n",
              "      <th>('block2_conv1', 1)</th>\n",
              "      <th>('block2_conv1', 2)</th>\n",
              "      <th>('block2_conv1', 3)</th>\n",
              "      <th>('block2_conv1', 4)</th>\n",
              "      <th>('block2_conv1', 5)</th>\n",
              "      <th>('block2_conv1', 6)</th>\n",
              "      <th>('block2_conv1', 7)</th>\n",
              "      <th>('block2_conv1', 8)</th>\n",
              "      <th>('block2_conv1', 9)</th>\n",
              "      <th>('block2_conv1', 10)</th>\n",
              "      <th>('block2_conv1', 11)</th>\n",
              "      <th>('block2_pool1', 0)</th>\n",
              "      <th>('block2_pool1', 1)</th>\n",
              "      <th>('block2_pool1', 2)</th>\n",
              "      <th>('block2_pool1', 3)</th>\n",
              "      <th>('block2_pool1', 4)</th>\n",
              "      <th>('block2_pool1', 5)</th>\n",
              "      <th>('block2_pool1', 6)</th>\n",
              "      <th>('block2_pool1', 7)</th>\n",
              "      <th>('block2_pool1', 8)</th>\n",
              "      <th>('block2_pool1', 9)</th>\n",
              "      <th>('block2_pool1', 10)</th>\n",
              "      <th>('block2_pool1', 11)</th>\n",
              "      <th>('before_softmax', 0)</th>\n",
              "      <th>('before_softmax', 1)</th>\n",
              "      <th>('before_softmax', 2)</th>\n",
              "      <th>('before_softmax', 3)</th>\n",
              "      <th>('before_softmax', 4)</th>\n",
              "      <th>('before_softmax', 5)</th>\n",
              "      <th>('before_softmax', 6)</th>\n",
              "      <th>('before_softmax', 7)</th>\n",
              "      <th>('before_softmax', 8)</th>\n",
              "      <th>('before_softmax', 9)</th>\n",
              "      <th>('predictions', 0)</th>\n",
              "      <th>('predictions', 1)</th>\n",
              "      <th>('predictions', 2)</th>\n",
              "      <th>('predictions', 3)</th>\n",
              "      <th>('predictions', 4)</th>\n",
              "      <th>('predictions', 5)</th>\n",
              "      <th>('predictions', 6)</th>\n",
              "      <th>('predictions', 7)</th>\n",
              "      <th>('predictions', 8)</th>\n",
              "      <th>('predictions', 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0_1</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.494</td>\n",
              "      <td>0.585</td>\n",
              "      <td>0.343</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.281</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.364</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.178</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.175</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.784</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.693</td>\n",
              "      <td>0.648</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.797</td>\n",
              "      <td>0.292</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0_2</td>\n",
              "      <td>0.598</td>\n",
              "      <td>0.681</td>\n",
              "      <td>0.452</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.670</td>\n",
              "      <td>0.388</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.915</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.882</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.792</td>\n",
              "      <td>0.613</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.197</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0_3</td>\n",
              "      <td>0.545</td>\n",
              "      <td>0.622</td>\n",
              "      <td>0.412</td>\n",
              "      <td>0.290</td>\n",
              "      <td>0.511</td>\n",
              "      <td>0.605</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.284</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.363</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.797</td>\n",
              "      <td>0.596</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.567</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.314</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0_4</td>\n",
              "      <td>0.580</td>\n",
              "      <td>0.662</td>\n",
              "      <td>0.439</td>\n",
              "      <td>0.309</td>\n",
              "      <td>0.554</td>\n",
              "      <td>0.655</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.192</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.363</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.289</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.161</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.853</td>\n",
              "      <td>0.589</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.716</td>\n",
              "      <td>0.704</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.926</td>\n",
              "      <td>0.336</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0_5</td>\n",
              "      <td>0.501</td>\n",
              "      <td>0.572</td>\n",
              "      <td>0.379</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.470</td>\n",
              "      <td>0.554</td>\n",
              "      <td>0.326</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.334</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.157</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.804</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.737</td>\n",
              "      <td>0.491</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.898</td>\n",
              "      <td>0.304</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Unnamed: 0  ('block1_conv1', 0)  ...  ('predictions', 8)  ('predictions', 9)\n",
              "0        0_1                0.525  ...               0.000               0.000\n",
              "1        0_2                0.598  ...               1.000               0.000\n",
              "2        0_3                0.545  ...               0.000               0.000\n",
              "3        0_4                0.580  ...               0.000               0.000\n",
              "4        0_5                0.501  ...               0.000               0.000\n",
              "\n",
              "[5 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QtyLAZiPeKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35808c90-bc3f-42ee-cfe0-6cfe917af389"
      },
      "source": [
        "df2_scale.iloc[:,1:]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>('block1_conv1', 0)</th>\n",
              "      <th>('block1_conv1', 1)</th>\n",
              "      <th>('block1_conv1', 2)</th>\n",
              "      <th>('block1_conv1', 3)</th>\n",
              "      <th>('block1_pool1', 0)</th>\n",
              "      <th>('block1_pool1', 1)</th>\n",
              "      <th>('block1_pool1', 2)</th>\n",
              "      <th>('block1_pool1', 3)</th>\n",
              "      <th>('block2_conv1', 0)</th>\n",
              "      <th>('block2_conv1', 1)</th>\n",
              "      <th>('block2_conv1', 2)</th>\n",
              "      <th>('block2_conv1', 3)</th>\n",
              "      <th>('block2_conv1', 4)</th>\n",
              "      <th>('block2_conv1', 5)</th>\n",
              "      <th>('block2_conv1', 6)</th>\n",
              "      <th>('block2_conv1', 7)</th>\n",
              "      <th>('block2_conv1', 8)</th>\n",
              "      <th>('block2_conv1', 9)</th>\n",
              "      <th>('block2_conv1', 10)</th>\n",
              "      <th>('block2_conv1', 11)</th>\n",
              "      <th>('block2_pool1', 0)</th>\n",
              "      <th>('block2_pool1', 1)</th>\n",
              "      <th>('block2_pool1', 2)</th>\n",
              "      <th>('block2_pool1', 3)</th>\n",
              "      <th>('block2_pool1', 4)</th>\n",
              "      <th>('block2_pool1', 5)</th>\n",
              "      <th>('block2_pool1', 6)</th>\n",
              "      <th>('block2_pool1', 7)</th>\n",
              "      <th>('block2_pool1', 8)</th>\n",
              "      <th>('block2_pool1', 9)</th>\n",
              "      <th>('block2_pool1', 10)</th>\n",
              "      <th>('block2_pool1', 11)</th>\n",
              "      <th>('before_softmax', 0)</th>\n",
              "      <th>('before_softmax', 1)</th>\n",
              "      <th>('before_softmax', 2)</th>\n",
              "      <th>('before_softmax', 3)</th>\n",
              "      <th>('before_softmax', 4)</th>\n",
              "      <th>('before_softmax', 5)</th>\n",
              "      <th>('before_softmax', 6)</th>\n",
              "      <th>('before_softmax', 7)</th>\n",
              "      <th>('before_softmax', 8)</th>\n",
              "      <th>('before_softmax', 9)</th>\n",
              "      <th>('predictions', 0)</th>\n",
              "      <th>('predictions', 1)</th>\n",
              "      <th>('predictions', 2)</th>\n",
              "      <th>('predictions', 3)</th>\n",
              "      <th>('predictions', 4)</th>\n",
              "      <th>('predictions', 5)</th>\n",
              "      <th>('predictions', 6)</th>\n",
              "      <th>('predictions', 7)</th>\n",
              "      <th>('predictions', 8)</th>\n",
              "      <th>('predictions', 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.525</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.494</td>\n",
              "      <td>0.585</td>\n",
              "      <td>0.343</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.281</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.364</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.178</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.175</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.784</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.693</td>\n",
              "      <td>0.648</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.797</td>\n",
              "      <td>0.292</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.598</td>\n",
              "      <td>0.681</td>\n",
              "      <td>0.452</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.670</td>\n",
              "      <td>0.388</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.915</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.882</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.792</td>\n",
              "      <td>0.613</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.197</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.545</td>\n",
              "      <td>0.622</td>\n",
              "      <td>0.412</td>\n",
              "      <td>0.290</td>\n",
              "      <td>0.511</td>\n",
              "      <td>0.605</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.284</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.363</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.797</td>\n",
              "      <td>0.596</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.794</td>\n",
              "      <td>0.567</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.973</td>\n",
              "      <td>0.314</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.580</td>\n",
              "      <td>0.662</td>\n",
              "      <td>0.439</td>\n",
              "      <td>0.309</td>\n",
              "      <td>0.554</td>\n",
              "      <td>0.655</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.192</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.363</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.289</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.161</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.853</td>\n",
              "      <td>0.589</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.716</td>\n",
              "      <td>0.704</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.926</td>\n",
              "      <td>0.336</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.501</td>\n",
              "      <td>0.572</td>\n",
              "      <td>0.379</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.470</td>\n",
              "      <td>0.554</td>\n",
              "      <td>0.326</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.334</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.157</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.804</td>\n",
              "      <td>0.684</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.737</td>\n",
              "      <td>0.491</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.898</td>\n",
              "      <td>0.304</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.481</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.364</td>\n",
              "      <td>0.256</td>\n",
              "      <td>0.432</td>\n",
              "      <td>0.512</td>\n",
              "      <td>0.293</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.331</td>\n",
              "      <td>0.698</td>\n",
              "      <td>0.727</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.064</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.469</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.499</td>\n",
              "      <td>0.286</td>\n",
              "      <td>0.152</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.261</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.722</td>\n",
              "      <td>0.688</td>\n",
              "      <td>0.763</td>\n",
              "      <td>0.197</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.387</td>\n",
              "      <td>0.112</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.555</td>\n",
              "      <td>0.633</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.599</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.246</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.469</td>\n",
              "      <td>0.726</td>\n",
              "      <td>0.847</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.589</td>\n",
              "      <td>0.451</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.026</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.329</td>\n",
              "      <td>0.334</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.377</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.912</td>\n",
              "      <td>0.533</td>\n",
              "      <td>0.857</td>\n",
              "      <td>0.314</td>\n",
              "      <td>0.555</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.581</td>\n",
              "      <td>0.611</td>\n",
              "      <td>0.670</td>\n",
              "      <td>0.242</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.464</td>\n",
              "      <td>0.399</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.603</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.619</td>\n",
              "      <td>0.706</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.329</td>\n",
              "      <td>0.585</td>\n",
              "      <td>0.690</td>\n",
              "      <td>0.397</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.281</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.509</td>\n",
              "      <td>0.178</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.802</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.648</td>\n",
              "      <td>0.528</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.985</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.504</td>\n",
              "      <td>0.575</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.472</td>\n",
              "      <td>0.553</td>\n",
              "      <td>0.326</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.258</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.344</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.653</td>\n",
              "      <td>0.345</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.859</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.727</td>\n",
              "      <td>0.693</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.933</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.701</td>\n",
              "      <td>0.798</td>\n",
              "      <td>0.529</td>\n",
              "      <td>0.371</td>\n",
              "      <td>0.662</td>\n",
              "      <td>0.784</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.224</td>\n",
              "      <td>0.292</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.512</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.841</td>\n",
              "      <td>0.830</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.654</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.715</td>\n",
              "      <td>0.811</td>\n",
              "      <td>0.543</td>\n",
              "      <td>0.386</td>\n",
              "      <td>0.672</td>\n",
              "      <td>0.797</td>\n",
              "      <td>0.445</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.284</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.340</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.229</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.862</td>\n",
              "      <td>0.820</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.649</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.757</td>\n",
              "      <td>0.840</td>\n",
              "      <td>0.619</td>\n",
              "      <td>0.717</td>\n",
              "      <td>0.478</td>\n",
              "      <td>0.498</td>\n",
              "      <td>0.720</td>\n",
              "      <td>0.555</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.532</td>\n",
              "      <td>0.606</td>\n",
              "      <td>0.402</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.494</td>\n",
              "      <td>0.580</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.290</td>\n",
              "      <td>0.131</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.373</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.291</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.178</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.603</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.621</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.860</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.420</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.224</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.463</td>\n",
              "      <td>0.281</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.263</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.585</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.728</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.719</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.495</td>\n",
              "      <td>0.564</td>\n",
              "      <td>0.374</td>\n",
              "      <td>0.263</td>\n",
              "      <td>0.456</td>\n",
              "      <td>0.536</td>\n",
              "      <td>0.312</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.292</td>\n",
              "      <td>0.740</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.727</td>\n",
              "      <td>0.446</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.953</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.539</td>\n",
              "      <td>0.615</td>\n",
              "      <td>0.408</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.513</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.356</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.410</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.349</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.197</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.459</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.684</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.229</td>\n",
              "      <td>0.721</td>\n",
              "      <td>0.368</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.947</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.516</td>\n",
              "      <td>0.588</td>\n",
              "      <td>0.390</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.475</td>\n",
              "      <td>0.562</td>\n",
              "      <td>0.324</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.303</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.394</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.303</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.370</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.782</td>\n",
              "      <td>0.644</td>\n",
              "      <td>0.853</td>\n",
              "      <td>0.473</td>\n",
              "      <td>0.573</td>\n",
              "      <td>0.021</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.394</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.298</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.376</td>\n",
              "      <td>0.441</td>\n",
              "      <td>0.269</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.314</td>\n",
              "      <td>0.411</td>\n",
              "      <td>0.907</td>\n",
              "      <td>0.427</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.313</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.394</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.465</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.425</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.292</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.298</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.194</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.387</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.229</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.192</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.613</td>\n",
              "      <td>0.705</td>\n",
              "      <td>0.989</td>\n",
              "      <td>0.554</td>\n",
              "      <td>0.366</td>\n",
              "      <td>0.133</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.331</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.354</td>\n",
              "      <td>0.404</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.328</td>\n",
              "      <td>0.388</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.382</td>\n",
              "      <td>0.247</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.474</td>\n",
              "      <td>0.413</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.459</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.245</td>\n",
              "      <td>0.423</td>\n",
              "      <td>0.499</td>\n",
              "      <td>0.292</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.376</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.296</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.231</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.258</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.551</td>\n",
              "      <td>0.547</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.488</td>\n",
              "      <td>0.404</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.808</td>\n",
              "      <td>0.321</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.474</td>\n",
              "      <td>0.542</td>\n",
              "      <td>0.359</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.527</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.373</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.611</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.772</td>\n",
              "      <td>0.594</td>\n",
              "      <td>0.458</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.845</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.961</td>\n",
              "      <td>0.346</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.373</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.109</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.373</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.109</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.779</td>\n",
              "      <td>0.621</td>\n",
              "      <td>0.734</td>\n",
              "      <td>0.332</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.341</td>\n",
              "      <td>0.776</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.765</td>\n",
              "      <td>0.602</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.314</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.762</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.464</td>\n",
              "      <td>0.529</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.442</td>\n",
              "      <td>0.517</td>\n",
              "      <td>0.312</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.328</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.263</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.231</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.546</td>\n",
              "      <td>0.145</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.605</td>\n",
              "      <td>0.276</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.677</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.805</td>\n",
              "      <td>0.263</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.503</td>\n",
              "      <td>0.495</td>\n",
              "      <td>0.891</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.320</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.546</td>\n",
              "      <td>0.393</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.602</td>\n",
              "      <td>0.123</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.555</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.552</td>\n",
              "      <td>0.630</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.527</td>\n",
              "      <td>0.621</td>\n",
              "      <td>0.368</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.358</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.266</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.639</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.712</td>\n",
              "      <td>0.510</td>\n",
              "      <td>0.313</td>\n",
              "      <td>0.678</td>\n",
              "      <td>0.958</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.591</td>\n",
              "      <td>0.673</td>\n",
              "      <td>0.447</td>\n",
              "      <td>0.314</td>\n",
              "      <td>0.552</td>\n",
              "      <td>0.653</td>\n",
              "      <td>0.374</td>\n",
              "      <td>0.199</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.194</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.472</td>\n",
              "      <td>0.199</td>\n",
              "      <td>0.725</td>\n",
              "      <td>0.624</td>\n",
              "      <td>0.462</td>\n",
              "      <td>0.758</td>\n",
              "      <td>0.752</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.393</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.297</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.367</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.311</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.152</td>\n",
              "      <td>0.382</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.320</td>\n",
              "      <td>0.577</td>\n",
              "      <td>0.340</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.751</td>\n",
              "      <td>0.263</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.523</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.396</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.484</td>\n",
              "      <td>0.572</td>\n",
              "      <td>0.332</td>\n",
              "      <td>0.178</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.339</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.742</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.862</td>\n",
              "      <td>0.743</td>\n",
              "      <td>0.540</td>\n",
              "      <td>0.816</td>\n",
              "      <td>0.994</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.231</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.521</td>\n",
              "      <td>0.594</td>\n",
              "      <td>0.394</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.497</td>\n",
              "      <td>0.586</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.194</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.379</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.286</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.245</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.192</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.178</td>\n",
              "      <td>0.641</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.788</td>\n",
              "      <td>0.421</td>\n",
              "      <td>0.384</td>\n",
              "      <td>0.584</td>\n",
              "      <td>0.963</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.505</td>\n",
              "      <td>0.576</td>\n",
              "      <td>0.382</td>\n",
              "      <td>0.269</td>\n",
              "      <td>0.463</td>\n",
              "      <td>0.547</td>\n",
              "      <td>0.317</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.286</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.373</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.854</td>\n",
              "      <td>0.961</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.515</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.812</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.437</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.655</td>\n",
              "      <td>0.745</td>\n",
              "      <td>0.494</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.615</td>\n",
              "      <td>0.728</td>\n",
              "      <td>0.412</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.373</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.793</td>\n",
              "      <td>0.906</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.618</td>\n",
              "      <td>0.303</td>\n",
              "      <td>0.259</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.549</td>\n",
              "      <td>0.626</td>\n",
              "      <td>0.414</td>\n",
              "      <td>0.291</td>\n",
              "      <td>0.506</td>\n",
              "      <td>0.598</td>\n",
              "      <td>0.342</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.317</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.194</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.397</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.298</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.192</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.340</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.678</td>\n",
              "      <td>0.917</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.592</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.506</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.422</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.319</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.396</td>\n",
              "      <td>0.464</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.291</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.837</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.194</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.924</td>\n",
              "      <td>0.905</td>\n",
              "      <td>0.519</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.568</td>\n",
              "      <td>0.647</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.301</td>\n",
              "      <td>0.528</td>\n",
              "      <td>0.623</td>\n",
              "      <td>0.358</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.313</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.195</td>\n",
              "      <td>0.192</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.388</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.414</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.859</td>\n",
              "      <td>0.886</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.370</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.853</td>\n",
              "      <td>0.566</td>\n",
              "      <td>0.397</td>\n",
              "      <td>0.717</td>\n",
              "      <td>0.850</td>\n",
              "      <td>0.473</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.131</td>\n",
              "      <td>0.803</td>\n",
              "      <td>0.807</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.673</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.749</td>\n",
              "      <td>0.852</td>\n",
              "      <td>0.565</td>\n",
              "      <td>0.397</td>\n",
              "      <td>0.716</td>\n",
              "      <td>0.848</td>\n",
              "      <td>0.472</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.293</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.178</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.512</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.805</td>\n",
              "      <td>0.824</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.682</td>\n",
              "      <td>0.529</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.853</td>\n",
              "      <td>0.566</td>\n",
              "      <td>0.397</td>\n",
              "      <td>0.717</td>\n",
              "      <td>0.850</td>\n",
              "      <td>0.473</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.292</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.229</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.518</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.825</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.678</td>\n",
              "      <td>0.520</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.751</td>\n",
              "      <td>0.855</td>\n",
              "      <td>0.567</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.719</td>\n",
              "      <td>0.852</td>\n",
              "      <td>0.474</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.509</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.670</td>\n",
              "      <td>0.508</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.749</td>\n",
              "      <td>0.853</td>\n",
              "      <td>0.566</td>\n",
              "      <td>0.397</td>\n",
              "      <td>0.716</td>\n",
              "      <td>0.849</td>\n",
              "      <td>0.472</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.293</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.178</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.515</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.802</td>\n",
              "      <td>0.823</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.672</td>\n",
              "      <td>0.518</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.520</td>\n",
              "      <td>0.593</td>\n",
              "      <td>0.393</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.475</td>\n",
              "      <td>0.562</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.199</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.388</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.308</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.243</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.701</td>\n",
              "      <td>0.788</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.731</td>\n",
              "      <td>0.416</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.418</td>\n",
              "      <td>0.478</td>\n",
              "      <td>0.317</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.281</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.109</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.337</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.229</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.152</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.503</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.631</td>\n",
              "      <td>0.601</td>\n",
              "      <td>0.615</td>\n",
              "      <td>0.484</td>\n",
              "      <td>0.293</td>\n",
              "      <td>0.670</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.800</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.535</td>\n",
              "      <td>0.610</td>\n",
              "      <td>0.404</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.497</td>\n",
              "      <td>0.585</td>\n",
              "      <td>0.340</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.407</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.463</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.593</td>\n",
              "      <td>0.709</td>\n",
              "      <td>0.517</td>\n",
              "      <td>0.632</td>\n",
              "      <td>0.368</td>\n",
              "      <td>0.247</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.471</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.525</td>\n",
              "      <td>0.598</td>\n",
              "      <td>0.397</td>\n",
              "      <td>0.279</td>\n",
              "      <td>0.487</td>\n",
              "      <td>0.574</td>\n",
              "      <td>0.334</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.192</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.393</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.309</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.493</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.730</td>\n",
              "      <td>0.754</td>\n",
              "      <td>0.556</td>\n",
              "      <td>0.595</td>\n",
              "      <td>0.397</td>\n",
              "      <td>0.367</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.608</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.471</td>\n",
              "      <td>0.538</td>\n",
              "      <td>0.357</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.437</td>\n",
              "      <td>0.514</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.356</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.290</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.390</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.477</td>\n",
              "      <td>0.702</td>\n",
              "      <td>0.317</td>\n",
              "      <td>0.552</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.332</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.520</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ('block1_conv1', 0)  ...  ('predictions', 9)\n",
              "0                0.525  ...               0.000\n",
              "1                0.598  ...               0.000\n",
              "2                0.545  ...               0.000\n",
              "3                0.580  ...               0.000\n",
              "4                0.501  ...               0.000\n",
              "0                0.481  ...               0.000\n",
              "1                0.469  ...               0.000\n",
              "2                0.555  ...               0.000\n",
              "3                0.000  ...               0.000\n",
              "4                0.377  ...               0.000\n",
              "0                0.000  ...               0.000\n",
              "1                0.619  ...               0.000\n",
              "2                0.504  ...               0.000\n",
              "3                0.701  ...               0.000\n",
              "4                0.715  ...               0.000\n",
              "0                0.001  ...               0.000\n",
              "1                0.532  ...               0.000\n",
              "2                0.420  ...               0.000\n",
              "3                0.495  ...               0.000\n",
              "4                0.539  ...               0.000\n",
              "0                0.516  ...               0.000\n",
              "1                0.394  ...               0.000\n",
              "2                0.465  ...               0.000\n",
              "3                0.354  ...               0.000\n",
              "4                0.459  ...               0.000\n",
              "0                0.474  ...               0.000\n",
              "1                  nan  ...               0.240\n",
              "2                0.464  ...               0.000\n",
              "3                0.000  ...               0.000\n",
              "4                0.000  ...               0.000\n",
              "0                0.552  ...               0.000\n",
              "1                0.591  ...               0.000\n",
              "2                0.393  ...               0.000\n",
              "3                0.523  ...               0.000\n",
              "4                0.521  ...               0.000\n",
              "0                0.505  ...               0.000\n",
              "1                0.655  ...               0.000\n",
              "2                0.549  ...               0.000\n",
              "3                0.422  ...               0.000\n",
              "4                0.568  ...               0.000\n",
              "0                0.750  ...               0.000\n",
              "1                0.749  ...               0.000\n",
              "2                0.750  ...               0.000\n",
              "3                0.751  ...               0.000\n",
              "4                0.749  ...               0.000\n",
              "0                0.520  ...               0.000\n",
              "1                0.418  ...               0.000\n",
              "2                0.535  ...               0.000\n",
              "3                0.525  ...               0.000\n",
              "4                0.471  ...               0.000\n",
              "\n",
              "[50 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiVj9Wjd-rVv",
        "colab_type": "text"
      },
      "source": [
        "以下が、df1_scaleに格納されているニューロンの出力値から発火/非発火の集計をとるアルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B19EsPFQrak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "thres = 0.1\n",
        "bools = df2_scale.iloc[:,1:] > thres\n",
        "\n",
        "for i, count in enumerate(bools.sum()):\n",
        "#    print(bools.sum().index[i][0], bools.sum().index[i][1], count)\n",
        "    cont = [bools.sum().index[i][0], count, bools.sum().index[i][1]]\n",
        "    cont = pd.Series(cont, index=bools_sum_layer.columns, name=str(i))\n",
        "    bools_sum_layer = bools_sum_layer.append(cont)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGmSlrMm7g6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bools.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W4ySii4Txkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bools.sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNk4OB6Pqq7v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "163d5689-f187-46a3-a431-a4dacd8a0a57"
      },
      "source": [
        "import pylab as pl\n",
        "\n",
        "#thres = 0.1\n",
        "#bools = df1_scale > thres\n",
        "bools.sum().hist()\n",
        "pl.xlabel(\"activation time\")\n",
        "pl.ylabel(\"number of neurons\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'number of neurons')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAWk5WLAPn5TcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRhR6N3GEi9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "outputId": "41f6c5c7-0dbc-4536-c8f2-32d7795e6458"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#print(test.index[0],test.index[0][0])\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(12, 8))\n",
        "bools.sum().plot(kind=\"bar\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0d12911be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAIwCAYAAACP/wPjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E4092HQvN73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}