{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "DeepXplore_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshii/de4test/blob/test_ozawa/DeepXplore/DeepXplore_03_16_trial_last_result.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVFjZpUIRF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 共通で使うパスなどの定義\n",
        "# 共通の変数設定\n",
        "# 共通フォルダパス\n",
        "\n",
        "import datetime\n",
        "import pytz\n",
        "dt_now = datetime.datetime.now(pytz.timezone('Asia/Tokyo'))\n",
        "dt_str = str(dt_now.strftime('%Y%m%d_%H%M'))\n",
        "print(dt_str)\n",
        "\n",
        "data_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data\"\n",
        "#data_imagenet = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet\"\n",
        "data_mnist = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/MNIST\"\n",
        "#data_imagenet_seeds = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet/seeds\"\n",
        "model_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/model\"\n",
        "#output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output\"\n",
        "output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output/\" + dt_str\n",
        "tmp_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/tmp\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hv53U_OdAFYb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4lP4POGn5Xj",
        "colab_type": "code",
        "outputId": "37f8d189-3117-4332-bc34-b33dca35ba95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "\n",
        "# Goggle Drive つなぐ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3d3Suv3Q5Db",
        "colab_type": "code",
        "outputId": "dcaa27da-b56a-4356-8da9-a7e78bbc96c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# outputフォルダ内容物のクリーンアップ削除\n",
        "!mkdir \"$output_dir\"\n",
        "!rm \"$output_dir\"/*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200208_2052/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdHYnLT0Nbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST')\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDY8sDVBqxmK",
        "colab_type": "code",
        "outputId": "5059e5ca-8ab3-4d86-8a17-8a8b5db7d141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input\n",
        "import imageio\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "import os\n",
        "\n",
        "# TensorFlowでGPUを使っているかのチェック：\n",
        "# \"device_type: \"GPU\" \" があればOK\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 379648895375880707, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 2714855111179867903\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 281997453793643818\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15956161332\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 16714337393379200846\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQ2M_8glJNM",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "98c7c53c-4b81-4ded-eb1b-0b1f1edbe5f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "# DeepXplore のパラメータ設定部\n",
        "# read the parameter\n",
        "# argument parsing\n",
        "parser = argparse.ArgumentParser(description='Main function for difference-inducing input generation in MNIST dataset')\n",
        "parser.add_argument('transformation', help=\"realistic transformation type\", choices=['light', 'occl', 'blackout'])\n",
        "parser.add_argument('weight_diff', help=\"weight hyperparm to control differential behavior\", type=float)\n",
        "parser.add_argument('weight_nc', help=\"weight hyperparm to control neuron coverage\", type=float)\n",
        "parser.add_argument('step', help=\"step size of gradient descent\", type=float)\n",
        "parser.add_argument('seeds', help=\"number of seeds of input\", type=int)\n",
        "parser.add_argument('grad_iterations', help=\"number of iterations of gradient descent\", type=int)\n",
        "parser.add_argument('threshold', help=\"threshold for determining neuron activated\", type=float)\n",
        "parser.add_argument('-t', '--target_model', help=\"target model that we want it predicts differently\",\n",
        "                    choices=[0, 1, 2], default=0, type=int)\n",
        "parser.add_argument('-sp', '--start_point', help=\"occlusion upper left corner coordinate\", default=(0, 0), type=tuple)\n",
        "parser.add_argument('-occl_size', '--occlusion_size', help=\"occlusion size\", default=(10, 10), type=tuple)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-occl_size', '--occlusion_size'], dest='occlusion_size', nargs=None, const=None, default=(10, 10), type=<class 'tuple'>, choices=None, help='occlusion size', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93KW-VGjMzun",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title ノイズのタイプ light / occl / blackout\n",
        "#@body {light,occl,blackout} weight_diff weight_nc step seeds\n",
        "#                   grad_iterations threshold\n",
        "noise_type = \"light\" #@param [\"light\", \"occl\", \"blackout\"]\n",
        "weight_diff = \"0.1\" #@param {type:\"string\"}\n",
        "weight_nc = \"0.1\" #@param {type:\"string\"}\n",
        "step = \"5\" #@param {type:\"string\"}　#50だとほぼ白飛び\n",
        "seeds = \"10\" #@param {type:\"string\"}\n",
        "grad_iterations = \"40\" #@param {type:\"string\"}\n",
        "threshold = \"0.1\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk80fCuPts9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = parser.parse_args([noise_type, weight_diff, weight_nc, step, seeds, grad_iterations, threshold])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tTYrllplJNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x.reshape(x.shape[1], x.shape[2])  # original shape (1,img_rows, img_cols,1)\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "def constraint_occl(gradients, start_point, rect_shape):\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def constraint_light(gradients):\n",
        "    new_grads = np.ones_like(gradients)\n",
        "    grad_mean = np.mean(gradients)\n",
        "    return grad_mean * new_grads\n",
        "\n",
        "\n",
        "def constraint_black(gradients, rect_shape=(6, 6)):\n",
        "    start_point = (\n",
        "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    if np.mean(patch) < 0:\n",
        "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def init_coverage_tables(model1, model2, model3):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    model_layer_dict2 = defaultdict(bool)\n",
        "    model_layer_dict3 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    init_dict(model2, model_layer_dict2)\n",
        "    init_dict(model3, model_layer_dict3)\n",
        "    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n",
        "\n",
        "\n",
        "def init_dict(model, model_layer_dict):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_dict[(layer.name, index)] = False\n",
        "\n",
        "\n",
        "def neuron_to_cover(model_layer_dict):\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "    else:\n",
        "        layer_name, index = random.choice(model_layer_dict.keys())\n",
        "    return layer_name, index\n",
        "\n",
        "\n",
        "def neuron_covered(model_layer_dict):\n",
        "    covered_neurons = len([v for v in model_layer_dict.values() if v])\n",
        "    total_neurons = len(model_layer_dict)\n",
        "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n",
        "\n",
        "\n",
        "def update_coverage(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n",
        "                model_layer_dict[(layer_names[i], num_neuron)] = True\n",
        "\n",
        "\n",
        "def full_coverage(model_layer_dict):\n",
        "    if False in model_layer_dict.values():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
        "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
        "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
        "    X_scaled = X_std * (rmax - rmin) + rmin\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def fired(model, layer_name, index, input_data, threshold=0):\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
        "    scaled = scale(intermediate_layer_output)\n",
        "    if np.mean(scaled[..., index]) > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def diverged(predictions1, predictions2, predictions3, target):\n",
        "    #     if predictions2 == predictions3 == target and predictions1 != target:\n",
        "    if not predictions1 == predictions2 == predictions3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "#add\n",
        "def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n",
        "    return [os.path.join(root, f)\n",
        "            for root, _, files in os.walk(directory) for f in files\n",
        "            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDDT47Er6u23",
        "colab_type": "code",
        "outputId": "529a0666-db23-4e2c-ef6d-e968c4fc2d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "'''\n",
        "LeNet-1\n",
        "'''\n",
        "def Model1(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        print(x_train.shape)\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(4, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(12, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "#        model.save_weights('./Model1.h5')\n",
        "        model.save_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "#        model.load_weights('./Model1.h5')\n",
        "        model.load_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        print(bcolors.OKBLUE + 'Model1 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model1(train=True)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n",
            "(60000, 28, 28)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 14s 234us/step - loss: 0.4348 - acc: 0.8687 - val_loss: 0.1455 - val_acc: 0.9577\n",
            "\n",
            "\n",
            "Overall Test score: 0.14546229083687068\n",
            "Overall Test accuracy: 0.9577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU88awZf68JO",
        "colab_type": "code",
        "outputId": "373bbe68-c1be-4912-8e8f-7f99d6610228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "'''\n",
        "LeNet-4\n",
        "'''\n",
        "def Model2(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(84, activation='relu', name='fc1')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model2.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model2.h5')\n",
        "        print(bcolors.OKBLUE + 'Model2 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model2(train=True)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.3409 - acc: 0.8962 - val_loss: 0.1182 - val_acc: 0.9608\n",
            "\n",
            "\n",
            "Overall Test score: 0.11821212214753032\n",
            "Overall Test accuracy: 0.9608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufs67uRN7KYL",
        "colab_type": "code",
        "outputId": "9547e6f6-c4d0-4b03-efc4-24797fec38e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "'''\n",
        "LeNet-5\n",
        "'''\n",
        "def Model3(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(120, activation='relu', name='fc1')(x)\n",
        "    x = Dense(84, activation='relu', name='fc2')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model3.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model3.h5')\n",
        "        print(bcolors.OKBLUE + 'Model3 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model3(train=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.3977 - acc: 0.8748 - val_loss: 0.2580 - val_acc: 0.9181\n",
            "\n",
            "\n",
            "Overall Test score: 0.2580157620459795\n",
            "Overall Test accuracy: 0.9181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9QiwlW7WWR",
        "colab_type": "code",
        "outputId": "3e6bab56-1723-4692-db90-824d5965438f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# input MNIST image dimensions \n",
        "img_rows, img_cols = 28, 28\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "#define the model instance\n",
        "model1 = Model1(input_tensor=input_tensor)\n",
        "model2 = Model2(input_tensor=input_tensor)\n",
        "model3 = Model3(input_tensor=input_tensor)\n",
        "\n",
        "#define the dictionary of neuron coverage\n",
        "model_layer_dict1, model_layer_dict2, model_layer_dict3 = init_coverage_tables(model1, model2, model3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[94mModel1 loaded\u001b[0m\n",
            "\u001b[94mModel2 loaded\u001b[0m\n",
            "\u001b[94mModel3 loaded\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MS5H74Q4XA",
        "colab_type": "text"
      },
      "source": [
        "### 入力するデータの選別(1/2)\n",
        "\n",
        "０～９の数字をそれぞれ束ねてデータを準備する\n",
        "[00...011...1......99...9]。\n",
        "\n",
        "* test_per_fig_x: \n",
        "    各数字に対する画像データを格納\n",
        "* test_per_fig_y: \n",
        "    各数字に対する教師ラベルを格納\n",
        "* tests_x: \n",
        "    test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: \n",
        "    tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brFIdSfL8nHP",
        "colab_type": "code",
        "outputId": "eb048fea-a45a-4170-b24b-cc317a918488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# load the MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "num_fig = 10\n",
        "test_per_fig_x = np.array([])\n",
        "test_per_fig_y = np.array([])\n",
        "tests_x = np.array([])\n",
        "tests_y = np.array([])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "#x_train = x_train.astype('float')\n",
        "#x_test = x_test.astype('float')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "length = int(args.seeds)\n",
        "#length = int(args.seeds)\n",
        "for i in range(num_fig):\n",
        "  cond = [(x==i) for x in y_test]\n",
        "  test_per_fig_x = x_test[cond]\n",
        "  test_per_fig_y = y_test[cond]\n",
        "#  np.set_printoptions(formatter={'int': '{:07d}'.format})\n",
        "  print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "  tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "  tests_y = np.append(tests_y, test_per_fig_y[:length])\n",
        "#  conds = [conds, cond]\n",
        "print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "\n",
        "###### MNIST data, shuffled and split by train and test sets\n",
        "#####(_, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "###### modify the numpy data for the Keras model\n",
        "#####x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "#####input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "#tests_x.shape\n",
        "tests_x = tests_x.astype('float32')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure: 0 , shape: (980, 28, 28) , deviation: -20\n",
            "figure: 1 , shape: (1135, 28, 28) , deviation: 135\n",
            "figure: 2 , shape: (1032, 28, 28) , deviation: 32\n",
            "figure: 3 , shape: (1010, 28, 28) , deviation: 10\n",
            "figure: 4 , shape: (982, 28, 28) , deviation: -18\n",
            "figure: 5 , shape: (892, 28, 28) , deviation: -108\n",
            "figure: 6 , shape: (958, 28, 28) , deviation: -42\n",
            "figure: 7 , shape: (1028, 28, 28) , deviation: 28\n",
            "figure: 8 , shape: (974, 28, 28) , deviation: -26\n",
            "figure: 9 , shape: (1009, 28, 28) , deviation: 9\n",
            "check! 10.0 = 10 equal?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUOK_EjJW2KT",
        "colab_type": "text"
      },
      "source": [
        "## 入力するデータの選別(2/2)\n",
        "０～９の数字をそれぞれ束ねてデータを準備する [00...000]。\n",
        "\n",
        "* test_per_fig_x: 各数字に対する画像データを格納\n",
        "* test_per_fig_y: 各数字に対する教師ラベルを格納\n",
        "* tests_x: test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "id": "SLee__9UAD1Y",
        "colab": {}
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "#load the MNIST \n",
        "\n",
        "def create_data(start_fig, num_fi, length):\n",
        "  (datax_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  test_per_fig_x = np.array([])\n",
        "  test_per_fig_y = np.array([])\n",
        "  tests_x = np.array([])\n",
        "  tests_y = np.array([])\n",
        "\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test  = x_test.astype('float32')\n",
        "  x_train = x_train.astype('float')\n",
        "  x_test = x_test.astype('float')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  #length = int(args.seeds/num_fig)\n",
        "  #length = int(args.seeds)\n",
        "  for i in range(start_fig, start_fig+num_fig):\n",
        "  #i=0\n",
        "    cond = [(x==i) for x in y_test]\n",
        "    test_per_fig_x = x_test[cond]\n",
        "    test_per_fig_y = y_test[cond]\n",
        "    print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "    tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "    tests_y = np.append(tests_y, test_per_fig_y[:length])  \n",
        "\n",
        "  #conds = [conds, cond]\n",
        "  print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "  tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "  tests_x.shape\n",
        "  tests_x = tests_x.astype('float32')\n",
        "  return tests_x, tests_y, length\n",
        "  tests_x, tests_y, length = create_data(0, 10, int(args.seeds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyo2SSSz8vkT",
        "colab_type": "code",
        "outputId": "efa8a0e4-64a0-4c60-b6a5-d869b4b0e916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "print(tests_x.shape)\n",
        "print(tests_y)\n",
        "for i in range(0,1):\n",
        "  print(i)\n",
        "print(length)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 28, 28, 1)\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 4. 4. 4. 4. 4. 4. 4. 4.\n",
            " 4. 4. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 7. 7.\n",
            " 7. 7. 7. 7. 7. 7. 7. 7. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 9. 9. 9. 9. 9. 9.\n",
            " 9. 9. 9. 9.]\n",
            "0\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLlKIMxG9kEA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfdTZi4mUPf",
        "colab_type": "code",
        "outputId": "9b1051af-65df-4d12-9b18-993a175f9bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "def neuron_output(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "    \n",
        "    out_list = []\n",
        "    out_list_scale = []\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "#        scaled = scale(intermediate_layer_output[0])\n",
        "        scaled = scale(intermediate_layer_output)\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            out_list.append(np.mean(intermediate_layer_output[..., num_neuron]))\n",
        "            out_list_scale.append(np.mean(scaled[..., num_neuron]))\n",
        "\n",
        "    return out_list, out_list_scale\n",
        "\n",
        "\n",
        "def deepXplore(model_layer_dict1, model_layer_dict2, model_layer_dict3, tests_x, model1, model2, model3, num_fig, start_fig, length):\n",
        "  #print(\"test01\")\n",
        "  count_already = 0\n",
        "  count_found = 0\n",
        "  count_not_found = 0\n",
        "  temp_per_nc1 = np.array([])\n",
        "  temp_per_nc2 = np.array([])\n",
        "  temp_per_nc3 = np.array([])\n",
        "  temp_num_nc1 = np.array([])\n",
        "  temp_num_nc2 = np.array([])\n",
        "  temp_num_nc3 = np.array([])\n",
        "  #print(\"test10\")\n",
        "\n",
        "  #for each neuron\n",
        "  num_neurons1 = neuron_covered(model_layer_dict1)[1]\n",
        "  num_neurons2 = neuron_covered(model_layer_dict2)[1]\n",
        "  num_neurons3 = neuron_covered(model_layer_dict3)[1]\n",
        "  #print(\"test11\")\n",
        "\n",
        "  column_tmp1 = list(model_layer_dict1.keys())\n",
        "  column_tmp2 = list(model_layer_dict2.keys())\n",
        "  column_tmp3 = list(model_layer_dict3.keys())\n",
        "  #print(\"test12\")\n",
        "  df1 = pd.DataFrame(columns=column_tmp1)\n",
        "  df2 = pd.DataFrame(columns=column_tmp2)\n",
        "  df3 = pd.DataFrame(columns=column_tmp3)\n",
        "  df1_scale = pd.DataFrame(columns=column_tmp1)\n",
        "  df2_scale = pd.DataFrame(columns=column_tmp2)\n",
        "  df3_scale = pd.DataFrame(columns=column_tmp3)\n",
        "  df1_trace = pd.DataFrame(columns=column_tmp1)\n",
        "  df2_trace = pd.DataFrame(columns=column_tmp2)\n",
        "  df3_trace = pd.DataFrame(columns=column_tmp3)\n",
        "  df2 = pd.DataFrame(columns=column_tmp2)\n",
        "  df3 = pd.DataFrame(columns=column_tmp3)\n",
        "  tmp_list = [\"already_diff\", \"found\", \"not_found\", \"layer1\", \"index1\", \"layer2\", \"index2\", \"layer3\", \"index3\"]\n",
        "  bug_result = pd.DataFrame(columns=tmp_list)\n",
        "  trial = 1\n",
        "  #print(\"test13\")\n",
        "  #print(\"test02\")\n",
        "  for index_fig in range(num_fig):\n",
        "    index_fig = index_fig + start_fig\n",
        "    print(\"figure\"+str(index_fig))\n",
        "    for _ in range(length):\n",
        "      #gen_img = np.expand_dims(random.choice(tests_x), axis=0)\n",
        "      gen_img = np.expand_dims(tests_x[(length*index_fig + _)], axis=0)\n",
        "      orig_img = gen_img.copy()\n",
        "      # first check if input already induces differences\n",
        "      label1, label2, label3 = np.argmax(model1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "      if not label1 == label2 == label3:\n",
        "          count_already += 1\n",
        "          print(bcolors.OKGREEN + '   {}/{}. input already causes different outputs ({},{},{}) at({}, {}, {}): '.format(_, length, label1, label2, label3, count_already, count_found, count_not_found) + bcolors.ENDC)        \n",
        "\n",
        "          update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "          temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "          temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "          temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "          temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "          temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "          temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "                               \n",
        "          print(bcolors.OKGREEN + '     covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'% (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  neuron_covered(model_layer_dict2)[2], len(model_layer_dict3), neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "          averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                       neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +neuron_covered(model_layer_dict3)[1])\n",
        "          print(bcolors.OKGREEN + '     averaged covered neurons %.3f' % (averaged_nc) + bcolors.ENDC)\n",
        "\n",
        "          gen_img_deprocessed = deprocess_image(gen_img)\n",
        "\n",
        "          # save the result to disk\n",
        "          outputfilepath0 = os.path.join(output_dir, 'already_differ_' + str(label1) + '_' + str(label2) + '_' + str(label3) +'_['+ str(_) +  '].png')\n",
        "          imageio.imwrite(outputfilepath0, gen_img_deprocessed)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df1 = df1.append(temp)\n",
        "          df1_scale = df1_scale.append(temp_scale)\n",
        "##############################\n",
        "          df1_trace = df1_trace.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df2 = df2.append(temp)\n",
        "          df2_scale = df2_scale.append(temp_scale)\n",
        "##############################\n",
        "          df2_trace = df2_trace.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "          temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df3 = df3.append(temp)\n",
        "          df3_scale = df3_scale.append(temp_scale)\n",
        "##############################\n",
        "          df3_trace = df3_trace.append(temp_scale)\n",
        "\n",
        "          #print(\"test10\")\n",
        "          temp = [1, 0, 0, None, None, None, None, None, None]\n",
        "          #print(\"test11\")\n",
        "          temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          #print(\"test12\")\n",
        "          bug_result = bug_result.append(temp)\n",
        "          #print(\"test13\")\n",
        "          trial += 1\n",
        "          continue\n",
        "\n",
        "      # if all label agrees\n",
        "      orig_label = label1\n",
        "      layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n",
        "      layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n",
        "      layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n",
        "\n",
        "\n",
        "\n",
        "      # construct joint loss function\n",
        "      if args.target_model == 0:\n",
        "        loss1 = -args.weight_diff * K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 1:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = -args.weight_diff * K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 2:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n",
        "      loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n",
        "      loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n",
        "      layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron)\n",
        "\n",
        "      # for adversarial image generation\n",
        "      final_loss = K.mean(layer_output)\n",
        "\n",
        "      # we compute the gradient of the input picture wrt this loss\n",
        "      grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "\n",
        "      # this function returns the loss and grads given the input picture\n",
        "      iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n",
        "\n",
        "      #print(\"test03\")\n",
        "      # we run gradient ascent for some steps\n",
        "      for iters in range(args.grad_iterations):\n",
        "        loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = iterate([gen_img])\n",
        "        if args.transformation == 'light':\n",
        "            grads_value = constraint_light(grads_value)  # constraint the gradients value\n",
        "        elif args.transformation == 'occl':\n",
        "            grads_value = constraint_occl(grads_value, args.start_point,\n",
        "                                          args.occlusion_size)  # constraint the gradients value\n",
        "        elif args.transformation == 'blackout':\n",
        "            grads_value = constraint_black(grads_value)  # constraint the gradients value\n",
        "\n",
        "        gen_img += grads_value * args.step\n",
        "        predictions1 = np.argmax(model1.predict(gen_img)[0])\n",
        "        predictions2 = np.argmax(model2.predict(gen_img)[0])\n",
        "        predictions3 = np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "##############################\n",
        "        temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "        #print(temp)\n",
        "        temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "        temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial) + \"_\" + str(iters))\n",
        "        df1_trace = df1_trace.append(temp_scale)\n",
        "\n",
        "        temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "        temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "        temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial) + \"_\" + str(iters))\n",
        "        df2_trace = df2_trace.append(temp_scale)\n",
        "\n",
        "        temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "        temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "        temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial) + \"_\" + str(iters))\n",
        "        df3_trace = df3_trace.append(temp_scale)\n",
        "\n",
        "        #print(\"test04\")\n",
        "        if not predictions1 == predictions2 == predictions3:\n",
        "            #print(\"test04-01\")\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            count_found += 1\n",
        "            print(bcolors.OKBLUE + '%4d/%d. found at %d! covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f at (%d, %d, %d)'\n",
        "#                  % (_, args.seeds, iters, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  % (_, length, iters + 1, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                     neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                     neuron_covered(model_layer_dict3)[2], count_already, count_found, count_not_found) + bcolors.ENDC)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            print(bcolors.OKBLUE + '     averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n",
        "\n",
        "            #print(\"test04-02\")\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            #print(\"test04-03\")\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  '].png')\n",
        "            #print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "#            outputfilepath2 = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '_orig.png')\n",
        "            outputfilepath2 = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  ']_orig.png')\n",
        "            #print(outputfilepath2)\n",
        "            imageio.imwrite(outputfilepath2, orig_img_deprocessed)\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "            #print(\"test04-04\")\n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "     \n",
        "            #print(\"test04-05\")\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            #print(\"test04-06\")\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "            #print(\"test04-07\")\n",
        "            temp = [0, iters+1, 0, layer_name1, index1, layer_name2, index2,layer_name3, index3]\n",
        "            temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            bug_result = bug_result.append(temp)\n",
        "            trial += 1\n",
        "            break\n",
        "          \n",
        "          #add\n",
        "        #print(\"test05\")\n",
        "        if iters == (args.grad_iterations-1):\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            count_not_found += 1\n",
        "#              print('%4d/%d. test suite was not found: averaged covered neurons %.3f at %d/%d' % (_, args.seeds, averaged_nc, count_not_found, count_already + count_found + count_not_found))\n",
        "            print('%4d/%d. test suite was not found: averaged covered neurons %.3f at (%d, %d, %d)' % (_, length, averaged_nc, count_already, count_found, count_not_found))\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            #orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, 'not_found_' + str(label1)+'_['+ str(_) + '].png')\n",
        "#           print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "\n",
        "            \n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "\n",
        "            temp = [0, 0, 1, layer_name1, index1, layer_name2, index2,layer_name3, index3]\n",
        "            temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            bug_result = bug_result.append(temp)\n",
        "\n",
        "            trial += 1\n",
        "            \n",
        "            #break\n",
        "\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "  #print(\"test06\")\n",
        "  temp_per_nc1=temp_per_nc1.reshape(num_fig, length)\n",
        "  temp_per_nc2=temp_per_nc2.reshape(num_fig, length)\n",
        "  temp_per_nc3=temp_per_nc3.reshape(num_fig, length)\n",
        "  temp_num_nc1=temp_num_nc1.reshape(num_fig, length)\n",
        "  temp_num_nc2=temp_num_nc2.reshape(num_fig, length)\n",
        "  temp_num_nc3=temp_num_nc3.reshape(num_fig, length)\n",
        "\n",
        "  #print(\"test07\")\n",
        "#  return df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result\n",
        "  return df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result, df1_trace"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 22 µs, sys: 0 ns, total: 22 µs\n",
            "Wall time: 26.5 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHi1ffJuWH5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "ef2174d2-93bd-4a4a-a535-1f693fbbe9c5"
      },
      "source": [
        "print(length, num_fig)\n",
        "print(output_dir)\n",
        "column_tmp1 = list(model_layer_dict1.keys())\n",
        "df1 = pd.DataFrame(columns=column_tmp1)\n",
        "df1.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 10\n",
            "/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200208_2052\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(block1_conv1, 0)</th>\n",
              "      <th>(block1_conv1, 1)</th>\n",
              "      <th>(block1_conv1, 2)</th>\n",
              "      <th>(block1_conv1, 3)</th>\n",
              "      <th>(block1_pool1, 0)</th>\n",
              "      <th>(block1_pool1, 1)</th>\n",
              "      <th>(block1_pool1, 2)</th>\n",
              "      <th>(block1_pool1, 3)</th>\n",
              "      <th>(block2_conv1, 0)</th>\n",
              "      <th>(block2_conv1, 1)</th>\n",
              "      <th>(block2_conv1, 2)</th>\n",
              "      <th>(block2_conv1, 3)</th>\n",
              "      <th>(block2_conv1, 4)</th>\n",
              "      <th>(block2_conv1, 5)</th>\n",
              "      <th>(block2_conv1, 6)</th>\n",
              "      <th>(block2_conv1, 7)</th>\n",
              "      <th>(block2_conv1, 8)</th>\n",
              "      <th>(block2_conv1, 9)</th>\n",
              "      <th>(block2_conv1, 10)</th>\n",
              "      <th>(block2_conv1, 11)</th>\n",
              "      <th>(block2_pool1, 0)</th>\n",
              "      <th>(block2_pool1, 1)</th>\n",
              "      <th>(block2_pool1, 2)</th>\n",
              "      <th>(block2_pool1, 3)</th>\n",
              "      <th>(block2_pool1, 4)</th>\n",
              "      <th>(block2_pool1, 5)</th>\n",
              "      <th>(block2_pool1, 6)</th>\n",
              "      <th>(block2_pool1, 7)</th>\n",
              "      <th>(block2_pool1, 8)</th>\n",
              "      <th>(block2_pool1, 9)</th>\n",
              "      <th>(block2_pool1, 10)</th>\n",
              "      <th>(block2_pool1, 11)</th>\n",
              "      <th>(before_softmax, 0)</th>\n",
              "      <th>(before_softmax, 1)</th>\n",
              "      <th>(before_softmax, 2)</th>\n",
              "      <th>(before_softmax, 3)</th>\n",
              "      <th>(before_softmax, 4)</th>\n",
              "      <th>(before_softmax, 5)</th>\n",
              "      <th>(before_softmax, 6)</th>\n",
              "      <th>(before_softmax, 7)</th>\n",
              "      <th>(before_softmax, 8)</th>\n",
              "      <th>(before_softmax, 9)</th>\n",
              "      <th>(predictions, 0)</th>\n",
              "      <th>(predictions, 1)</th>\n",
              "      <th>(predictions, 2)</th>\n",
              "      <th>(predictions, 3)</th>\n",
              "      <th>(predictions, 4)</th>\n",
              "      <th>(predictions, 5)</th>\n",
              "      <th>(predictions, 6)</th>\n",
              "      <th>(predictions, 7)</th>\n",
              "      <th>(predictions, 8)</th>\n",
              "      <th>(predictions, 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [(block1_conv1, 0), (block1_conv1, 1), (block1_conv1, 2), (block1_conv1, 3), (block1_pool1, 0), (block1_pool1, 1), (block1_pool1, 2), (block1_pool1, 3), (block2_conv1, 0), (block2_conv1, 1), (block2_conv1, 2), (block2_conv1, 3), (block2_conv1, 4), (block2_conv1, 5), (block2_conv1, 6), (block2_conv1, 7), (block2_conv1, 8), (block2_conv1, 9), (block2_conv1, 10), (block2_conv1, 11), (block2_pool1, 0), (block2_pool1, 1), (block2_pool1, 2), (block2_pool1, 3), (block2_pool1, 4), (block2_pool1, 5), (block2_pool1, 6), (block2_pool1, 7), (block2_pool1, 8), (block2_pool1, 9), (block2_pool1, 10), (block2_pool1, 11), (before_softmax, 0), (before_softmax, 1), (before_softmax, 2), (before_softmax, 3), (before_softmax, 4), (before_softmax, 5), (before_softmax, 6), (before_softmax, 7), (before_softmax, 8), (before_softmax, 9), (predictions, 0), (predictions, 1), (predictions, 2), (predictions, 3), (predictions, 4), (predictions, 5), (predictions, 6), (predictions, 7), (predictions, 8), (predictions, 9)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSSBfoAZreWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f16bb134-cada-4829-9665-8705daf70a37"
      },
      "source": [
        "index_fig = 0\n",
        "trial = 0\n",
        "gen_img = np.expand_dims(tests_x[(length*index_fig + 0)], axis=0)\n",
        "temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "print(type(temp_scale))\n",
        "temp_scale.name"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0_0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yymgs3ldNiDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fc517461-4722-4dfa-9844-05b21703d46b"
      },
      "source": [
        "%%time\n",
        "#num_fig = 1\n",
        "#start_fig = 0\n",
        "#deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "\n",
        "num_fig = 1\n",
        "#for start_fig in range(10):\n",
        "for start_fig in range(4,10):\n",
        "#for start_fig in range(2):\n",
        "#  df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result, df1_trace, df2_trace. df3_trace= deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "  df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result, df1_trace= deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "#  df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result= deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "  df1_scale.to_csv(output_dir+ \"/03_16_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")\n",
        "  bug_result.to_csv(output_dir+ \"/03_16_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_index.csv\")\n",
        "  df1_trace.to_csv(output_dir+ \"/03_16_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_trace.csv\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure4\n",
            "\u001b[92m   0/10. input already causes different outputs (4,4,9) at(1, 0, 0): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.462, 148 neurons 0.459, 268 neurons 0.470\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.466\u001b[0m\n",
            "\u001b[92m   1/10. input already causes different outputs (4,4,8) at(2, 0, 0): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.596, 148 neurons 0.561, 268 neurons 0.604\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.590\u001b[0m\n",
            "\u001b[94m   2/10. found at 2! covered neurons percentage 52 neurons 0.654, 148 neurons 0.649, 268 neurons 0.660 at (2, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.656\u001b[0m\n",
            "\u001b[92m   3/10. input already causes different outputs (4,4,9) at(3, 1, 0): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.654, 148 neurons 0.669, 268 neurons 0.668\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.667\u001b[0m\n",
            "\u001b[92m   4/10. input already causes different outputs (4,4,9) at(4, 1, 0): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.654, 148 neurons 0.669, 268 neurons 0.672\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.669\u001b[0m\n",
            "   5/10. test suite was not found: averaged covered neurons 0.714 at (4, 1, 1)\n",
            "\u001b[92m   6/10. input already causes different outputs (4,4,9) at(5, 1, 1): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.712, 148 neurons 0.709, 268 neurons 0.731\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.722\u001b[0m\n",
            "\u001b[92m   7/10. input already causes different outputs (4,4,9) at(6, 1, 1): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.712, 148 neurons 0.709, 268 neurons 0.735\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.724\u001b[0m\n",
            "\u001b[94m   8/10. found at 5! covered neurons percentage 52 neurons 0.731, 148 neurons 0.709, 268 neurons 0.735 at (6, 2, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.726\u001b[0m\n",
            "\u001b[94m   9/10. found at 8! covered neurons percentage 52 neurons 0.750, 148 neurons 0.709, 268 neurons 0.735 at (6, 3, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.729\u001b[0m\n",
            "figure5\n",
            "   0/10. test suite was not found: averaged covered neurons 0.765 at (0, 0, 1)\n",
            "   1/10. test suite was not found: averaged covered neurons 0.776 at (0, 0, 2)\n",
            "   2/10. test suite was not found: averaged covered neurons 0.778 at (0, 0, 3)\n",
            "   3/10. test suite was not found: averaged covered neurons 0.784 at (0, 0, 4)\n",
            "   4/10. test suite was not found: averaged covered neurons 0.791 at (0, 0, 5)\n",
            "\u001b[94m   5/10. found at 18! covered neurons percentage 52 neurons 0.865, 148 neurons 0.777, 268 neurons 0.813 at (0, 1, 5)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.808\u001b[0m\n",
            "\u001b[94m   6/10. found at 12! covered neurons percentage 52 neurons 0.885, 148 neurons 0.777, 268 neurons 0.817 at (0, 2, 5)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.812\u001b[0m\n",
            "   7/10. test suite was not found: averaged covered neurons 0.812 at (0, 2, 6)\n",
            "   8/10. test suite was not found: averaged covered neurons 0.814 at (0, 2, 7)\n",
            "   9/10. test suite was not found: averaged covered neurons 0.814 at (0, 2, 8)\n",
            "figure6\n",
            "\u001b[94m   0/10. found at 10! covered neurons percentage 52 neurons 0.885, 148 neurons 0.784, 268 neurons 0.828 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.821\u001b[0m\n",
            "\u001b[94m   1/10. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.784, 268 neurons 0.828 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.821\u001b[0m\n",
            "   2/10. test suite was not found: averaged covered neurons 0.821 at (0, 2, 1)\n",
            "\u001b[94m   3/10. found at 12! covered neurons percentage 52 neurons 0.885, 148 neurons 0.784, 268 neurons 0.836 at (0, 3, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.825\u001b[0m\n",
            "\u001b[94m   4/10. found at 17! covered neurons percentage 52 neurons 0.885, 148 neurons 0.784, 268 neurons 0.840 at (0, 4, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.827\u001b[0m\n",
            "\u001b[94m   5/10. found at 3! covered neurons percentage 52 neurons 0.885, 148 neurons 0.791, 268 neurons 0.840 at (0, 5, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.829\u001b[0m\n",
            "   6/10. test suite was not found: averaged covered neurons 0.831 at (0, 5, 2)\n",
            "   7/10. test suite was not found: averaged covered neurons 0.831 at (0, 5, 3)\n",
            "\u001b[94m   8/10. found at 5! covered neurons percentage 52 neurons 0.885, 148 neurons 0.791, 268 neurons 0.843 at (0, 6, 3)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.831\u001b[0m\n",
            "\u001b[94m   9/10. found at 8! covered neurons percentage 52 neurons 0.885, 148 neurons 0.791, 268 neurons 0.843 at (0, 7, 3)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.831\u001b[0m\n",
            "figure7\n",
            "   0/10. test suite was not found: averaged covered neurons 0.840 at (0, 0, 1)\n",
            "   1/10. test suite was not found: averaged covered neurons 0.840 at (0, 0, 2)\n",
            "\u001b[94m   2/10. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.804, 268 neurons 0.851 at (0, 1, 2)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.840\u001b[0m\n",
            "   3/10. test suite was not found: averaged covered neurons 0.846 at (0, 1, 3)\n",
            "   4/10. test suite was not found: averaged covered neurons 0.846 at (0, 1, 4)\n",
            "   5/10. test suite was not found: averaged covered neurons 0.846 at (0, 1, 5)\n",
            "\u001b[94m   6/10. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.818, 268 neurons 0.858 at (0, 2, 5)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.848\u001b[0m\n",
            "\u001b[94m   7/10. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.818, 268 neurons 0.858 at (0, 3, 5)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.848\u001b[0m\n",
            "   8/10. test suite was not found: averaged covered neurons 0.848 at (0, 3, 6)\n",
            "\u001b[94m   9/10. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.818, 268 neurons 0.858 at (0, 4, 6)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.848\u001b[0m\n",
            "figure8\n",
            "   0/10. test suite was not found: averaged covered neurons 0.853 at (0, 0, 1)\n",
            "   1/10. test suite was not found: averaged covered neurons 0.855 at (0, 0, 2)\n",
            "\u001b[94m   2/10. found at 37! covered neurons percentage 52 neurons 0.885, 148 neurons 0.831, 268 neurons 0.862 at (0, 1, 2)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.855\u001b[0m\n",
            "   3/10. test suite was not found: averaged covered neurons 0.855 at (0, 1, 3)\n",
            "   4/10. test suite was not found: averaged covered neurons 0.855 at (0, 1, 4)\n",
            "   5/10. test suite was not found: averaged covered neurons 0.855 at (0, 1, 5)\n",
            "\u001b[94m   6/10. found at 38! covered neurons percentage 52 neurons 0.885, 148 neurons 0.831, 268 neurons 0.862 at (0, 2, 5)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.855\u001b[0m\n",
            "   7/10. test suite was not found: averaged covered neurons 0.855 at (0, 2, 6)\n",
            "   8/10. test suite was not found: averaged covered neurons 0.855 at (0, 2, 7)\n",
            "\u001b[92m   9/10. input already causes different outputs (8,3,8) at(1, 2, 7): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.885, 148 neurons 0.831, 268 neurons 0.866\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.857\u001b[0m\n",
            "figure9\n",
            "\u001b[94m   0/10. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.838, 268 neurons 0.869 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.863\u001b[0m\n",
            "   1/10. test suite was not found: averaged covered neurons 0.863 at (0, 1, 1)\n",
            "\u001b[94m   2/10. found at 3! covered neurons percentage 52 neurons 0.904, 148 neurons 0.838, 268 neurons 0.869 at (0, 2, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.863\u001b[0m\n",
            "\u001b[94m   3/10. found at 3! covered neurons percentage 52 neurons 0.904, 148 neurons 0.838, 268 neurons 0.869 at (0, 3, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.863\u001b[0m\n",
            "\u001b[94m   4/10. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.838, 268 neurons 0.869 at (0, 4, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.863\u001b[0m\n",
            "\u001b[94m   5/10. found at 30! covered neurons percentage 52 neurons 0.904, 148 neurons 0.838, 268 neurons 0.869 at (0, 5, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.863\u001b[0m\n",
            "   6/10. test suite was not found: averaged covered neurons 0.863 at (0, 5, 2)\n",
            "\u001b[92m   7/10. input already causes different outputs (7,9,9) at(1, 5, 2): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.904, 148 neurons 0.838, 268 neurons 0.873\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.865\u001b[0m\n",
            "   8/10. test suite was not found: averaged covered neurons 0.865 at (1, 5, 3)\n",
            "\u001b[92m   9/10. input already causes different outputs (4,9,9) at(2, 5, 3): \u001b[0m\n",
            "\u001b[92m     covered neurons percentage 52 neurons 0.904, 148 neurons 0.838, 268 neurons 0.873\u001b[0m\n",
            "\u001b[92m     averaged covered neurons 0.865\u001b[0m\n",
            "CPU times: user 40min 1s, sys: 10 s, total: 40min 11s\n",
            "Wall time: 40min 9s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpsI1IOM-hUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "de844e4f-8596-4ca8-eac3-57547efe91eb"
      },
      "source": [
        "df2_scale = pd.DataFrame()\n",
        "df3_scale = pd.DataFrame()\n",
        "df4_scale = pd.DataFrame()\n",
        "for start_fig in range(10):\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")\n",
        "  df2_scale = pd.concat([df2_scale, df1_scale])\n",
        "\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_index.csv\")\n",
        "  df3_scale = pd.concat([df3_scale, df1_scale])\n",
        "\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_trace.csv\")\n",
        "  df4_scale = pd.concat([df4_scale, df1_scale])\n",
        "\n",
        "tmp_list = list(df2_scale.iloc[:,0])\n",
        "df2_scale.index = tmp_list\n",
        "print(df2_scale.iloc[:,1:].head())\n",
        "\n",
        "tmp_list = list(df3_scale.iloc[:,0])\n",
        "df3_scale.index = tmp_list\n",
        "print(df3_scale.iloc[:,1:].head())\n",
        "\n",
        "tmp_list = list(df4_scale.iloc[:,0])\n",
        "df4_scale.index = tmp_list\n",
        "print(df4_scale.iloc[:,1:].head())\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     ('block1_conv1', 0)  ...  ('predictions', 9)\n",
            "0_1                0.384  ...               0.000\n",
            "0_2                0.387  ...               0.000\n",
            "0_3                0.257  ...               0.000\n",
            "0_4                0.421  ...               0.000\n",
            "0_5                0.223  ...               0.000\n",
            "\n",
            "[5 rows x 52 columns]\n",
            "     already_diff  found  not_found  ... index2        layer3 index3\n",
            "0_1         0.000  0.000      1.000  ...  6.000  block2_pool1  0.000\n",
            "0_2         0.000  0.000      1.000  ...  7.000           fc2 83.000\n",
            "0_3         0.000  0.000      1.000  ... 76.000           fc2  9.000\n",
            "0_4         0.000  0.000      1.000  ...  3.000  block1_pool1  3.000\n",
            "0_5         0.000  0.000      1.000  ...  2.000  block2_pool1 12.000\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "       ('block1_conv1', 0)  ...  ('predictions', 9)\n",
            "0_1_0                0.307  ...               0.000\n",
            "0_1_1                0.323  ...               0.000\n",
            "0_1_2                0.342  ...               0.000\n",
            "0_1_3                0.371  ...               0.000\n",
            "0_1_4                0.385  ...               0.000\n",
            "\n",
            "[5 rows x 52 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QtyLAZiPeKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "8ee3da3d-b486-4f21-9517-05ea1371bb7e"
      },
      "source": [
        "df2_scale.iloc[0:6,1:]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>('block1_conv1', 0)</th>\n",
              "      <th>('block1_conv1', 1)</th>\n",
              "      <th>('block1_conv1', 2)</th>\n",
              "      <th>('block1_conv1', 3)</th>\n",
              "      <th>('block1_pool1', 0)</th>\n",
              "      <th>('block1_pool1', 1)</th>\n",
              "      <th>('block1_pool1', 2)</th>\n",
              "      <th>('block1_pool1', 3)</th>\n",
              "      <th>('block2_conv1', 0)</th>\n",
              "      <th>('block2_conv1', 1)</th>\n",
              "      <th>('block2_conv1', 2)</th>\n",
              "      <th>('block2_conv1', 3)</th>\n",
              "      <th>('block2_conv1', 4)</th>\n",
              "      <th>('block2_conv1', 5)</th>\n",
              "      <th>('block2_conv1', 6)</th>\n",
              "      <th>('block2_conv1', 7)</th>\n",
              "      <th>('block2_conv1', 8)</th>\n",
              "      <th>('block2_conv1', 9)</th>\n",
              "      <th>('block2_conv1', 10)</th>\n",
              "      <th>('block2_conv1', 11)</th>\n",
              "      <th>('block2_pool1', 0)</th>\n",
              "      <th>('block2_pool1', 1)</th>\n",
              "      <th>('block2_pool1', 2)</th>\n",
              "      <th>('block2_pool1', 3)</th>\n",
              "      <th>('block2_pool1', 4)</th>\n",
              "      <th>('block2_pool1', 5)</th>\n",
              "      <th>('block2_pool1', 6)</th>\n",
              "      <th>('block2_pool1', 7)</th>\n",
              "      <th>('block2_pool1', 8)</th>\n",
              "      <th>('block2_pool1', 9)</th>\n",
              "      <th>('block2_pool1', 10)</th>\n",
              "      <th>('block2_pool1', 11)</th>\n",
              "      <th>('before_softmax', 0)</th>\n",
              "      <th>('before_softmax', 1)</th>\n",
              "      <th>('before_softmax', 2)</th>\n",
              "      <th>('before_softmax', 3)</th>\n",
              "      <th>('before_softmax', 4)</th>\n",
              "      <th>('before_softmax', 5)</th>\n",
              "      <th>('before_softmax', 6)</th>\n",
              "      <th>('before_softmax', 7)</th>\n",
              "      <th>('before_softmax', 8)</th>\n",
              "      <th>('before_softmax', 9)</th>\n",
              "      <th>('predictions', 0)</th>\n",
              "      <th>('predictions', 1)</th>\n",
              "      <th>('predictions', 2)</th>\n",
              "      <th>('predictions', 3)</th>\n",
              "      <th>('predictions', 4)</th>\n",
              "      <th>('predictions', 5)</th>\n",
              "      <th>('predictions', 6)</th>\n",
              "      <th>('predictions', 7)</th>\n",
              "      <th>('predictions', 8)</th>\n",
              "      <th>('predictions', 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_1</th>\n",
              "      <td>0.384</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.434</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.258</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.311</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.183</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.432</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.413</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.481</td>\n",
              "      <td>0.234</td>\n",
              "      <td>0.386</td>\n",
              "      <td>0.305</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_2</th>\n",
              "      <td>0.387</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.447</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.366</td>\n",
              "      <td>0.259</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.259</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.185</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.543</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.179</td>\n",
              "      <td>0.437</td>\n",
              "      <td>0.558</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.586</td>\n",
              "      <td>0.187</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_3</th>\n",
              "      <td>0.257</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.317</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.145</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.290</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.152</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.231</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.198</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.556</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.614</td>\n",
              "      <td>0.534</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_4</th>\n",
              "      <td>0.421</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.478</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.145</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.234</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.371</td>\n",
              "      <td>0.263</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.181</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.167</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.638</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.517</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.656</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.651</td>\n",
              "      <td>0.358</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_5</th>\n",
              "      <td>0.223</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.145</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.192</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.470</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.457</td>\n",
              "      <td>0.351</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.493</td>\n",
              "      <td>0.266</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_6</th>\n",
              "      <td>0.217</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.272</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.152</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.296</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.438</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.723</td>\n",
              "      <td>0.355</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ('block1_conv1', 0)  ...  ('predictions', 9)\n",
              "0_1                0.384  ...               0.000\n",
              "0_2                0.387  ...               0.000\n",
              "0_3                0.257  ...               0.000\n",
              "0_4                0.421  ...               0.000\n",
              "0_5                0.223  ...               0.000\n",
              "0_6                0.217  ...               0.000\n",
              "\n",
              "[6 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ1DCqKX9lTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_list = list(df2_scale.iloc[:,0])\n",
        "#df2_scale = df2_scale.rename(index=tmp_list)\n",
        "type(tmp_list)\n",
        "df2_scale.index = tmp_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiVj9Wjd-rVv",
        "colab_type": "text"
      },
      "source": [
        "以下が、df1_scaleに格納されているニューロンの出力値から発火/非発火の集計をとるアルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B19EsPFQrak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "f44c6a86-158e-4b73-f3bb-80bc2591cb41"
      },
      "source": [
        "import pylab as pl\n",
        "\n",
        "thres = 0.1\n",
        "bools = df2_scale.iloc[:,1:] > thres\n",
        "\n",
        "bools.sum().hist()\n",
        "pl.xlabel(\"activation time\")\n",
        "pl.ylabel(\"number of neurons\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'number of neurons')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUmEMfkkqzP8DFAE+H/IDHpoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGmSlrMm7g6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "outputId": "0059c14b-1521-4574-f2a5-881ec05a7a5c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#print(test.index[0],test.index[0][0])\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(12, 8))\n",
        "bools.sum().plot(kind=\"bar\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd0da63cba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsUAAAIwCAYAAABulBayAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPP1-qoLB_b-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "d021e7e1-4dff-480e-c546-420080c5348f"
      },
      "source": [
        "bools = df3_scale.iloc[:,1:4] > 0\n",
        "bools.sum().plot(kind=\"bar\")\n",
        "#bools.sum()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd0da067390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEpCAYAAACKmHkAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz77SyGDD2L2",
        "colab_type": "text"
      },
      "source": [
        "フィルタ処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z06piFAVDDOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0aea0ac9-d4b4-4429-9aa0-f1cc348c5c68"
      },
      "source": [
        "import glob\n",
        "img_list = glob.glob(output_dir + \"/l*].png\")\n",
        "print(img_list[0][79:80])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HqwPQNxDp_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3504836-d721-4ef6-a681-c85482a9e25a"
      },
      "source": [
        "from PIL import Image\n",
        "im = np.array(Image.open(img_list[0]))\n",
        "np.max(im), np.min(im)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(255, 87)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRueeE7hEUg4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "5e778aa8-a26a-4ff4-b1a6-8937fade3e13"
      },
      "source": [
        "im_dev =np.array([])\n",
        "im_bug = pd.DataFrame(columns=[\"fig\"])\n",
        "im_all = pd.DataFrame(columns=[\"fig\"])\n",
        "index = 0\n",
        "\n",
        "for i in img_list:\n",
        "  im = np.array(Image.open(i))  \n",
        "  tmp = np.max(im)-np.min(im)\n",
        "  im_dev = np.append(im_dev, tmp)\n",
        "  if tmp > 50:\n",
        "    im_bug.loc[str(index)] = i[79:80]\n",
        "  im_all.loc[str(index)] = i[79:80]\n",
        "  index += 1\n",
        "\n",
        "plt.hist(im_dev)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2., 0., 3., 3., 3., 2., 3., 5., 1., 1.]),\n",
              " array([  0. ,  21.2,  42.4,  63.6,  84.8, 106. , 127.2, 148.4, 169.6,\n",
              "        190.8, 212. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ9QAwwk1wHD/D4kiGxRVzkq3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUS1749kHv90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "6505158b-df4e-4279-bdf0-b9bb16c39ed5"
      },
      "source": [
        "im_all[\"fig\"].value_counts().sort_index().plot(kind=\"bar\")\n",
        "#im_bug[\"fig\"].value_counts().sort_index().plot(kind=\"bar\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd0d9f9d160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD1CAYAAAB5n7/BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuL8AkKnk+u2kEbcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ISWtgVLQTSQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "54ddc3ef-c15a-4441-9e37-d6dd059694ae"
      },
      "source": [
        "im_bug[\"fig\"].value_counts().sort_index().plot(kind=\"bar\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd0d9e79da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD1CAYAAAB5n7/BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60e_7IL1JM8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "9d429198-7cd0-4f13-94fb-e08bdc36eb57"
      },
      "source": [
        "im_all"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fig</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fig\n",
              "0    4\n",
              "1    4\n",
              "2    4\n",
              "3    5\n",
              "4    5\n",
              "5    6\n",
              "6    6\n",
              "7    6\n",
              "8    6\n",
              "9    6\n",
              "10   6\n",
              "11   6\n",
              "12   7\n",
              "13   7\n",
              "14   7\n",
              "15   7\n",
              "16   8\n",
              "17   8\n",
              "18   9\n",
              "19   9\n",
              "20   9\n",
              "21   9\n",
              "22   9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGHvvsEkJzXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fig = plt.figure()\n",
        "#ax = fig.add_subplot(1,1,1)\n",
        "#x1 = im_all[\"fig\"].value_counts()\n",
        "#x2 = im_bug[\"fig\"].value_counts()\n",
        "#ax.hist([x1, x2], bins=10, normed=True, color=['red', 'blue', 'green'], label=['x1', 'x2', 'x3'])\n",
        "#ax.set_title('seventh histogram $\\mu1=100,\\ \\sigma1=15,\\ \\mu2=50,\\ \\sigma2=4$')\n",
        "#ax.set_xlabel('x')\n",
        "#ax.set_ylabel('freq')\n",
        "#ax.legend(loc='upper left')\n",
        "#fig.show()\n",
        "#\n",
        "#x1, x2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxkrXvEuLOjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "32110f07-20dc-4a0c-8f8f-eb7b7b125c42"
      },
      "source": [
        "x1 = im_all[\"fig\"].value_counts()\n",
        "x1, type(x1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6    7\n",
              " 9    5\n",
              " 7    4\n",
              " 4    3\n",
              " 5    2\n",
              " 8    2\n",
              " Name: fig, dtype: int64, pandas.core.series.Series)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ7spEzhOy10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(x1, x2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK8sscq8LTWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "828e9a27-01e3-40e0-ed54-8439c5180680"
      },
      "source": [
        "x1.values"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 5, 4, 3, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcJirLEsbtHB",
        "colab_type": "text"
      },
      "source": [
        "以下は実際に各画像でどのくらい\"薄い\"画像があるかの分布"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAwybx1JLkn8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "677d53af-78a9-4231-fc9d-678804ed0667"
      },
      "source": [
        "im_dev =np.array([])\n",
        "im_bug = pd.DataFrame(columns=[\"filter\", \"org\"])\n",
        "index = 0\n",
        "\n",
        "for i in img_list:\n",
        "  im = np.array(Image.open(i))  \n",
        "  tmp = np.max(im)-np.min(im)\n",
        "  im_dev = np.append(im_dev, tmp)\n",
        "  if tmp > 50:\n",
        "    im_bug.loc[str(index)] = [i[79:80], i[79:80]]\n",
        "  else:\n",
        "    im_bug.loc[str(index)] = [None, i[79:80]]\n",
        "#  im_all.loc[str(index)] = i[79:80]\n",
        "  index += 1\n",
        "\n",
        "plt.hist(im_dev)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2., 0., 3., 3., 3., 2., 3., 5., 1., 1.]),\n",
              " array([  0. ,  21.2,  42.4,  63.6,  84.8, 106. , 127.2, 148.4, 169.6,\n",
              "        190.8, 212. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ9QAwwk1wHD/D4kiGxRVzkq3AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ_waU1DMtUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "outputId": "ce85fda7-e1c2-4b1e-f43a-4ce01fb446e9"
      },
      "source": [
        "im_bug"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filter</th>\n",
              "      <th>org</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>None</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   filter org\n",
              "0       4   4\n",
              "1       4   4\n",
              "2       4   4\n",
              "3       5   5\n",
              "4       5   5\n",
              "5       6   6\n",
              "6       6   6\n",
              "7       6   6\n",
              "8       6   6\n",
              "9       6   6\n",
              "10      6   6\n",
              "11      6   6\n",
              "12      7   7\n",
              "13      7   7\n",
              "14      7   7\n",
              "15      7   7\n",
              "16   None   8\n",
              "17   None   8\n",
              "18      9   9\n",
              "19      9   9\n",
              "20   None   9\n",
              "21      9   9\n",
              "22      9   9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwBNwplZM3UI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "1ddba2e5-e77a-49cd-fa67-5b1617082d50"
      },
      "source": [
        "print(im_bug[\"filter\"].value_counts().sort_index())\n",
        "print(im_bug[\"org\"].value_counts().sort_index())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4    3\n",
            "5    2\n",
            "6    7\n",
            "7    4\n",
            "9    4\n",
            "Name: filter, dtype: int64\n",
            "4    3\n",
            "5    2\n",
            "6    7\n",
            "7    4\n",
            "8    2\n",
            "9    5\n",
            "Name: org, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWbPrxZRNBZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "baddaaae-fa20-489f-cb35-ef9e4b891931"
      },
      "source": [
        "type(im_all[\"fig\"])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lncXwCAINPAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#im_bug[\"filter\"].value_counts().sort_index().merge(im_bug[\"org\"].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obbro2n3ONLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "outputId": "8edfad2a-cff8-4d06-917e-52b76c5a9168"
      },
      "source": [
        "df4_scale.head(20)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>('block1_conv1', 0)</th>\n",
              "      <th>('block1_conv1', 1)</th>\n",
              "      <th>('block1_conv1', 2)</th>\n",
              "      <th>('block1_conv1', 3)</th>\n",
              "      <th>('block1_pool1', 0)</th>\n",
              "      <th>('block1_pool1', 1)</th>\n",
              "      <th>('block1_pool1', 2)</th>\n",
              "      <th>('block1_pool1', 3)</th>\n",
              "      <th>('block2_conv1', 0)</th>\n",
              "      <th>('block2_conv1', 1)</th>\n",
              "      <th>('block2_conv1', 2)</th>\n",
              "      <th>('block2_conv1', 3)</th>\n",
              "      <th>('block2_conv1', 4)</th>\n",
              "      <th>('block2_conv1', 5)</th>\n",
              "      <th>('block2_conv1', 6)</th>\n",
              "      <th>('block2_conv1', 7)</th>\n",
              "      <th>('block2_conv1', 8)</th>\n",
              "      <th>('block2_conv1', 9)</th>\n",
              "      <th>('block2_conv1', 10)</th>\n",
              "      <th>('block2_conv1', 11)</th>\n",
              "      <th>('block2_pool1', 0)</th>\n",
              "      <th>('block2_pool1', 1)</th>\n",
              "      <th>('block2_pool1', 2)</th>\n",
              "      <th>('block2_pool1', 3)</th>\n",
              "      <th>('block2_pool1', 4)</th>\n",
              "      <th>('block2_pool1', 5)</th>\n",
              "      <th>('block2_pool1', 6)</th>\n",
              "      <th>('block2_pool1', 7)</th>\n",
              "      <th>('block2_pool1', 8)</th>\n",
              "      <th>('block2_pool1', 9)</th>\n",
              "      <th>('block2_pool1', 10)</th>\n",
              "      <th>('block2_pool1', 11)</th>\n",
              "      <th>('before_softmax', 0)</th>\n",
              "      <th>('before_softmax', 1)</th>\n",
              "      <th>('before_softmax', 2)</th>\n",
              "      <th>('before_softmax', 3)</th>\n",
              "      <th>('before_softmax', 4)</th>\n",
              "      <th>('before_softmax', 5)</th>\n",
              "      <th>('before_softmax', 6)</th>\n",
              "      <th>('before_softmax', 7)</th>\n",
              "      <th>('before_softmax', 8)</th>\n",
              "      <th>('before_softmax', 9)</th>\n",
              "      <th>('predictions', 0)</th>\n",
              "      <th>('predictions', 1)</th>\n",
              "      <th>('predictions', 2)</th>\n",
              "      <th>('predictions', 3)</th>\n",
              "      <th>('predictions', 4)</th>\n",
              "      <th>('predictions', 5)</th>\n",
              "      <th>('predictions', 6)</th>\n",
              "      <th>('predictions', 7)</th>\n",
              "      <th>('predictions', 8)</th>\n",
              "      <th>('predictions', 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_1_0</th>\n",
              "      <td>0_1_0</td>\n",
              "      <td>0.307</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.361</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.198</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.361</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.502</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.360</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_1</th>\n",
              "      <td>0_1_1</td>\n",
              "      <td>0.323</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.313</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.178</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.261</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.195</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.427</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.370</td>\n",
              "      <td>0.371</td>\n",
              "      <td>0.498</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.343</td>\n",
              "      <td>0.349</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_2</th>\n",
              "      <td>0_1_2</td>\n",
              "      <td>0.342</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.394</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.185</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.192</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.366</td>\n",
              "      <td>0.494</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.337</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_3</th>\n",
              "      <td>0_1_3</td>\n",
              "      <td>0.371</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.245</td>\n",
              "      <td>0.423</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.343</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.199</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.263</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.187</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.398</td>\n",
              "      <td>0.357</td>\n",
              "      <td>0.485</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.317</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_4</th>\n",
              "      <td>0_1_4</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.308</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_5</th>\n",
              "      <td>0_1_5</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.312</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_6</th>\n",
              "      <td>0_1_6</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.282</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.403</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.483</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.377</td>\n",
              "      <td>0.313</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_7</th>\n",
              "      <td>0_1_7</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.185</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.308</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_8</th>\n",
              "      <td>0_1_8</td>\n",
              "      <td>0.382</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.269</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.407</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.481</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.310</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_9</th>\n",
              "      <td>0_1_9</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.284</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.311</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_10</th>\n",
              "      <td>0_1_10</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.404</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.483</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.377</td>\n",
              "      <td>0.313</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_11</th>\n",
              "      <td>0_1_11</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.185</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.308</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_12</th>\n",
              "      <td>0_1_12</td>\n",
              "      <td>0.382</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.269</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.407</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.481</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.310</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_13</th>\n",
              "      <td>0_1_13</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.284</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.311</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_14</th>\n",
              "      <td>0_1_14</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.404</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.483</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.377</td>\n",
              "      <td>0.313</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_15</th>\n",
              "      <td>0_1_15</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.185</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.308</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_16</th>\n",
              "      <td>0_1_16</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.433</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.204</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.269</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.407</td>\n",
              "      <td>0.354</td>\n",
              "      <td>0.481</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.310</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_17</th>\n",
              "      <td>0_1_17</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.431</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.284</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.348</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.482</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.311</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_18</th>\n",
              "      <td>0_1_18</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.429</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.267</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.186</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.404</td>\n",
              "      <td>0.355</td>\n",
              "      <td>0.483</td>\n",
              "      <td>0.241</td>\n",
              "      <td>0.377</td>\n",
              "      <td>0.312</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_19</th>\n",
              "      <td>0_1_19</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.254</td>\n",
              "      <td>0.436</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.287</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.185</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.432</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.409</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.480</td>\n",
              "      <td>0.236</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.308</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  ('block1_conv1', 0)  ...  ('predictions', 8)  ('predictions', 9)\n",
              "0_1_0       0_1_0                0.307  ...               0.000               0.000\n",
              "0_1_1       0_1_1                0.323  ...               0.000               0.000\n",
              "0_1_2       0_1_2                0.342  ...               0.000               0.000\n",
              "0_1_3       0_1_3                0.371  ...               0.000               0.000\n",
              "0_1_4       0_1_4                0.385  ...               0.000               0.000\n",
              "0_1_5       0_1_5                0.380  ...               0.000               0.000\n",
              "0_1_6       0_1_6                0.378  ...               0.000               0.000\n",
              "0_1_7       0_1_7                0.385  ...               0.000               0.000\n",
              "0_1_8       0_1_8                0.382  ...               0.000               0.000\n",
              "0_1_9       0_1_9                0.380  ...               0.000               0.000\n",
              "0_1_10     0_1_10                0.378  ...               0.000               0.000\n",
              "0_1_11     0_1_11                0.385  ...               0.000               0.000\n",
              "0_1_12     0_1_12                0.382  ...               0.000               0.000\n",
              "0_1_13     0_1_13                0.380  ...               0.000               0.000\n",
              "0_1_14     0_1_14                0.378  ...               0.000               0.000\n",
              "0_1_15     0_1_15                0.385  ...               0.000               0.000\n",
              "0_1_16     0_1_16                0.383  ...               0.000               0.000\n",
              "0_1_17     0_1_17                0.380  ...               0.000               0.000\n",
              "0_1_18     0_1_18                0.378  ...               0.000               0.000\n",
              "0_1_19     0_1_19                0.385  ...               0.000               0.000\n",
              "\n",
              "[20 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5HVngqRZ4GY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}