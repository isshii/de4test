{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "DeepXplore_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshii/de4test/blob/test_ozawa/DeepXplore/DeepXplore_03_12_coverage_trace_befiore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVFjZpUIRF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 共通で使うパスなどの定義\n",
        "# 共通の変数設定\n",
        "# 共通フォルダパス\n",
        "\n",
        "import datetime\n",
        "import pytz\n",
        "dt_now = datetime.datetime.now(pytz.timezone('Asia/Tokyo'))\n",
        "dt_str = str(dt_now.strftime('%Y%m%d_%H%M'))\n",
        "\n",
        "data_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data\"\n",
        "#data_imagenet = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet\"\n",
        "data_mnist = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/MNIST\"\n",
        "#data_imagenet_seeds = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet/seeds\"\n",
        "model_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/model\"\n",
        "#output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output\"\n",
        "output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output/\" + dt_str\n",
        "tmp_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/tmp\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hv53U_OdAFYb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4lP4POGn5Xj",
        "colab_type": "code",
        "outputId": "74e3195c-bbec-4250-86ea-1e0755a317a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "\n",
        "# Goggle Drive つなぐ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3d3Suv3Q5Db",
        "colab_type": "code",
        "outputId": "875422ef-43cf-488c-e6f6-80108e7ee0f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# outputフォルダ内容物のクリーンアップ削除\n",
        "!mkdir \"$output_dir\"\n",
        "!rm \"$output_dir\"/*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200207_1836/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdHYnLT0Nbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST')\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDY8sDVBqxmK",
        "colab_type": "code",
        "outputId": "b2331cbe-59d8-4a90-f23f-7fc4983e5ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input\n",
        "import imageio\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "import os\n",
        "\n",
        "# TensorFlowでGPUを使っているかのチェック：\n",
        "# \"device_type: \"GPU\" \" があればOK\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 8349765792137929528, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 2424177915956619006\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 15715121075122073320\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15956161332\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 2476737475755100681\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQ2M_8glJNM",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "e2e7bbe5-cc62-48c2-f191-18e5ea15de9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "# DeepXplore のパラメータ設定部\n",
        "# read the parameter\n",
        "# argument parsing\n",
        "parser = argparse.ArgumentParser(description='Main function for difference-inducing input generation in MNIST dataset')\n",
        "parser.add_argument('transformation', help=\"realistic transformation type\", choices=['light', 'occl', 'blackout'])\n",
        "parser.add_argument('weight_diff', help=\"weight hyperparm to control differential behavior\", type=float)\n",
        "parser.add_argument('weight_nc', help=\"weight hyperparm to control neuron coverage\", type=float)\n",
        "parser.add_argument('step', help=\"step size of gradient descent\", type=float)\n",
        "parser.add_argument('seeds', help=\"number of seeds of input\", type=int)\n",
        "parser.add_argument('grad_iterations', help=\"number of iterations of gradient descent\", type=int)\n",
        "parser.add_argument('threshold', help=\"threshold for determining neuron activated\", type=float)\n",
        "parser.add_argument('-t', '--target_model', help=\"target model that we want it predicts differently\",\n",
        "                    choices=[0, 1, 2], default=0, type=int)\n",
        "parser.add_argument('-sp', '--start_point', help=\"occlusion upper left corner coordinate\", default=(0, 0), type=tuple)\n",
        "parser.add_argument('-occl_size', '--occlusion_size', help=\"occlusion size\", default=(10, 10), type=tuple)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-occl_size', '--occlusion_size'], dest='occlusion_size', nargs=None, const=None, default=(10, 10), type=<class 'tuple'>, choices=None, help='occlusion size', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93KW-VGjMzun",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title ノイズのタイプ light / occl / blackout\n",
        "#@body {light,occl,blackout} weight_diff weight_nc step seeds\n",
        "#                   grad_iterations threshold\n",
        "noise_type = \"light\" #@param [\"light\", \"occl\", \"blackout\"]\n",
        "weight_diff = \"0.1\" #@param {type:\"string\"}\n",
        "weight_nc = \"0.1\" #@param {type:\"string\"}\n",
        "step = \"20\" #@param {type:\"string\"}　#50だとほぼ白飛び\n",
        "seeds = \"5\" #@param {type:\"string\"}\n",
        "grad_iterations = \"10\" #@param {type:\"string\"}\n",
        "threshold = \"0.1\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk80fCuPts9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = parser.parse_args([noise_type, weight_diff, weight_nc, step, seeds, grad_iterations, threshold])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tTYrllplJNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x.reshape(x.shape[1], x.shape[2])  # original shape (1,img_rows, img_cols,1)\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "def constraint_occl(gradients, start_point, rect_shape):\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def constraint_light(gradients):\n",
        "    new_grads = np.ones_like(gradients)\n",
        "    grad_mean = np.mean(gradients)\n",
        "    return grad_mean * new_grads\n",
        "\n",
        "\n",
        "def constraint_black(gradients, rect_shape=(6, 6)):\n",
        "    start_point = (\n",
        "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    if np.mean(patch) < 0:\n",
        "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def init_coverage_tables(model1, model2, model3):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    model_layer_dict2 = defaultdict(bool)\n",
        "    model_layer_dict3 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    init_dict(model2, model_layer_dict2)\n",
        "    init_dict(model3, model_layer_dict3)\n",
        "    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n",
        "\n",
        "\n",
        "def init_dict(model, model_layer_dict):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_dict[(layer.name, index)] = False\n",
        "\n",
        "\n",
        "def neuron_to_cover(model_layer_dict):\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "    else:\n",
        "        layer_name, index = random.choice(model_layer_dict.keys())\n",
        "    return layer_name, index\n",
        "\n",
        "\n",
        "def neuron_covered(model_layer_dict):\n",
        "    covered_neurons = len([v for v in model_layer_dict.values() if v])\n",
        "    total_neurons = len(model_layer_dict)\n",
        "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n",
        "\n",
        "\n",
        "def update_coverage(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n",
        "                model_layer_dict[(layer_names[i], num_neuron)] = True\n",
        "\n",
        "\n",
        "def full_coverage(model_layer_dict):\n",
        "    if False in model_layer_dict.values():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
        "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
        "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
        "    X_scaled = X_std * (rmax - rmin) + rmin\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def fired(model, layer_name, index, input_data, threshold=0):\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
        "    scaled = scale(intermediate_layer_output)\n",
        "    if np.mean(scaled[..., index]) > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def diverged(predictions1, predictions2, predictions3, target):\n",
        "    #     if predictions2 == predictions3 == target and predictions1 != target:\n",
        "    if not predictions1 == predictions2 == predictions3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "#add\n",
        "def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n",
        "    return [os.path.join(root, f)\n",
        "            for root, _, files in os.walk(directory) for f in files\n",
        "            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDDT47Er6u23",
        "colab_type": "code",
        "outputId": "b78c4d14-3727-403a-a3e6-6282376552ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        }
      },
      "source": [
        "'''\n",
        "LeNet-1\n",
        "'''\n",
        "def Model1(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        print(x_train.shape)\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(4, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(12, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "#        model.save_weights('./Model1.h5')\n",
        "        model.save_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "#        model.load_weights('./Model1.h5')\n",
        "        model.load_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        print(bcolors.OKBLUE + 'Model1 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model1(train=True)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "(60000, 28, 28)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 14s 227us/step - loss: 0.4228 - acc: 0.8738 - val_loss: 0.1506 - val_acc: 0.9555\n",
            "\n",
            "\n",
            "Overall Test score: 0.1506246224537492\n",
            "Overall Test accuracy: 0.9555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU88awZf68JO",
        "colab_type": "code",
        "outputId": "eabb8ff0-089a-42eb-dc43-e60a1a033a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "'''\n",
        "LeNet-4\n",
        "'''\n",
        "def Model2(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(84, activation='relu', name='fc1')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model2.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model2.h5')\n",
        "        print(bcolors.OKBLUE + 'Model2 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model2(train=True)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.3781 - acc: 0.8809 - val_loss: 0.0934 - val_acc: 0.9723\n",
            "\n",
            "\n",
            "Overall Test score: 0.09335203053243459\n",
            "Overall Test accuracy: 0.9723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufs67uRN7KYL",
        "colab_type": "code",
        "outputId": "544881f7-448d-49a7-989a-b50f53593604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "'''\n",
        "LeNet-5\n",
        "'''\n",
        "def Model3(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(120, activation='relu', name='fc1')(x)\n",
        "    x = Dense(84, activation='relu', name='fc2')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model3.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model3.h5')\n",
        "        print(bcolors.OKBLUE + 'Model3 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model3(train=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.3809 - acc: 0.8781 - val_loss: 0.0830 - val_acc: 0.9726\n",
            "\n",
            "\n",
            "Overall Test score: 0.08298780426643788\n",
            "Overall Test accuracy: 0.9726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9QiwlW7WWR",
        "colab_type": "code",
        "outputId": "f783fb89-2fe4-43c8-8a1c-a7a398fd4711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# input MNIST image dimensions \n",
        "img_rows, img_cols = 28, 28\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "#define the model instance\n",
        "model1 = Model1(input_tensor=input_tensor)\n",
        "model2 = Model2(input_tensor=input_tensor)\n",
        "model3 = Model3(input_tensor=input_tensor)\n",
        "\n",
        "#define the dictionary of neuron coverage\n",
        "model_layer_dict1, model_layer_dict2, model_layer_dict3 = init_coverage_tables(model1, model2, model3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[94mModel1 loaded\u001b[0m\n",
            "\u001b[94mModel2 loaded\u001b[0m\n",
            "\u001b[94mModel3 loaded\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MS5H74Q4XA",
        "colab_type": "text"
      },
      "source": [
        "### 入力するデータの選別(1/2)\n",
        "\n",
        "０～９の数字をそれぞれ束ねてデータを準備する\n",
        "[00...011...1......99...9]。\n",
        "\n",
        "* test_per_fig_x: \n",
        "    各数字に対する画像データを格納\n",
        "* test_per_fig_y: \n",
        "    各数字に対する教師ラベルを格納\n",
        "* tests_x: \n",
        "    test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: \n",
        "    tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brFIdSfL8nHP",
        "colab_type": "code",
        "outputId": "79aa6eb3-8e89-4cfe-8d51-3382ce7e9db1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# load the MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "num_fig = 10\n",
        "test_per_fig_x = np.array([])\n",
        "test_per_fig_y = np.array([])\n",
        "tests_x = np.array([])\n",
        "tests_y = np.array([])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "#x_train = x_train.astype('float')\n",
        "#x_test = x_test.astype('float')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "length = int(args.seeds)\n",
        "#length = int(args.seeds)\n",
        "for i in range(num_fig):\n",
        "  cond = [(x==i) for x in y_test]\n",
        "  test_per_fig_x = x_test[cond]\n",
        "  test_per_fig_y = y_test[cond]\n",
        "#  np.set_printoptions(formatter={'int': '{:07d}'.format})\n",
        "  print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "  tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "  tests_y = np.append(tests_y, test_per_fig_y[:length])\n",
        "#  conds = [conds, cond]\n",
        "print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "\n",
        "###### MNIST data, shuffled and split by train and test sets\n",
        "#####(_, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "###### modify the numpy data for the Keras model\n",
        "#####x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "#####input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "#tests_x.shape\n",
        "tests_x = tests_x.astype('float32')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure: 0 , shape: (980, 28, 28) , deviation: -20\n",
            "figure: 1 , shape: (1135, 28, 28) , deviation: 135\n",
            "figure: 2 , shape: (1032, 28, 28) , deviation: 32\n",
            "figure: 3 , shape: (1010, 28, 28) , deviation: 10\n",
            "figure: 4 , shape: (982, 28, 28) , deviation: -18\n",
            "figure: 5 , shape: (892, 28, 28) , deviation: -108\n",
            "figure: 6 , shape: (958, 28, 28) , deviation: -42\n",
            "figure: 7 , shape: (1028, 28, 28) , deviation: 28\n",
            "figure: 8 , shape: (974, 28, 28) , deviation: -26\n",
            "figure: 9 , shape: (1009, 28, 28) , deviation: 9\n",
            "check! 5.0 = 5 equal?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUOK_EjJW2KT",
        "colab_type": "text"
      },
      "source": [
        "## 入力するデータの選別(2/2)\n",
        "０～９の数字をそれぞれ束ねてデータを準備する [00...000]。\n",
        "\n",
        "* test_per_fig_x: 各数字に対する画像データを格納\n",
        "* test_per_fig_y: 各数字に対する教師ラベルを格納\n",
        "* tests_x: test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "id": "SLee__9UAD1Y",
        "colab": {}
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "#load the MNIST \n",
        "\n",
        "def create_data(start_fig, num_fi, length):\n",
        "  (datax_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  test_per_fig_x = np.array([])\n",
        "  test_per_fig_y = np.array([])\n",
        "  tests_x = np.array([])\n",
        "  tests_y = np.array([])\n",
        "\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test  = x_test.astype('float32')\n",
        "  x_train = x_train.astype('float')\n",
        "  x_test = x_test.astype('float')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  #length = int(args.seeds/num_fig)\n",
        "  #length = int(args.seeds)\n",
        "  for i in range(start_fig, start_fig+num_fig):\n",
        "  #i=0\n",
        "    cond = [(x==i) for x in y_test]\n",
        "    test_per_fig_x = x_test[cond]\n",
        "    test_per_fig_y = y_test[cond]\n",
        "    print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "    tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "    tests_y = np.append(tests_y, test_per_fig_y[:length])  \n",
        "\n",
        "  #conds = [conds, cond]\n",
        "  print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "  tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "  tests_x.shape\n",
        "  tests_x = tests_x.astype('float32')\n",
        "  return tests_x, tests_y, length\n",
        "  tests_x, tests_y, length = create_data(0, 10, int(args.seeds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyo2SSSz8vkT",
        "colab_type": "code",
        "outputId": "5aa986be-fc2d-4cab-bf7b-a1aa1b89ccc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "print(tests_x.shape)\n",
        "print(tests_y)\n",
        "for i in range(0,1):\n",
        "  print(i)\n",
        "print(length)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 28, 28, 1)\n",
            "[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 4. 4. 4. 4.\n",
            " 4. 5. 5. 5. 5. 5. 6. 6. 6. 6. 6. 7. 7. 7. 7. 7. 8. 8. 8. 8. 8. 9. 9. 9.\n",
            " 9. 9.]\n",
            "0\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLlKIMxG9kEA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfdTZi4mUPf",
        "colab_type": "code",
        "outputId": "b64aaa77-58fe-471b-d298-b0dc5b743da5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "def neuron_output(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "    \n",
        "    out_list = []\n",
        "    out_list_scale = []\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            out_list.append(np.mean(intermediate_layer_output[..., num_neuron]))\n",
        "            out_list_scale.append(np.mean(scaled[..., num_neuron]))\n",
        "\n",
        "    return out_list, out_list_scale\n",
        "\n",
        "\n",
        "def deepXplore(model_layer_dict1, model_layer_dict2, model_layer_dict3, tests_x, model1, model2, model3, num_fig, start_fig, length):\n",
        "  print(\"test01\")\n",
        "  count_already = 0\n",
        "  count_found = 0\n",
        "  count_not_found = 0\n",
        "  temp_per_nc1 = np.array([])\n",
        "  temp_per_nc2 = np.array([])\n",
        "  temp_per_nc3 = np.array([])\n",
        "  temp_num_nc1 = np.array([])\n",
        "  temp_num_nc2 = np.array([])\n",
        "  temp_num_nc3 = np.array([])\n",
        "  print(\"test10\")\n",
        "\n",
        "  #for each neuron\n",
        "  num_neurons1 = neuron_covered(model_layer_dict1)[1]\n",
        "  num_neurons2 = neuron_covered(model_layer_dict2)[1]\n",
        "  num_neurons3 = neuron_covered(model_layer_dict3)[1]\n",
        "  print(\"test11\")\n",
        "\n",
        "  column_tmp1 = list(model_layer_dict1.keys())\n",
        "  column_tmp2 = list(model_layer_dict2.keys())\n",
        "  column_tmp3 = list(model_layer_dict3.keys())\n",
        "  print(\"test12\")\n",
        "  df1 = pd.DataFrame(columns=column_tmp1)\n",
        "  df2 = pd.DataFrame(columns=column_tmp2)\n",
        "  df3 = pd.DataFrame(columns=column_tmp3)\n",
        "  df1_scale = pd.DataFrame(columns=column_tmp1)\n",
        "  df2_scale = pd.DataFrame(columns=column_tmp2)\n",
        "  df3_scale = pd.DataFrame(columns=column_tmp3)\n",
        "  df1_trace = pd.DataFrame(columns=column_tmp1)\n",
        "  df2 = pd.DataFrame(columns=column_tmp2)\n",
        "  df3 = pd.DataFrame(columns=column_tmp3)\n",
        "  tmp_list = [\"already_diff\", \"found\", \"not_found\", \"layer1\", \"index1\", \"layer2\", \"index2\", \"layer3\", \"index3\"]\n",
        "  bug_result = pd.DataFrame(columns=tmp_list)\n",
        "  trial = 1\n",
        "  #print(\"test13\")\n",
        "  print(\"test02\")\n",
        "  for index_fig in range(num_fig):\n",
        "    index_fig = index_fig + start_fig\n",
        "    print(\"figure\"+str(index_fig))\n",
        "    for _ in range(length):\n",
        "      #gen_img = np.expand_dims(random.choice(tests_x), axis=0)\n",
        "      gen_img = np.expand_dims(tests_x[(length*index_fig + _)], axis=0)\n",
        "      orig_img = gen_img.copy()\n",
        "      # first check if input already induces differences\n",
        "      label1, label2, label3 = np.argmax(model1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "      if not label1 == label2 == label3:\n",
        "          count_already += 1\n",
        "          print(bcolors.OKGREEN + '   {}/{}. input already causes different outputs ({},{},{}) at({}, {}, {}): '.format(_, length, label1, label2, label3, count_already, count_found, count_not_found) + bcolors.ENDC)        \n",
        "\n",
        "          update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "          temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "          temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "          temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "          temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "          temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "          temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "                               \n",
        "          print(bcolors.OKGREEN + '     covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'% (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  neuron_covered(model_layer_dict2)[2], len(model_layer_dict3), neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "          averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                       neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +neuron_covered(model_layer_dict3)[1])\n",
        "          print(bcolors.OKGREEN + '     averaged covered neurons %.3f' % (averaged_nc) + bcolors.ENDC)\n",
        "\n",
        "          gen_img_deprocessed = deprocess_image(gen_img)\n",
        "\n",
        "          # save the result to disk\n",
        "          outputfilepath0 = os.path.join(output_dir, 'already_differ_' + str(label1) + '_' + str(label2) + '_' + str(label3) +'_['+ str(_) +  '].png')\n",
        "          imageio.imwrite(outputfilepath0, gen_img_deprocessed)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df1 = df1.append(temp)\n",
        "          df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df2 = df2.append(temp)\n",
        "          df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "          temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df3 = df3.append(temp)\n",
        "          df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "          #print(\"test10\")\n",
        "          temp = [1, 0, 0, None, None, None, None, None, None]\n",
        "          #print(\"test11\")\n",
        "          temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          #print(\"test12\")\n",
        "          bug_result = bug_result.append(temp)\n",
        "          #print(\"test13\")\n",
        "          trial += 1\n",
        "          continue\n",
        "\n",
        "      # if all label agrees\n",
        "      orig_label = label1\n",
        "      layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n",
        "      layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n",
        "      layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n",
        "\n",
        "      # construct joint loss function\n",
        "      if args.target_model == 0:\n",
        "        loss1 = -args.weight_diff * K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 1:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = -args.weight_diff * K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 2:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n",
        "      loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n",
        "      loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n",
        "      layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron)\n",
        "\n",
        "      # for adversarial image generation\n",
        "      final_loss = K.mean(layer_output)\n",
        "\n",
        "      # we compute the gradient of the input picture wrt this loss\n",
        "      grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "\n",
        "      # this function returns the loss and grads given the input picture\n",
        "      iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n",
        "\n",
        "      print(\"test03\")\n",
        "      # we run gradient ascent for some steps\n",
        "      for iters in range(args.grad_iterations):\n",
        "        loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = iterate([gen_img])\n",
        "        if args.transformation == 'light':\n",
        "            grads_value = constraint_light(grads_value)  # constraint the gradients value\n",
        "        elif args.transformation == 'occl':\n",
        "            grads_value = constraint_occl(grads_value, args.start_point,\n",
        "                                          args.occlusion_size)  # constraint the gradients value\n",
        "        elif args.transformation == 'blackout':\n",
        "            grads_value = constraint_black(grads_value)  # constraint the gradients value\n",
        "\n",
        "        gen_img += grads_value * args.step\n",
        "        predictions1 = np.argmax(model1.predict(gen_img)[0])\n",
        "        predictions2 = np.argmax(model2.predict(gen_img)[0])\n",
        "        predictions3 = np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "        print(\"test04\")\n",
        "        if not predictions1 == predictions2 == predictions3:\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            count_found += 1\n",
        "            print(bcolors.OKBLUE + '%4d/%d. found at %d! covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f at (%d, %d, %d)'\n",
        "#                  % (_, args.seeds, iters, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  % (_, length, iters + 1, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                     neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                     neuron_covered(model_layer_dict3)[2], count_already, count_found, count_not_found) + bcolors.ENDC)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            print(bcolors.OKBLUE + '     averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  '].png')\n",
        "            #print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "#            outputfilepath2 = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '_orig.png')\n",
        "            outputfilepath2 = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  ']_orig.png')\n",
        "            #print(outputfilepath2)\n",
        "            imageio.imwrite(outputfilepath2, orig_img_deprocessed)\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "            temp = [0, iters+1, 0, layer_name1, index1, layer_name2, index2,layer_name3, index3]\n",
        "            temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            bug_result = bug_result.append(temp)\n",
        "            trial += 1\n",
        "            break\n",
        "          \n",
        "          #add\n",
        "        print(\"test05\")\n",
        "        if iters == (args.grad_iterations-1):\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            count_not_found += 1\n",
        "#              print('%4d/%d. test suite was not found: averaged covered neurons %.3f at %d/%d' % (_, args.seeds, averaged_nc, count_not_found, count_already + count_found + count_not_found))\n",
        "            print('%4d/%d. test suite was not found: averaged covered neurons %.3f at (%d, %d, %d)' % (_, length, averaged_nc, count_already, count_found, count_not_found))\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            #orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, 'not_found_' + str(label1)+'_['+ str(_) + '].png')\n",
        "#           print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "\n",
        "            \n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "\n",
        "            temp = [0, 0, 1, layer_name1, index1, layer_name2, index2,layer_name3, index3]\n",
        "            temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            bug_result = bug_result.append(temp)\n",
        "\n",
        "            trial += 1\n",
        "            \n",
        "            #break\n",
        "\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "  print(\"test06\")\n",
        "  temp_per_nc1=temp_per_nc1.reshape(num_fig, length)\n",
        "  temp_per_nc2=temp_per_nc2.reshape(num_fig, length)\n",
        "  temp_per_nc3=temp_per_nc3.reshape(num_fig, length)\n",
        "  temp_num_nc1=temp_num_nc1.reshape(num_fig, length)\n",
        "  temp_num_nc2=temp_num_nc2.reshape(num_fig, length)\n",
        "  temp_num_nc3=temp_num_nc3.reshape(num_fig, length)\n",
        "\n",
        "  print(\"test07\")\n",
        "  return df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
            "Wall time: 7.63 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHi1ffJuWH5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "fabe4a2c-19b4-442f-c1d0-6040566da45b"
      },
      "source": [
        "print(length, num_fig)\n",
        "print(output_dir)\n",
        "column_tmp1 = list(model_layer_dict1.keys())\n",
        "df1 = pd.DataFrame(columns=column_tmp1)\n",
        "df1.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 10\n",
            "/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200207_1836\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(block1_conv1, 0)</th>\n",
              "      <th>(block1_conv1, 1)</th>\n",
              "      <th>(block1_conv1, 2)</th>\n",
              "      <th>(block1_conv1, 3)</th>\n",
              "      <th>(block1_pool1, 0)</th>\n",
              "      <th>(block1_pool1, 1)</th>\n",
              "      <th>(block1_pool1, 2)</th>\n",
              "      <th>(block1_pool1, 3)</th>\n",
              "      <th>(block2_conv1, 0)</th>\n",
              "      <th>(block2_conv1, 1)</th>\n",
              "      <th>(block2_conv1, 2)</th>\n",
              "      <th>(block2_conv1, 3)</th>\n",
              "      <th>(block2_conv1, 4)</th>\n",
              "      <th>(block2_conv1, 5)</th>\n",
              "      <th>(block2_conv1, 6)</th>\n",
              "      <th>(block2_conv1, 7)</th>\n",
              "      <th>(block2_conv1, 8)</th>\n",
              "      <th>(block2_conv1, 9)</th>\n",
              "      <th>(block2_conv1, 10)</th>\n",
              "      <th>(block2_conv1, 11)</th>\n",
              "      <th>(block2_pool1, 0)</th>\n",
              "      <th>(block2_pool1, 1)</th>\n",
              "      <th>(block2_pool1, 2)</th>\n",
              "      <th>(block2_pool1, 3)</th>\n",
              "      <th>(block2_pool1, 4)</th>\n",
              "      <th>(block2_pool1, 5)</th>\n",
              "      <th>(block2_pool1, 6)</th>\n",
              "      <th>(block2_pool1, 7)</th>\n",
              "      <th>(block2_pool1, 8)</th>\n",
              "      <th>(block2_pool1, 9)</th>\n",
              "      <th>(block2_pool1, 10)</th>\n",
              "      <th>(block2_pool1, 11)</th>\n",
              "      <th>(before_softmax, 0)</th>\n",
              "      <th>(before_softmax, 1)</th>\n",
              "      <th>(before_softmax, 2)</th>\n",
              "      <th>(before_softmax, 3)</th>\n",
              "      <th>(before_softmax, 4)</th>\n",
              "      <th>(before_softmax, 5)</th>\n",
              "      <th>(before_softmax, 6)</th>\n",
              "      <th>(before_softmax, 7)</th>\n",
              "      <th>(before_softmax, 8)</th>\n",
              "      <th>(before_softmax, 9)</th>\n",
              "      <th>(predictions, 0)</th>\n",
              "      <th>(predictions, 1)</th>\n",
              "      <th>(predictions, 2)</th>\n",
              "      <th>(predictions, 3)</th>\n",
              "      <th>(predictions, 4)</th>\n",
              "      <th>(predictions, 5)</th>\n",
              "      <th>(predictions, 6)</th>\n",
              "      <th>(predictions, 7)</th>\n",
              "      <th>(predictions, 8)</th>\n",
              "      <th>(predictions, 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [(block1_conv1, 0), (block1_conv1, 1), (block1_conv1, 2), (block1_conv1, 3), (block1_pool1, 0), (block1_pool1, 1), (block1_pool1, 2), (block1_pool1, 3), (block2_conv1, 0), (block2_conv1, 1), (block2_conv1, 2), (block2_conv1, 3), (block2_conv1, 4), (block2_conv1, 5), (block2_conv1, 6), (block2_conv1, 7), (block2_conv1, 8), (block2_conv1, 9), (block2_conv1, 10), (block2_conv1, 11), (block2_pool1, 0), (block2_pool1, 1), (block2_pool1, 2), (block2_pool1, 3), (block2_pool1, 4), (block2_pool1, 5), (block2_pool1, 6), (block2_pool1, 7), (block2_pool1, 8), (block2_pool1, 9), (block2_pool1, 10), (block2_pool1, 11), (before_softmax, 0), (before_softmax, 1), (before_softmax, 2), (before_softmax, 3), (before_softmax, 4), (before_softmax, 5), (before_softmax, 6), (before_softmax, 7), (before_softmax, 8), (before_softmax, 9), (predictions, 0), (predictions, 1), (predictions, 2), (predictions, 3), (predictions, 4), (predictions, 5), (predictions, 6), (predictions, 7), (predictions, 8), (predictions, 9)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSSBfoAZreWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0aa0a2a9-700a-4dee-8bb9-825cd7e1d016"
      },
      "source": [
        "index_fig = 0\n",
        "trial = 0\n",
        "gen_img = np.expand_dims(tests_x[(length*index_fig + 0)], axis=0)\n",
        "temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "print(type(temp_scale))\n",
        "temp_scale.name"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0_0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yymgs3ldNiDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2dd032a3-cf46-4e70-96d5-ab502352898e"
      },
      "source": [
        "%%time\n",
        "#num_fig = 1\n",
        "#start_fig = 0\n",
        "#deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "\n",
        "num_fig = 1\n",
        "for start_fig in range(10):\n",
        "#for start_fig in range(2):\n",
        "#  df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result, df1_trace, df2_trace. df3_trace= deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "  df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result= deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "  df1_scale.to_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")\n",
        "  bug_result.to_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_index.csv\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test01\n",
            "test10\n",
            "test11\n",
            "test12\n",
            "test02\n",
            "figure0\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   0/5. found at 4! covered neurons percentage 52 neurons 0.500, 148 neurons 0.473, 268 neurons 0.522 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.504\u001b[0m\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.519, 148 neurons 0.486, 268 neurons 0.534 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.517\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.519, 148 neurons 0.561, 268 neurons 0.556 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.553\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   3/5. found at 3! covered neurons percentage 52 neurons 0.519, 148 neurons 0.561, 268 neurons 0.556 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.553\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   4/5. found at 3! covered neurons percentage 52 neurons 0.519, 148 neurons 0.568, 268 neurons 0.563 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.560\u001b[0m\n",
            "test06\n",
            "test07\n",
            "test01\n",
            "test10\n",
            "test11\n",
            "test12\n",
            "test02\n",
            "figure1\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   0/5. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.764, 268 neurons 0.739 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.763\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.764, 268 neurons 0.754 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.771\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.770, 268 neurons 0.754 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.774\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.770, 268 neurons 0.754 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.774\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.797, 268 neurons 0.784 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.799\u001b[0m\n",
            "test06\n",
            "test07\n",
            "test01\n",
            "test10\n",
            "test11\n",
            "test12\n",
            "test02\n",
            "figure2\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   0/5. found at 3! covered neurons percentage 52 neurons 0.885, 148 neurons 0.797, 268 neurons 0.787 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.801\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.818, 268 neurons 0.821 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.827\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.885, 148 neurons 0.818, 268 neurons 0.828 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.831\u001b[0m\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.885, 148 neurons 0.818, 268 neurons 0.828 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.831\u001b[0m\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.885, 148 neurons 0.818, 268 neurons 0.828 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.831\u001b[0m\n",
            "test06\n",
            "test07\n",
            "test01\n",
            "test10\n",
            "test11\n",
            "test12\n",
            "test02\n",
            "figure3\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.824, 268 neurons 0.847 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.846\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   1/5. found at 4! covered neurons percentage 52 neurons 0.904, 148 neurons 0.824, 268 neurons 0.847 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.846\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "   2/5. test suite was not found: averaged covered neurons 0.850 at (0, 2, 1)\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.831, 268 neurons 0.851 at (0, 3, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.850\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   4/5. found at 3! covered neurons percentage 52 neurons 0.904, 148 neurons 0.845, 268 neurons 0.851 at (0, 4, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.855\u001b[0m\n",
            "test06\n",
            "test07\n",
            "test01\n",
            "test10\n",
            "test11\n",
            "test12\n",
            "test02\n",
            "figure4\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   0/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.845, 268 neurons 0.851 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.855\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.851, 268 neurons 0.851 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.857\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   2/5. found at 3! covered neurons percentage 52 neurons 0.904, 148 neurons 0.851, 268 neurons 0.858 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.861\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.851, 268 neurons 0.858 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.861\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.851, 268 neurons 0.858 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.861\u001b[0m\n",
            "test06\n",
            "test07\n",
            "test01\n",
            "test10\n",
            "test11\n",
            "test12\n",
            "test02\n",
            "figure5\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.865, 268 neurons 0.858 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.865\u001b[0m\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.865, 268 neurons 0.858 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.865\u001b[0m\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.865, 268 neurons 0.858 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.865\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.865, 268 neurons 0.858 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.865\u001b[0m\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.865, 268 neurons 0.862 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.868\u001b[0m\n",
            "test06\n",
            "test07\n",
            "test01\n",
            "test10\n",
            "test11\n",
            "test12\n",
            "test02\n",
            "figure6\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.872, 268 neurons 0.866 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.872\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.872, 268 neurons 0.866 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.872\u001b[0m\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.872, 268 neurons 0.873 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.876\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.872, 268 neurons 0.873 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.878\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.878, 268 neurons 0.873 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.880\u001b[0m\n",
            "test06\n",
            "test07\n",
            "test01\n",
            "test10\n",
            "test11\n",
            "test12\n",
            "test02\n",
            "figure7\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   0/5. found at 3! covered neurons percentage 52 neurons 0.923, 148 neurons 0.878, 268 neurons 0.873 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.880\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   1/5. found at 3! covered neurons percentage 52 neurons 0.923, 148 neurons 0.878, 268 neurons 0.877 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.882\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   2/5. found at 6! covered neurons percentage 52 neurons 0.923, 148 neurons 0.878, 268 neurons 0.877 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.882\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.878, 268 neurons 0.877 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.882\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.878, 268 neurons 0.877 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.882\u001b[0m\n",
            "test06\n",
            "test07\n",
            "test01\n",
            "test10\n",
            "test11\n",
            "test12\n",
            "test02\n",
            "figure8\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   0/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.878, 268 neurons 0.877 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.882\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   1/5. found at 3! covered neurons percentage 52 neurons 0.923, 148 neurons 0.878, 268 neurons 0.877 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.882\u001b[0m\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.878, 268 neurons 0.881 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.885\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   3/5. found at 3! covered neurons percentage 52 neurons 0.923, 148 neurons 0.878, 268 neurons 0.881 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.885\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.878, 268 neurons 0.881 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.885\u001b[0m\n",
            "test06\n",
            "test07\n",
            "test01\n",
            "test10\n",
            "test11\n",
            "test12\n",
            "test02\n",
            "figure9\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.885, 268 neurons 0.881 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.887\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.885, 268 neurons 0.881 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.887\u001b[0m\n",
            "test03\n",
            "test04\n",
            "test05\n",
            "test04\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.884 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.891\u001b[0m\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.884 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.891\u001b[0m\n",
            "test03\n",
            "test04\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.892, 268 neurons 0.884 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.891\u001b[0m\n",
            "test06\n",
            "test07\n",
            "CPU times: user 2min 11s, sys: 894 ms, total: 2min 12s\n",
            "Wall time: 2min 21s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpsI1IOM-hUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "d8928934-2c80-4b91-89db-831f565c7031"
      },
      "source": [
        "df2_scale = pd.DataFrame()\n",
        "df3_scale = pd.DataFrame()\n",
        "for start_fig in range(10):\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")\n",
        "  df2_scale = pd.concat([df2_scale, df1_scale])\n",
        "\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_index.csv\")\n",
        "  df3_scale = pd.concat([df3_scale, df1_scale])\n",
        "\n",
        "\n",
        "tmp_list = list(df2_scale.iloc[:,0])\n",
        "df2_scale.index = tmp_list\n",
        "print(df2_scale.iloc[:,1:].head())\n",
        "\n",
        "tmp_list = list(df3_scale.iloc[:,0])\n",
        "df3_scale.index = tmp_list\n",
        "print(df3_scale.iloc[:,1:].head())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     ('block1_conv1', 0)  ...  ('predictions', 9)\n",
            "0_1                0.766  ...               0.000\n",
            "0_2                0.680  ...               0.000\n",
            "0_3                0.521  ...               0.000\n",
            "0_4                0.789  ...               0.000\n",
            "0_5                0.750  ...               0.000\n",
            "\n",
            "[5 rows x 52 columns]\n",
            "     already_diff  found  not_found  ... index2  layer3 index3\n",
            "0_1             0      4          0  ...     11     fc1     92\n",
            "0_2             0      1          0  ...     11     fc2     49\n",
            "0_3             0      2          0  ...     12     fc2     71\n",
            "0_4             0      3          0  ...     59     fc1     99\n",
            "0_5             0      3          0  ...      2     fc2     28\n",
            "\n",
            "[5 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QtyLAZiPeKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "63877d2f-e2f8-498d-a73e-6768c46c5aee"
      },
      "source": [
        "df2_scale.iloc[0:6,1:]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>('block1_conv1', 0)</th>\n",
              "      <th>('block1_conv1', 1)</th>\n",
              "      <th>('block1_conv1', 2)</th>\n",
              "      <th>('block1_conv1', 3)</th>\n",
              "      <th>('block1_pool1', 0)</th>\n",
              "      <th>('block1_pool1', 1)</th>\n",
              "      <th>('block1_pool1', 2)</th>\n",
              "      <th>('block1_pool1', 3)</th>\n",
              "      <th>('block2_conv1', 0)</th>\n",
              "      <th>('block2_conv1', 1)</th>\n",
              "      <th>('block2_conv1', 2)</th>\n",
              "      <th>('block2_conv1', 3)</th>\n",
              "      <th>('block2_conv1', 4)</th>\n",
              "      <th>('block2_conv1', 5)</th>\n",
              "      <th>('block2_conv1', 6)</th>\n",
              "      <th>('block2_conv1', 7)</th>\n",
              "      <th>('block2_conv1', 8)</th>\n",
              "      <th>('block2_conv1', 9)</th>\n",
              "      <th>('block2_conv1', 10)</th>\n",
              "      <th>('block2_conv1', 11)</th>\n",
              "      <th>('block2_pool1', 0)</th>\n",
              "      <th>('block2_pool1', 1)</th>\n",
              "      <th>('block2_pool1', 2)</th>\n",
              "      <th>('block2_pool1', 3)</th>\n",
              "      <th>('block2_pool1', 4)</th>\n",
              "      <th>('block2_pool1', 5)</th>\n",
              "      <th>('block2_pool1', 6)</th>\n",
              "      <th>('block2_pool1', 7)</th>\n",
              "      <th>('block2_pool1', 8)</th>\n",
              "      <th>('block2_pool1', 9)</th>\n",
              "      <th>('block2_pool1', 10)</th>\n",
              "      <th>('block2_pool1', 11)</th>\n",
              "      <th>('before_softmax', 0)</th>\n",
              "      <th>('before_softmax', 1)</th>\n",
              "      <th>('before_softmax', 2)</th>\n",
              "      <th>('before_softmax', 3)</th>\n",
              "      <th>('before_softmax', 4)</th>\n",
              "      <th>('before_softmax', 5)</th>\n",
              "      <th>('before_softmax', 6)</th>\n",
              "      <th>('before_softmax', 7)</th>\n",
              "      <th>('before_softmax', 8)</th>\n",
              "      <th>('before_softmax', 9)</th>\n",
              "      <th>('predictions', 0)</th>\n",
              "      <th>('predictions', 1)</th>\n",
              "      <th>('predictions', 2)</th>\n",
              "      <th>('predictions', 3)</th>\n",
              "      <th>('predictions', 4)</th>\n",
              "      <th>('predictions', 5)</th>\n",
              "      <th>('predictions', 6)</th>\n",
              "      <th>('predictions', 7)</th>\n",
              "      <th>('predictions', 8)</th>\n",
              "      <th>('predictions', 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_1</th>\n",
              "      <td>0.766</td>\n",
              "      <td>0.514</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.545</td>\n",
              "      <td>0.759</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.164</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.553</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.516</td>\n",
              "      <td>0.846</td>\n",
              "      <td>0.830</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.537</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_2</th>\n",
              "      <td>0.680</td>\n",
              "      <td>0.456</td>\n",
              "      <td>0.637</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.721</td>\n",
              "      <td>0.489</td>\n",
              "      <td>0.677</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.694</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.601</td>\n",
              "      <td>0.847</td>\n",
              "      <td>0.657</td>\n",
              "      <td>0.320</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.463</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.599</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_3</th>\n",
              "      <td>0.521</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.488</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.561</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.531</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.195</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.876</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.583</td>\n",
              "      <td>0.677</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.484</td>\n",
              "      <td>0.257</td>\n",
              "      <td>0.581</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.687</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_4</th>\n",
              "      <td>0.789</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.739</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.831</td>\n",
              "      <td>0.562</td>\n",
              "      <td>0.782</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.213</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.541</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.541</td>\n",
              "      <td>0.734</td>\n",
              "      <td>0.790</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.505</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.528</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_5</th>\n",
              "      <td>0.750</td>\n",
              "      <td>0.503</td>\n",
              "      <td>0.703</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.788</td>\n",
              "      <td>0.533</td>\n",
              "      <td>0.741</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.199</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.565</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.542</td>\n",
              "      <td>0.870</td>\n",
              "      <td>0.767</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.497</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.560</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_1</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.186</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.131</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.217</td>\n",
              "      <td>0.249</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.290</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.870</td>\n",
              "      <td>0.665</td>\n",
              "      <td>0.529</td>\n",
              "      <td>0.788</td>\n",
              "      <td>0.614</td>\n",
              "      <td>0.803</td>\n",
              "      <td>0.266</td>\n",
              "      <td>0.452</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ('block1_conv1', 0)  ...  ('predictions', 9)\n",
              "0_1                0.766  ...               0.000\n",
              "0_2                0.680  ...               0.000\n",
              "0_3                0.521  ...               0.000\n",
              "0_4                0.789  ...               0.000\n",
              "0_5                0.750  ...               0.000\n",
              "1_1                0.000  ...               0.000\n",
              "\n",
              "[6 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ1DCqKX9lTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_list = list(df2_scale.iloc[:,0])\n",
        "#df2_scale = df2_scale.rename(index=tmp_list)\n",
        "type(tmp_list)\n",
        "df2_scale.index = tmp_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiVj9Wjd-rVv",
        "colab_type": "text"
      },
      "source": [
        "以下が、df1_scaleに格納されているニューロンの出力値から発火/非発火の集計をとるアルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B19EsPFQrak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "32de8c57-49aa-4b71-e9b1-3f4be2582232"
      },
      "source": [
        "import pylab as pl\n",
        "\n",
        "thres = 0.1\n",
        "bools = df2_scale.iloc[:,1:] > thres\n",
        "\n",
        "bools.sum().hist()\n",
        "pl.xlabel(\"activation time\")\n",
        "pl.ylabel(\"number of neurons\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'number of neurons')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL8GwmfYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGmSlrMm7g6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "4eb5e325-ef70-46a6-bbc0-5a6641025477"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#print(test.index[0],test.index[0][0])\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(12, 8))\n",
        "bools.sum().plot(kind=\"bar\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f05ce5d3438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAIwCAYAAACP/wPjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPP1-qoLB_b-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "d70f1360-c981-49d7-90d9-df9cd5e87043"
      },
      "source": [
        "bools = df3_scale.iloc[:,1:4] > 0\n",
        "bools.sum().plot(kind=\"bar\")\n",
        "#bools.sum()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f05ce456f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEpCAYAAACKmHkAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDXKApekRv0/vdnBgJ15NjIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz77SyGDD2L2",
        "colab_type": "text"
      },
      "source": [
        "フィルタ処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z06piFAVDDOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "43d6a7bd-f1b6-42b2-8118-b44bf96ad1c3"
      },
      "source": [
        "import glob\n",
        "img_list = glob.glob(output_dir + \"/l*].png\")\n",
        "print(img_list[0][79:80])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HqwPQNxDp_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "707c4d80-0832-4c6a-c112-2ca6b3026f0f"
      },
      "source": [
        "from PIL import Image\n",
        "im = np.array(Image.open(img_list[0]))\n",
        "np.max(im), np.min(im)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(255, 255)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRueeE7hEUg4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "476b59f3-e2dd-4c8f-d168-6afe6a393e59"
      },
      "source": [
        "im_dev =np.array([])\n",
        "im_bug = pd.DataFrame(columns=[\"fig\"])\n",
        "im_all = pd.DataFrame(columns=[\"fig\"])\n",
        "index = 0\n",
        "\n",
        "for i in img_list:\n",
        "  im = np.array(Image.open(i))  \n",
        "  tmp = np.max(im)-np.min(im)\n",
        "  im_dev = np.append(im_dev, tmp)\n",
        "  if tmp > 50:\n",
        "    im_bug.loc[str(index)] = i[79:80]\n",
        "  im_all.loc[str(index)] = i[79:80]\n",
        "  index += 1\n",
        "\n",
        "plt.hist(im_dev)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([33.,  1.,  3.,  2.,  3.,  3.,  2.,  1.,  0.,  1.]),\n",
              " array([  0. ,  16.5,  33. ,  49.5,  66. ,  82.5,  99. , 115.5, 132. ,\n",
              "        148.5, 165. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noyxwSWqUBS5JjbLAJalR/we8laRCL6qnaQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUS1749kHv90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "7ceb0c37-e6e4-4cf5-9caf-e0ab09446769"
      },
      "source": [
        "im_all[\"fig\"].value_counts().sort_index().plot(kind=\"bar\")\n",
        "#im_bug[\"fig\"].value_counts().sort_index().plot(kind=\"bar\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f05ce3c55c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD1CAYAAAB5n7/BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nktScoZak5gy1JDVnqCWpuf8Bd6mgKBYYmvwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ISWtgVLQTSQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "d6f5b3c2-9bca-457f-c82a-9273603cbb40"
      },
      "source": [
        "im_bug[\"fig\"].value_counts().sort_index().plot(kind=\"bar\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f05ce3bdf98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD1CAYAAABA+A6aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60e_7IL1JM8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2f73c6c-21e6-444e-dc0d-e4761d72ce56"
      },
      "source": [
        "im_all"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fig</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fig\n",
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "5    1\n",
              "6    1\n",
              "7    1\n",
              "8    1\n",
              "9    1\n",
              "10   2\n",
              "11   2\n",
              "12   2\n",
              "13   2\n",
              "14   2\n",
              "15   3\n",
              "16   3\n",
              "17   3\n",
              "18   3\n",
              "19   4\n",
              "20   4\n",
              "21   4\n",
              "22   4\n",
              "23   4\n",
              "24   5\n",
              "25   5\n",
              "26   5\n",
              "27   5\n",
              "28   5\n",
              "29   6\n",
              "30   6\n",
              "31   6\n",
              "32   6\n",
              "33   6\n",
              "34   7\n",
              "35   7\n",
              "36   7\n",
              "37   7\n",
              "38   7\n",
              "39   8\n",
              "40   8\n",
              "41   8\n",
              "42   8\n",
              "43   8\n",
              "44   9\n",
              "45   9\n",
              "46   9\n",
              "47   9\n",
              "48   9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGHvvsEkJzXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fig = plt.figure()\n",
        "#ax = fig.add_subplot(1,1,1)\n",
        "#x1 = im_all[\"fig\"].value_counts()\n",
        "#x2 = im_bug[\"fig\"].value_counts()\n",
        "#ax.hist([x1, x2], bins=10, normed=True, color=['red', 'blue', 'green'], label=['x1', 'x2', 'x3'])\n",
        "#ax.set_title('seventh histogram $\\mu1=100,\\ \\sigma1=15,\\ \\mu2=50,\\ \\sigma2=4$')\n",
        "#ax.set_xlabel('x')\n",
        "#ax.set_ylabel('freq')\n",
        "#ax.legend(loc='upper left')\n",
        "#fig.show()\n",
        "#\n",
        "#x1, x2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxkrXvEuLOjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "1e8f6fb9-5697-4f9b-84f7-5a2fcf5eba57"
      },
      "source": [
        "x1 = im_all[\"fig\"].value_counts()\n",
        "x1, type(x1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7    5\n",
              " 0    5\n",
              " 4    5\n",
              " 2    5\n",
              " 9    5\n",
              " 5    5\n",
              " 1    5\n",
              " 8    5\n",
              " 6    5\n",
              " 3    4\n",
              " Name: fig, dtype: int64, pandas.core.series.Series)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ7spEzhOy10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(x1, x2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK8sscq8LTWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a83a45fb-570a-4dcb-f5f2-f331d7bf4340"
      },
      "source": [
        "x1.values"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcJirLEsbtHB",
        "colab_type": "text"
      },
      "source": [
        "以下は実際に各画像でどのくらい\"薄い\"画像があるかの分布"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAwybx1JLkn8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "5af7f81a-4682-465c-af8a-a956a34777e7"
      },
      "source": [
        "im_dev =np.array([])\n",
        "im_bug = pd.DataFrame(columns=[\"filter\", \"org\"])\n",
        "index = 0\n",
        "\n",
        "for i in img_list:\n",
        "  im = np.array(Image.open(i))  \n",
        "  tmp = np.max(im)-np.min(im)\n",
        "  im_dev = np.append(im_dev, tmp)\n",
        "  if tmp > 50:\n",
        "    im_bug.loc[str(index)] = [i[79:80], i[79:80]]\n",
        "  else:\n",
        "    im_bug.loc[str(index)] = [None, i[79:80]]\n",
        "#  im_all.loc[str(index)] = i[79:80]\n",
        "  index += 1\n",
        "\n",
        "plt.hist(im_dev)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([33.,  1.,  3.,  2.,  3.,  3.,  2.,  1.,  0.,  1.]),\n",
              " array([  0. ,  16.5,  33. ,  49.5,  66. ,  82.5,  99. , 115.5, 132. ,\n",
              "        148.5, 165. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noyxwSWqUBS5JjbLAJalR/we8laRCL6qnaQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ_waU1DMtUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fbddfac0-4df0-42d1-86c6-d564e7bdeb1c"
      },
      "source": [
        "im_bug"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filter</th>\n",
              "      <th>org</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>None</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>None</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>None</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>None</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>None</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>None</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>None</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>None</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>None</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>None</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>None</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>None</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>None</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>None</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   filter org\n",
              "0    None   0\n",
              "1    None   0\n",
              "2    None   0\n",
              "3    None   0\n",
              "4    None   0\n",
              "5    None   1\n",
              "6    None   1\n",
              "7    None   1\n",
              "8    None   1\n",
              "9    None   1\n",
              "10   None   2\n",
              "11      2   2\n",
              "12      2   2\n",
              "13   None   2\n",
              "14   None   2\n",
              "15      3   3\n",
              "16   None   3\n",
              "17   None   3\n",
              "18   None   3\n",
              "19   None   4\n",
              "20   None   4\n",
              "21   None   4\n",
              "22   None   4\n",
              "23   None   4\n",
              "24      5   5\n",
              "25   None   5\n",
              "26   None   5\n",
              "27   None   5\n",
              "28      5   5\n",
              "29      6   6\n",
              "30   None   6\n",
              "31      6   6\n",
              "32      6   6\n",
              "33      6   6\n",
              "34   None   7\n",
              "35   None   7\n",
              "36   None   7\n",
              "37   None   7\n",
              "38   None   7\n",
              "39   None   8\n",
              "40   None   8\n",
              "41   None   8\n",
              "42   None   8\n",
              "43   None   8\n",
              "44      9   9\n",
              "45   None   9\n",
              "46      9   9\n",
              "47   None   9\n",
              "48      9   9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwBNwplZM3UI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "254442bf-f43d-40b8-c7c5-5900988d851a"
      },
      "source": [
        "print(im_bug[\"filter\"].value_counts().sort_index())\n",
        "print(im_bug[\"org\"].value_counts().sort_index())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2    2\n",
            "3    1\n",
            "5    2\n",
            "6    4\n",
            "9    3\n",
            "Name: filter, dtype: int64\n",
            "0    5\n",
            "1    5\n",
            "2    5\n",
            "3    4\n",
            "4    5\n",
            "5    5\n",
            "6    5\n",
            "7    5\n",
            "8    5\n",
            "9    5\n",
            "Name: org, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWbPrxZRNBZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8fc2104a-5d5f-4a85-f753-e26facbb119e"
      },
      "source": [
        "type(im_all[\"fig\"])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lncXwCAINPAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#im_bug[\"filter\"].value_counts().sort_index().merge(im_bug[\"org\"].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obbro2n3ONLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}