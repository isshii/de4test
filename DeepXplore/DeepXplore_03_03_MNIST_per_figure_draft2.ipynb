{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "DeepXplore_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshii/de4test/blob/test_ozawa/DeepXplore/DeepXplore_03_03_MNIST_per_figure_draft2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHbF5GaCQlkP",
        "colab_type": "text"
      },
      "source": [
        "# 03 MNISTでデータ仕分けてncの変化みてみた\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BJ_KpsvA0M8",
        "colab_type": "text"
      },
      "source": [
        "## 03 01 これまでのコード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVFjZpUIRF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 共通で使うパスなどの定義\n",
        "# 共通の変数設定\n",
        "# 共通フォルダパス\n",
        "import datetime\n",
        "import pytz\n",
        "dt_now = datetime.datetime.now(pytz.timezone('Asia/Tokyo'))\n",
        "dt_str = str(dt_now.strftime('%Y%m%d_%H%M'))\n",
        "\n",
        "data_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data\"\n",
        "#data_imagenet = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet\"\n",
        "data_mnist = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/MNIST\"\n",
        "#data_imagenet_seeds = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet/seeds\"\n",
        "model_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/model\"\n",
        "#MNISTということでデータ量が小さいので、log残せるようにしました\n",
        "output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output/\" + dt_str\n",
        "tmp_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/tmp\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4lP4POGn5Xj",
        "colab_type": "code",
        "outputId": "744c9c9d-f1ef-4891-a370-4aeb63bcaa68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Goggle Drive つなぐ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3d3Suv3Q5Db",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f49766d-e871-437b-fe3b-86cc5d5cc896"
      },
      "source": [
        "# outputフォルダ内容物のクリーンアップ削除\n",
        "#!rm \"$output_dir\"/*\n",
        "!mkdir \"$output_dir\"\n",
        "!rm \"$output_dir\"/*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20191213_1724/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdHYnLT0Nbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST')\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDY8sDVBqxmK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8ea7755-850c-44a3-992e-c431b1719f8a"
      },
      "source": [
        "import argparse\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input\n",
        "import imageio\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "import os"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiGGwQVElJNH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "outputId": "d2b4e748-ba31-4980-b6b4-6435f8b5dd5e"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 477792367207114048, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 11819787962416814679\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 14534352244232139039\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15956161332\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 17188448684820799918\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQ2M_8glJNM",
        "colab_type": "code",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87816b7a-a8ff-4e6e-c510-4927f543cd2e"
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "# DeepXplore のパラメータ設定部\n",
        "# read the parameter\n",
        "# argument parsing\n",
        "parser = argparse.ArgumentParser(description='Main function for difference-inducing input generation in MNIST dataset')\n",
        "parser.add_argument('transformation', help=\"realistic transformation type\", choices=['light', 'occl', 'blackout'])\n",
        "parser.add_argument('weight_diff', help=\"weight hyperparm to control differential behavior\", type=float)\n",
        "parser.add_argument('weight_nc', help=\"weight hyperparm to control neuron coverage\", type=float)\n",
        "parser.add_argument('step', help=\"step size of gradient descent\", type=float)\n",
        "parser.add_argument('seeds', help=\"number of seeds of input\", type=int)\n",
        "parser.add_argument('grad_iterations', help=\"number of iterations of gradient descent\", type=int)\n",
        "parser.add_argument('threshold', help=\"threshold for determining neuron activated\", type=float)\n",
        "parser.add_argument('-t', '--target_model', help=\"target model that we want it predicts differently\",\n",
        "                    choices=[0, 1, 2], default=0, type=int)\n",
        "parser.add_argument('-sp', '--start_point', help=\"occlusion upper left corner coordinate\", default=(0, 0), type=tuple)\n",
        "parser.add_argument('-occl_size', '--occlusion_size', help=\"occlusion size\", default=(10, 10), type=tuple)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-occl_size', '--occlusion_size'], dest='occlusion_size', nargs=None, const=None, default=(10, 10), type=<class 'tuple'>, choices=None, help='occlusion size', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93KW-VGjMzun",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title ノイズのタイプ light / occl / blackout\n",
        "#@body {light,occl,blackout} weight_diff weight_nc step seeds\n",
        "#                   grad_iterations threshold\n",
        "noise_type = \"light\" #@param [\"light\", \"occl\", \"blackout\"]\n",
        "weight_diff = \"0.1\" #@param {type:\"string\"}\n",
        "weight_nc = \"0.1\" #@param {type:\"string\"}\n",
        "step = \"4\" #@param {type:\"string\"}　#50だとほぼ白飛び\n",
        "seeds = \"50\" #@param {type:\"string\"}\n",
        "grad_iterations = \"10\" #@param {type:\"string\"}\n",
        "threshold = \"0.9\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk80fCuPts9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = parser.parse_args([noise_type, weight_diff, weight_nc, step, seeds, grad_iterations, threshold])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tTYrllplJNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x.reshape(x.shape[1], x.shape[2])  # original shape (1,img_rows, img_cols,1)\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "def constraint_occl(gradients, start_point, rect_shape):\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def constraint_light(gradients):\n",
        "    new_grads = np.ones_like(gradients)\n",
        "    grad_mean = np.mean(gradients)\n",
        "    return grad_mean * new_grads\n",
        "\n",
        "\n",
        "def constraint_black(gradients, rect_shape=(6, 6)):\n",
        "    start_point = (\n",
        "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    if np.mean(patch) < 0:\n",
        "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def init_coverage_tables(model1, model2, model3):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    model_layer_dict2 = defaultdict(bool)\n",
        "    model_layer_dict3 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    init_dict(model2, model_layer_dict2)\n",
        "    init_dict(model3, model_layer_dict3)\n",
        "    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n",
        "\n",
        "\n",
        "def init_dict(model, model_layer_dict):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_dict[(layer.name, index)] = False\n",
        "\n",
        "\n",
        "def neuron_to_cover(model_layer_dict):\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "    else:\n",
        "        layer_name, index = random.choice(model_layer_dict.keys())\n",
        "    return layer_name, index\n",
        "\n",
        "\n",
        "def neuron_covered(model_layer_dict):\n",
        "    covered_neurons = len([v for v in model_layer_dict.values() if v])\n",
        "    total_neurons = len(model_layer_dict)\n",
        "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n",
        "\n",
        "\n",
        "def update_coverage(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n",
        "                model_layer_dict[(layer_names[i], num_neuron)] = True\n",
        "\n",
        "\n",
        "def full_coverage(model_layer_dict):\n",
        "    if False in model_layer_dict.values():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
        "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
        "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
        "    X_scaled = X_std * (rmax - rmin) + rmin\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def fired(model, layer_name, index, input_data, threshold=0):\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
        "    scaled = scale(intermediate_layer_output)\n",
        "    if np.mean(scaled[..., index]) > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def diverged(predictions1, predictions2, predictions3, target):\n",
        "    #     if predictions2 == predictions3 == target and predictions1 != target:\n",
        "    if not predictions1 == predictions2 == predictions3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "#add\n",
        "def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n",
        "    return [os.path.join(root, f)\n",
        "            for root, _, files in os.walk(directory) for f in files\n",
        "            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]\n",
        "\n",
        "def _compute_gradients(tensor, var_list):\n",
        "    grads = tf.gradients(tensor, var_list)\n",
        "    return [grad if grad is not None else tf.zeros_like(var) for var, grad in zip(var_list, grads)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoRphb5Fl0JT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3431b706-e13e-44b6-e695-1c96cee5558c"
      },
      "source": [
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "# the data, shuffled and split between train and test sets\n",
        "(_, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "\n",
        "# define input tensor as a placeholder\n",
        "input_tensor = Input(shape=input_shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDDT47Er6u23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e55a3ae9-671c-4b40-eb54-5ae9fa6c5d59"
      },
      "source": [
        "%%time\n",
        "'''\n",
        "LeNet-1　##学習が少ない\n",
        "'''\n",
        "def Model1(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "#        ######################\n",
        "#        x_train = x_train[:1000]\n",
        "#        y_train = y_train[:1000]\n",
        "#        nb_epoch = 2\n",
        "#        ######################\n",
        "        print(x_train.shape)\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(4, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(12, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "#        model.save_weights('./Model1.h5')\n",
        "        model.save_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "#        model.load_weights('./Model1.h5')\n",
        "        model.load_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        print(bcolors.OKBLUE + 'Model1 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    Model1(train=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 6.91 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIb0Nn5tkaq_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "ff8b9a2f-46db-497d-8d36-6727a2cc10a0"
      },
      "source": [
        "model1 = Model1(input_tensor=input_tensor)\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "test_x = x_test.reshape(-1,1,28,28,1)\n",
        "gen_img = test_x[0]\n",
        "orig_img = gen_img.copy()\n",
        "print(model1.predict(orig_img), y_test[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "\u001b[94mModel1 loaded\u001b[0m\n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]] 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU88awZf68JO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "afb944d4-00ce-458a-a56c-3e565eed733f"
      },
      "source": [
        "%%time\n",
        "'''\n",
        "LeNet-4\n",
        "'''\n",
        "def Model2(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(84, activation='relu', name='fc1')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model2.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "#        model.load_weights('./content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model2.h5')\n",
        "        model.load_weights('./Model2.h5')\n",
        "        print(bcolors.OKBLUE + 'Model2 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    Model2(train=True)\n",
        "#    Model2(train=False)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 6.91 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufs67uRN7KYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9411c44b-1801-4d3c-c645-29dbb41bc9e1"
      },
      "source": [
        "%%time\n",
        "'''\n",
        "LeNet-5\n",
        "'''\n",
        "def Model3(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(120, activation='relu', name='fc1')(x)\n",
        "    x = Dense(84, activation='relu', name='fc2')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model3.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model3.h5')\n",
        "        print(bcolors.OKBLUE + 'Model3 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    Model3(train=True)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
            "Wall time: 7.39 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9QiwlW7WWR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "68607bb0-5804-4a05-fecf-aea7802f98e6"
      },
      "source": [
        "%%time\n",
        "model1 = Model1(input_tensor=input_tensor, train=True)\n",
        "model2 = Model2(input_tensor=input_tensor, train=True)\n",
        "model3 = Model3(input_tensor=input_tensor, train=True)\n",
        "model_layer_dict1, model_layer_dict2, model_layer_dict3 = init_coverage_tables(model1, model2, model3)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.4499 - acc: 0.8600 - val_loss: 0.1174 - val_acc: 0.9642\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.1108 - acc: 0.9671 - val_loss: 0.0890 - val_acc: 0.9727\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 0.0821 - acc: 0.9760 - val_loss: 0.0692 - val_acc: 0.9795\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 25us/step - loss: 0.0684 - acc: 0.9792 - val_loss: 0.0666 - val_acc: 0.9780\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 25us/step - loss: 0.0599 - acc: 0.9815 - val_loss: 0.0600 - val_acc: 0.9810\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 25us/step - loss: 0.0537 - acc: 0.9836 - val_loss: 0.0469 - val_acc: 0.9850\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 1s 25us/step - loss: 0.0489 - acc: 0.9853 - val_loss: 0.0459 - val_acc: 0.9841\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 0.0459 - acc: 0.9860 - val_loss: 0.0463 - val_acc: 0.9849\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 25us/step - loss: 0.0425 - acc: 0.9868 - val_loss: 0.0418 - val_acc: 0.9865\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 0.0402 - acc: 0.9876 - val_loss: 0.0418 - val_acc: 0.9866\n",
            "\n",
            "\n",
            "Overall Test score: 0.04176943360313307\n",
            "Overall Test accuracy: 0.9866\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.3837 - acc: 0.8810 - val_loss: 0.1228 - val_acc: 0.9619\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0844 - acc: 0.9740 - val_loss: 0.0905 - val_acc: 0.9692\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0590 - acc: 0.9818 - val_loss: 0.0567 - val_acc: 0.9812\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0468 - acc: 0.9850 - val_loss: 0.0427 - val_acc: 0.9855\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0389 - acc: 0.9878 - val_loss: 0.0331 - val_acc: 0.9886\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0327 - acc: 0.9900 - val_loss: 0.0492 - val_acc: 0.9848\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0282 - acc: 0.9909 - val_loss: 0.0323 - val_acc: 0.9901\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0247 - acc: 0.9922 - val_loss: 0.0325 - val_acc: 0.9895\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0219 - acc: 0.9930 - val_loss: 0.0379 - val_acc: 0.9891\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0195 - acc: 0.9940 - val_loss: 0.0311 - val_acc: 0.9901\n",
            "\n",
            "\n",
            "Overall Test score: 0.031098138961410587\n",
            "Overall Test accuracy: 0.9901\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.3822 - acc: 0.8803 - val_loss: 0.1294 - val_acc: 0.9587\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0764 - acc: 0.9762 - val_loss: 0.0556 - val_acc: 0.9815\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0534 - acc: 0.9836 - val_loss: 0.0422 - val_acc: 0.9866\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0413 - acc: 0.9871 - val_loss: 0.0651 - val_acc: 0.9780\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0325 - acc: 0.9896 - val_loss: 0.0369 - val_acc: 0.9878\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0276 - acc: 0.9908 - val_loss: 0.0493 - val_acc: 0.9831\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0235 - acc: 0.9925 - val_loss: 0.0371 - val_acc: 0.9884\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0198 - acc: 0.9936 - val_loss: 0.0292 - val_acc: 0.9906\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0174 - acc: 0.9946 - val_loss: 0.0354 - val_acc: 0.9883\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0142 - acc: 0.9955 - val_loss: 0.0314 - val_acc: 0.9905\n",
            "\n",
            "\n",
            "Overall Test score: 0.031363907285498496\n",
            "Overall Test accuracy: 0.9905\n",
            "CPU times: user 1min 5s, sys: 5.77 s, total: 1min 11s\n",
            "Wall time: 55.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z5-Z5kQFNIP",
        "colab_type": "text"
      },
      "source": [
        "## 03 02 MNISTの画像の仕分け"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZczbjEImFQEY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c1a66a44-637b-4aee-d2bb-3de421d07fb2"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "test_per_fig_x = np.array([])\n",
        "test_per_fig_y = np.array([])\n",
        "tests_x = np.array([])\n",
        "tests_y = np.array([])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "length = int(args.seeds/10)\n",
        "for i in range(10):\n",
        "  cond = [(x==i) for x in y_test]\n",
        "  test_per_fig_x = x_test[cond]\n",
        "  test_per_fig_y = y_test[cond]\n",
        "  print(i, test_per_fig_x.shape, test_per_fig_x.shape[0]-1000)\n",
        "  tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "  tests_y = np.append(tests_y, test_per_fig_y[:length])\n",
        "#  conds = [conds, cond]\n",
        "print(\"check!\", tests_x.shape[0]/28/28/10, \"=\", length, \"equal?\")\n",
        "tests_x = tests_x.reshape(-1,1,28,28,1)\n",
        "tests_x.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 (980, 28, 28) -20\n",
            "1 (1135, 28, 28) 135\n",
            "2 (1032, 28, 28) 32\n",
            "3 (1010, 28, 28) 10\n",
            "4 (982, 28, 28) -18\n",
            "5 (892, 28, 28) -108\n",
            "6 (958, 28, 28) -42\n",
            "7 (1028, 28, 28) 28\n",
            "8 (974, 28, 28) -26\n",
            "9 (1009, 28, 28) 9\n",
            "check! 5.0 = 5 equal?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 1, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUMHHCzp8WFY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e4c21314-9c58-40e0-a340-903be89fef30"
      },
      "source": [
        "index_fig = 0\n",
        "_ = 0\n",
        "gen_img = tests_x[(length*index_fig + _)]\n",
        "orig_img = gen_img.copy()\n",
        "print(model1.predict(gen_img))\n",
        "print(model2.predict(gen_img))\n",
        "print(model3.predict(gen_img))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[9.9992859e-01 2.8022482e-10 2.7219187e-06 2.1420776e-09 1.4030316e-08\n",
            "  1.4882934e-07 6.7595145e-05 1.2640587e-07 1.3443398e-08 7.9278999e-07]]\n",
            "[[9.9999630e-01 1.6266185e-12 8.8053133e-07 6.8224697e-11 3.5310843e-10\n",
            "  5.1026601e-09 2.8396155e-06 3.0903689e-11 9.0312607e-10 5.8925713e-09]]\n",
            "[[9.9999869e-01 3.4396295e-11 6.1293064e-09 2.8042371e-09 1.7833514e-10\n",
            "  2.8417350e-08 1.2774207e-06 8.8844160e-10 4.7295340e-10 1.8292376e-08]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3-Xbbw96Zjk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "outputId": "e2cd5340-b0bf-4486-e505-5b31c5db2ed8"
      },
      "source": [
        "%%time\n",
        "\n",
        "count_already = 0\n",
        "count_found = 0\n",
        "count_not_found = 0\n",
        "import pandas as pd\n",
        "temp_per_nc1 = np.array([])\n",
        "temp_per_nc2 = np.array([])\n",
        "temp_per_nc3 = np.array([])\n",
        "temp_num_nc1 = np.array([])\n",
        "temp_num_nc2 = np.array([])\n",
        "temp_num_nc3 = np.array([])\n",
        "\n",
        "for index_fig in range(10):\n",
        "  print(index_fig)\n",
        "  for _ in range(length):\n",
        "    gen_img = tests_x[(length*index_fig + _)]\n",
        "    orig_img = gen_img.copy()\n",
        "    # first check if input already induces differences\n",
        "    label1, label2, label3 = np.argmax(model1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "    if not label1 == label2 == label3:\n",
        "        count_already += 1\n",
        "        print(bcolors.OKGREEN + '{}/10. {}/{}. input already causes different outputs ({},{},{}) at{}/{}: '.format(index_fig, _, length, label1, label2, label3, count_already, count_already + count_found + count_not_found) + bcolors.ENDC)\n",
        "        update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "        update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "        update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "        temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "        temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "        temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "        temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "        temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "        temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "        print(bcolors.OKGREEN + '     covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'\n",
        "              % (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                 neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                 neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "        averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                       neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "            neuron_covered(model_layer_dict3)[1])\n",
        "        print(bcolors.OKGREEN + '     averaged covered neurons %.3f' % (averaged_nc) + bcolors.ENDC)\n",
        "\n",
        "        gen_img_deprocessed = deprocess_image(gen_img)\n",
        "\n",
        "        # save the result to disk\n",
        "        outputfilepath0 = os.path.join(output_dir, 'already_differ_' + str(label1) + '_' + str(label2) + '_' + str(label3) +'_['+ str(_) +  '].png')\n",
        "        imageio.imwrite(outputfilepath0, gen_img_deprocessed)\n",
        "        continue\n",
        "\n",
        "    # if all label agrees\n",
        "    orig_label = label1\n",
        "    layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n",
        "    layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n",
        "    layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n",
        "\n",
        "    # construct joint loss function\n",
        "    if args.target_model == 0:\n",
        "        loss1 = -args.weight_diff * K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "    elif args.target_model == 1:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = -args.weight_diff * K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "    elif args.target_model == 2:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "    loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n",
        "    loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n",
        "    loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n",
        "    layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron)\n",
        "\n",
        "    # for adversarial image generation\n",
        "    final_loss = K.mean(layer_output)\n",
        "\n",
        "    # we compute the gradient of the input picture wrt this loss\n",
        "    grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "    #grads = normalize(_compute_gradients(final_loss, [input_tensor])[0])\n",
        "\n",
        "    # this function returns the loss and grads given the input picture\n",
        "    iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n",
        "\n",
        "    # we run gradient ascent\n",
        "    for iters in range(args.grad_iterations):\n",
        "        loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = iterate([gen_img])\n",
        "        if args.transformation == 'light':\n",
        "            grads_value = constraint_light(grads_value)  # constraint the gradients value\n",
        "        elif args.transformation == 'occl':\n",
        "            grads_value = constraint_occl(grads_value, args.start_point,\n",
        "                                          args.occlusion_size)  # constraint the gradients value\n",
        "        elif args.transformation == 'blackout':\n",
        "            grads_value = constraint_black(grads_value)  # constraint the gradients value\n",
        "\n",
        "        gen_img += grads_value * args.step\n",
        "        predictions1 = np.argmax(model1.predict(gen_img)[0])\n",
        "        predictions2 = np.argmax(model2.predict(gen_img)[0])\n",
        "        predictions3 = np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "        if not predictions1 == predictions2 == predictions3:\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            count_found += 1\n",
        "            print(bcolors.OKBLUE + '%2d/10. %4d/%d. found at %d! covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f at %d/%d'\n",
        "                  % (index_fig, _, length, iters, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                     neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                     neuron_covered(model_layer_dict3)[2], count_found, count_already + count_found + count_not_found) + bcolors.ENDC)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[\n",
        "                    1])\n",
        "            print(bcolors.OKBLUE + '     averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  '].png')\n",
        "            #print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "#            outputfilepath2 = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '_orig.png')\n",
        "            outputfilepath2 = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  ']_orig.png')\n",
        "            #print(outputfilepath2)\n",
        "            imageio.imwrite(outputfilepath2, orig_img_deprocessed)\n",
        "            break\n",
        "        #add\n",
        "        if iters == (args.grad_iterations-1):\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[\n",
        "                    1])\n",
        "            count_not_found += 1\n",
        "            print('%2d/10. %4d/%d. test suite was not found: averaged covered neurons %.3f at %d/%d, labels:%d,%d,%d' % (index_fig, _, length, averaged_nc, count_not_found, count_already + count_found + count_not_found,label1, label2, label3 ))\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            #orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, 'not_found_' + str(label1)+'_['+ str(_) + '].png')\n",
        "#            print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "\n",
        "    temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "    temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "    temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "    temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "    temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "    temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "temp_per_nc1=temp_per_nc1.reshape(10, length)\n",
        "temp_per_nc2=temp_per_nc2.reshape(10, length)\n",
        "temp_per_nc3=temp_per_nc3.reshape(10, length)\n",
        "\n",
        "temp_num_nc1=temp_num_nc1.reshape(10, length)\n",
        "temp_num_nc2=temp_num_nc2.reshape(10, length)\n",
        "temp_num_nc3=temp_num_nc3.reshape(10, length)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    527\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    529\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 227\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    264\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    266\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: None values not supported.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    541\u001b[0m               observed = ops.internal_convert_to_tensor(\n\u001b[0;32m--> 542\u001b[0;31m                   values, as_ref=input_arg.is_ref).dtype.name\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 227\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    264\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    266\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: None values not supported.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-5ac465102d9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\ncount_already = 0\\ncount_found = 0\\ncount_not_found = 0\\nimport pandas as pd\\ntemp_per_nc1 = np.array([])\\ntemp_per_nc2 = np.array([])\\ntemp_per_nc3 = np.array([])\\ntemp_num_nc1 = np.array([])\\ntemp_num_nc2 = np.array([])\\ntemp_num_nc3 = np.array([])\\n\\nfor index_fig in range(10):\\n  print(index_fig)\\n  for _ in range(length):\\n    gen_img = tests_x[(length*index_fig + _)]\\n    orig_img = gen_img.copy()\\n    # first check if input already induces differences\\n    label1, label2, label3 = np.argmax(model1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(model3.predict(gen_img)[0])\\n\\n    if not label1 == label2 == label3:\\n        count_already += 1\\n        print(bcolors.OKGREEN + '{}/10. {}/{}. input already causes different outputs ({},{},{}) at{}/{}: '.format(index_fig, _, length, label1, label2, label3, count_already, count_already + count_found + count_not_found) + bcolors.ENDC)\\n        update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\\n        update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\\n        update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\\n\\n        temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\\n        temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2]...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-76568bf30214>\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# utility function to normalize a tensor by its L2 norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msquare\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1648\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m     \"\"\"\n\u001b[0;32m-> 1650\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36msquare\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m  10927\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10928\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m> 10929\u001b[0;31m         \"Square\", x=x, name=name)\n\u001b[0m\u001b[1;32m  10930\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10931\u001b[0m     result = _dispatch.dispatch(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    544\u001b[0m               raise ValueError(\n\u001b[1;32m    545\u001b[0m                   \u001b[0;34m\"Tried to convert '%s' to a tensor and failed. Error: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m                   (input_name, err))\n\u001b[0m\u001b[1;32m    547\u001b[0m             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n\u001b[1;32m    548\u001b[0m                       (input_name, op_type_name, observed))\n",
            "\u001b[0;31mValueError\u001b[0m: Tried to convert 'x' to a tensor and failed. Error: None values not supported."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o91ROs8sX1_e",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO4GHk4c9J-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d1a3e69-56ac-4a2a-f100-01dcf56ef6fd"
      },
      "source": [
        "iterate"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.backend.tensorflow_backend.Function at 0x7f6660c634a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5PnNLTHlEL1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b24336d8-2f9c-4e2e-b099-ceef12fc413f"
      },
      "source": [
        "print(temp_per_nc1, temp_per_nc1.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]] (10, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNyhas2rrpUn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5a6768a-0c96-47a9-ee32-6c0da9c70733"
      },
      "source": [
        "data_per_x1 = np.average(temp_per_nc1, axis=1)\n",
        "data_per_x2 = np.average(temp_per_nc2, axis=1)\n",
        "data_per_x3 = np.average(temp_per_nc3, axis=1)\n",
        "std_per_x1 = np.std(temp_per_nc1, axis=1)\n",
        "std_per_x2 = np.std(temp_per_nc2, axis=1)\n",
        "std_per_x3 = np.std(temp_per_nc3, axis=1)\n",
        "\n",
        "print(data_per_x1, std_per_x1, std_per_x2, std_per_x3)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ILBFhyE7w_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe0fd79c-d52d-4e7a-9ee0-3709d6e08019"
      },
      "source": [
        "data_num_x1 = np.average(temp_num_nc1, axis=1)\n",
        "data_num_x2 = np.average(temp_num_nc2, axis=1)\n",
        "data_num_x3 = np.average(temp_num_nc3, axis=1)\n",
        "std_num_x1 = np.std(temp_num_nc1, axis=1)\n",
        "std_num_x2 = np.std(temp_num_nc2, axis=1)\n",
        "std_num_x3 = np.std(temp_num_nc3, axis=1)\n",
        "\n",
        "print(data_num_x1, std_num_x1, std_num_x2, std_num_x3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-_oC8HwstgE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "d2680390-cea5-4c07-af45-462a0172a16b"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "t = np.linspace(0, 10, 10)\n",
        "fig = plt.figure(figsize=(20,7),dpi=60)\n",
        "ax = fig.add_subplot(1,2,1)\n",
        "plt.errorbar(t, data_per_x1, yerr=std_per_x1, marker=\"o\", label=\"model1\")\n",
        "plt.errorbar(t, data_per_x2, yerr=std_per_x2, marker=\"s\", label=\"model2\")\n",
        "plt.errorbar(t, data_per_x3, yerr=std_per_x3, marker=\"^\", label=\"model3\")\n",
        "ax.legend(loc=0)\n",
        "plt.legend(fontsize=18)\n",
        "plt.title(\"neuron coverage\", fontsize=18)\n",
        "ax.set_xlabel('step', fontsize=18)\n",
        "ax.set_ylabel('neuron coverage', fontsize=18)\n",
        "plt.tick_params(labelsize=16)\n",
        "\n",
        "ax = fig.add_subplot(1,2,2)\n",
        "plt.errorbar(t, data_num_x1, yerr=std_num_x1, marker=\"o\", label=\"model1:\"+str(neuron_covered(model_layer_dict1)[1]))\n",
        "plt.errorbar(t, data_num_x2, yerr=std_num_x2, marker=\"s\", label=\"model2:\"+str(neuron_covered(model_layer_dict2)[1]))\n",
        "plt.errorbar(t, data_num_x3, yerr=std_num_x3, marker=\"^\", label=\"model3:\"+str(neuron_covered(model_layer_dict3)[1]))\n",
        "ax.legend(loc=0)\n",
        "plt.legend(fontsize=18)\n",
        "plt.title(\"neuron coverage\", fontsize=18)\n",
        "ax.set_xlabel('step', fontsize=18)\n",
        "ax.set_ylabel('# covered neuron ', fontsize=18)\n",
        "plt.tick_params(labelsize=16)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+8AAAF/CAYAAAA4g/hPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1200x420 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMdZZSDI4P_8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e1847ff-67f7-4ae8-adc4-7cd41311c919"
      },
      "source": [
        "neuron_covered(model_layer_dict1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 52, 0.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxWV5ElWzHTy",
        "colab_type": "text"
      },
      "source": [
        "# 02 neuron coverage測定\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGZQRy9H-84T",
        "colab_type": "text"
      },
      "source": [
        "## 02 01 layerごと"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a71yjeqUDVRj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "eb28d4d4-3b15-4a46-9003-101d727e8c4d"
      },
      "source": [
        "%%time\n",
        "#list(model_layer_dict1.keys())\n",
        "data_list_layer_nc = list([x[0][0], x[1]] for x in model_layer_dict3.items())\n",
        "data_list_layer = list(x[0][0] for x in model_layer_dict3.items())\n",
        "data_list_nc = list(x[1] for x in model_layer_dict3.items())\n",
        "#print(len(x))\n",
        "print([len(data_list_layer_nc), len(data_list_layer_nc[0]), len(data_list_layer), len(data_list_nc)])\n",
        "#import matplotlib\n",
        "#matplotlib.pyplot.hist(x)\n",
        "\n",
        "\n",
        "data_list_layer_nc_on = list(x[0][0] for x in model_layer_dict3.items() if x[1]==True)\n",
        "data_list_layer_nc_off = list(x[0][0] for x in model_layer_dict3.items() if x[1]==False)\n",
        "print(len(data_list_layer_nc_on), len(data_list_layer_nc_off), len(data_list_layer_nc_on)+len(data_list_layer_nc_off))\n",
        "data_list_layer_name = list(x[0][0] for x in model_layer_dict3.items() if x[0][1]==0)\n",
        "print(len(data_list_layer_name))\n",
        "bins = len(data_list_layer_name)-1\n",
        "\n",
        "data_list_layer_nc_0 = list(0 for x in model_layer_dict3.items() if x[1]==True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[268, 2, 268, 268]\n",
            "0 268 268\n",
            "8\n",
            "CPU times: user 2.41 ms, sys: 43 µs, total: 2.45 ms\n",
            "Wall time: 2.98 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1idGNH-zSvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "28d21e38-15d6-464c-87e6-e64881791dde"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(12,5),dpi=60)\n",
        "#ax = fig.add_subplot(1,2,1)\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "matplotlib.pyplot.hist(data_list_layer, rwidth=0.8, bins=bins)\n",
        "# x軸縦書き。\n",
        "#plt.xticks(rotation=90)\n",
        "ax.set_xlabel('layer')\n",
        "ax.set_ylabel('# of neurons')\n",
        "\n",
        "#ax = fig.add_subplot(1,2,2)\n",
        "#matplotlib.pyplot.hist(data_list_layer, rwidth=0.8, bins=bins)\n",
        "## x 軸のラベル\n",
        "#dt_labels = [dt.strftime('%H:%M') for dt in df['datetime']]  # ラベル一覧作成\n",
        "## 4つおきにラベルを表示する。\n",
        "#plt.xticks(rotation=90)\n",
        "#plt.ylim(0,12000)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, '# of neurons')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAEJCAYAAADcuvDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x300 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6px7zF7tzU5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e19f047-7374-4345-9850-cffb989532b4"
      },
      "source": [
        "\n",
        "fig = plt.figure(figsize=(12,5),dpi=60)\n",
        "#ax = fig.add_subplot(1,2,1)\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "#ax.hist([data_list_layer_nc_on, data_list_layer_nc_off],label=['ON', 'OFF'], histtype='bar', stacked=True, rwidth=0.9, bins=bins)\n",
        "ax.hist([data_list_layer_nc_on, data_list_layer_nc_off],label=['ON','OFF'], histtype='bar', stacked=True, rwidth=0.8, bins=bins)\n",
        "#ax.hist(data_list_layer_nc_on,label='ON', histtype='bar', rwidth=0.9, bins=bins)\n",
        "plt.xticks(rotation=90)\n",
        "ax.set_xlabel('layer name')\n",
        "ax.set_ylabel('# of neurons')\n",
        "ax.legend(loc='upper left')\n",
        "#ax = fig.add_subplot(1,2,2)\n",
        "#ax.hist([data_list_layer_nc_on, data_list_layer_nc_off],label=['ON', 'OFF'], histtype='bar', stacked=False, rwidth=0.9, bins=bins)\n",
        "#plt.xticks(rotation=90)\n",
        "#ax.set_xlabel('layer')\n",
        "#ax.set_ylabel('# of neurons')\n",
        "#ax.legend(loc='upper left')\n",
        "#plt.ylim(0,100)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConversionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mconvert_units\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/category.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, unit, axis)\u001b[0m\n\u001b[1;32m     51\u001b[0m             raise ValueError(\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0;34m'Missing category information for StrCategoryConverter; '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0;34m'this might be caused by unintendedly mixing categorical and '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Missing category information for StrCategoryConverter; this might be caused by unintendedly mixing categorical and numeric data",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mConversionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-607da5229845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#ax.hist([data_list_layer_nc_on, data_list_layer_nc_off],label=['ON', 'OFF'], histtype='bar', stacked=True, rwidth=0.9, bins=bins)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_list_layer_nc_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_list_layer_nc_off\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ON'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'OFF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhisttype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bar'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#ax.hist(data_list_layer_nc_on,label='ON', histtype='bar', rwidth=0.9, bins=bins)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mhist\u001b[0;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, normed, **kwargs)\u001b[0m\n\u001b[1;32m   6700\u001b[0m         \u001b[0;31m# Unit conversion is done individually on each dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6701\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_unit_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6702\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_xunits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6704\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbin_range\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   6700\u001b[0m         \u001b[0;31m# Unit conversion is done individually on each dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6701\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_unit_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6702\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_xunits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6704\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbin_range\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mconvert_xunits\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_yunits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mconvert_units\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m             raise munits.ConversionError('Failed to convert value(s) to axis '\n\u001b[0;32m-> 1553\u001b[0;31m                                          f'units: {x!r}') from e\n\u001b[0m\u001b[1;32m   1554\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConversionError\u001b[0m: Failed to convert value(s) to axis units: array(['block1_conv1', 'block1_conv1', 'block1_conv1', 'block1_conv1',\n       'block1_conv1', 'block1_conv1', 'block1_pool1', 'block1_pool1',\n       'block1_pool1', 'block1_pool1', 'block1_pool1', 'block1_pool1',\n       'block2_conv1', 'block2_conv1', 'block2_conv1', 'block2_conv1',\n       'block2_conv1', 'block2_conv1', 'block2_conv1', 'block2_conv1',\n       'block2_conv1', 'block2_conv1', 'block2_conv1', 'block2_conv1',\n       'block2_conv1', 'block2_conv1', 'block2_conv1', 'block2_conv1',\n       'block2_pool1', 'block2_pool1', 'block2_pool1', 'block2_pool1',\n       'block2_pool1', 'block2_pool1', 'block2_pool1', 'block2_pool1',\n       'block2_pool1', 'block2_pool1', 'block2_pool1', 'block2_pool1',\n       'block2_pool1', 'block2_pool1', 'block2_pool1', 'block2_pool1',\n       'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1',\n       'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1',\n       'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1',\n       'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1',\n       'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1',\n       'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1',\n       'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1',\n       'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1',\n       'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1',\n       'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1',\n       'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1',\n       'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1',\n       'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1', 'fc1',\n       'fc1', 'fc1', 'fc1', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2',\n       'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2',\n       'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2',\n       'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2',\n       'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2',\n       'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2',\n       'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2',\n       'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2',\n       'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2',\n       'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'fc2', 'before_softmax',\n       'before_softmax', 'before_softmax', 'before_softmax',\n       'before_softmax', 'before_softmax', 'before_softmax',\n       'before_softmax', 'before_softmax', 'before_softmax',\n       'predictions', 'predictions', 'predictions', 'predictions',\n       'predictions', 'predictions', 'predictions', 'predictions',\n       'predictions', 'predictions'], dtype='<U14')"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEACAYAAACXs/MsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA2IKAGBATAEADPwfOHqzQw3vkesAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x300 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MceoR_F83dKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i_FkG64BOYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6PyYcda5qOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG5HK0WPhr9R",
        "colab_type": "text"
      },
      "source": [
        "### memo MNISTのtrainデータを減らす"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTyNiYju5qML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIQ1gU5c5qJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLyEmzHrhiE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU8iwtK-hkLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train[:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiytAA7fhoXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoMgZ0EYhp7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2cfeXCZwCzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "count_already = 0\n",
        "count_found = 0\n",
        "count_not_found = 0\n",
        "import pandas as pd\n",
        "temp_per_nc1 = np.array([])\n",
        "temp_per_nc2 = np.array([])\n",
        "temp_per_nc3 = np.array([])\n",
        "temp_num_nc1 = np.array([])\n",
        "temp_num_nc2 = np.array([])\n",
        "temp_num_nc3 = np.array([])\n",
        "for index_fig in range(10):\n",
        "\n",
        "  for _ in range(length):\n",
        "    gen_img = tests_x[(length*index_fig + _)]\n",
        "    orig_img = gen_img.copy()\n",
        "    # first check if input already induces differences\n",
        "    label1, label2, label3 = np.argmax(model1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "    if not label1 == label2 == label3:\n",
        "        count_already += 1\n",
        "        print(bcolors.OKGREEN + '{}/10. {}/{}. input already causes different outputs ({},{},{}) at{}/{}: '.format(index_fig, _, length, label1, label2, label3, count_already, count_already + count_found + count_not_found) + bcolors.ENDC)\n",
        "        update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "        update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "        update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "        temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "        temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "        temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "        temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "        temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "        temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "        print(bcolors.OKGREEN + '     covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'\n",
        "              % (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                 neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                 neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "        averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                       neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "            neuron_covered(model_layer_dict3)[1])\n",
        "        print(bcolors.OKGREEN + '     averaged covered neurons %.3f' % (averaged_nc) + bcolors.ENDC)\n",
        "\n",
        "        gen_img_deprocessed = deprocess_image(gen_img)\n",
        "\n",
        "        # save the result to disk\n",
        "        outputfilepath0 = os.path.join(output_dir, 'already_differ_' + str(label1) + '_' + str(label2) + '_' + str(label3) +'_['+ str(_) +  '].png')\n",
        "        imageio.imwrite(outputfilepath0, gen_img_deprocessed)\n",
        "        continue\n",
        "\n",
        "\n",
        "    # if all label agrees\n",
        "    orig_label = label1\n",
        "    layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n",
        "    layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n",
        "    layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n",
        "\n",
        "    # construct joint loss function\n",
        "    if args.target_model == 0:\n",
        "        loss1 = -args.weight_diff * K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "    elif args.target_model == 1:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = -args.weight_diff * K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "    elif args.target_model == 2:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "    loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n",
        "    loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n",
        "    loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n",
        "    layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron)\n",
        "\n",
        "    # for adversarial image generation\n",
        "    final_loss = K.mean(layer_output)\n",
        "\n",
        "    # we compute the gradient of the input picture wrt this loss\n",
        "    grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "\n",
        "    # this function returns the loss and grads given the input picture\n",
        "    iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n",
        "\n",
        "    # run gradient ascent\n",
        "    for iters in range(args.grad_iterations):\n",
        "        loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = iterate(\n",
        "            [gen_img])\n",
        "        if args.transformation == 'light':\n",
        "            grads_value = constraint_light(grads_value)  # constraint the gradients value\n",
        "        elif args.transformation == 'occl':\n",
        "            grads_value = constraint_occl(grads_value, args.start_point,\n",
        "                                          args.occlusion_size)  # constraint the gradients value\n",
        "        elif args.transformation == 'blackout':\n",
        "            grads_value = constraint_black(grads_value)  # constraint the gradients value\n",
        "\n",
        "        gen_img += grads_value * args.step\n",
        "        predictions1 = np.argmax(model1.predict(gen_img)[0])\n",
        "        predictions2 = np.argmax(model2.predict(gen_img)[0])\n",
        "        predictions3 = np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "        if not predictions1 == predictions2 == predictions3:\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            count_found += 1\n",
        "            print(bcolors.OKBLUE + '%2d/10. %4d/%d. found at %d! covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f at %d/%d'\n",
        "                  % (index_fig, _, length, iters, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                     neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                     neuron_covered(model_layer_dict3)[2], count_found, count_already + count_found + count_not_found) + bcolors.ENDC)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[\n",
        "                    1])\n",
        "            print(bcolors.OKBLUE + '     averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  '].png')\n",
        "            #print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "#            outputfilepath2 = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '_orig.png')\n",
        "            outputfilepath2 = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  ']_orig.png')\n",
        "            #print(outputfilepath2)\n",
        "            imageio.imwrite(outputfilepath2, orig_img_deprocessed)\n",
        "            break\n",
        "        #add\n",
        "        if iters == (args.grad_iterations-1):\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[\n",
        "                    1])\n",
        "            count_not_found += 1\n",
        "            print('%2d/10. %4d/%d. test suite was not found: averaged covered neurons %.3f at %d/%d, labels:%d,%d,%d' % (index_fig, _, length, averaged_nc, count_not_found, count_already + count_found + count_not_found,label1, label2, label3 ))\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            #orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, 'not_found_' + str(label1)+'_['+ str(_) + '].png')\n",
        "#            print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "\n",
        "    temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "    temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "    temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "    temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "    temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "    temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "temp_per_nc1=temp_per_nc1.reshape(10, length)\n",
        "temp_per_nc2=temp_per_nc2.reshape(10, length)\n",
        "temp_per_nc3=temp_per_nc3.reshape(10, length)\n",
        "\n",
        "temp_num_nc1=temp_num_nc1.reshape(10, length)\n",
        "temp_num_nc2=temp_num_nc2.reshape(10, length)\n",
        "temp_num_nc3=temp_num_nc3.reshape(10, length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM5D9xf-zOfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}