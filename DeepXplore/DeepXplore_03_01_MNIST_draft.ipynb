{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "DeepXplore_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshii/de4test/blob/test_ozawa/DeepXplore/DeepXplore_03_01_MNIST_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVFjZpUIRF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 共通で使うパスなどの定義\n",
        "# 共通の変数設定\n",
        "# 共通フォルダパス\n",
        "data_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data\"\n",
        "#data_imagenet = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet\"\n",
        "data_mnist = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/MNIST\"\n",
        "#data_imagenet_seeds = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet/seeds\"\n",
        "model_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/model\"\n",
        "output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output\"\n",
        "tmp_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/tmp\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4lP4POGn5Xj",
        "colab_type": "code",
        "outputId": "05eb1bbe-5609-4fdd-e38c-899a3f35e8bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Goggle Drive つなぐ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3d3Suv3Q5Db",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a0ca3ddc-6264-4d0d-9892-c2d2869b604c"
      },
      "source": [
        "# outputフォルダ内容物のクリーンアップ削除\n",
        "!rm \"$output_dir\"/*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/ColabNotebooks/test4ai/output/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdHYnLT0Nbt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "051bf83b-dcc0-44f7-ea2b-733e9807665e"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST')\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDY8sDVBqxmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input\n",
        "import imageio\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiGGwQVElJNH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "af45d5e5-1de9-459c-c2df-d28fbf672997"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 17541259076891865258, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 14424652485678415114\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 13195106771664066421\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 7470045594\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 1174578177256227244\n",
              " physical_device_desc: \"device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQ2M_8glJNM",
        "colab_type": "code",
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ea113312-1824-443c-cdef-9a9f7632f30b"
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "# DeepXplore のパラメータ設定部\n",
        "# read the parameter\n",
        "# argument parsing\n",
        "parser = argparse.ArgumentParser(description='Main function for difference-inducing input generation in MNIST dataset')\n",
        "parser.add_argument('transformation', help=\"realistic transformation type\", choices=['light', 'occl', 'blackout'])\n",
        "parser.add_argument('weight_diff', help=\"weight hyperparm to control differential behavior\", type=float)\n",
        "parser.add_argument('weight_nc', help=\"weight hyperparm to control neuron coverage\", type=float)\n",
        "parser.add_argument('step', help=\"step size of gradient descent\", type=float)\n",
        "parser.add_argument('seeds', help=\"number of seeds of input\", type=int)\n",
        "parser.add_argument('grad_iterations', help=\"number of iterations of gradient descent\", type=int)\n",
        "parser.add_argument('threshold', help=\"threshold for determining neuron activated\", type=float)\n",
        "parser.add_argument('-t', '--target_model', help=\"target model that we want it predicts differently\",\n",
        "                    choices=[0, 1, 2], default=0, type=int)\n",
        "parser.add_argument('-sp', '--start_point', help=\"occlusion upper left corner coordinate\", default=(0, 0), type=tuple)\n",
        "parser.add_argument('-occl_size', '--occlusion_size', help=\"occlusion size\", default=(10, 10), type=tuple)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-occl_size', '--occlusion_size'], dest='occlusion_size', nargs=None, const=None, default=(10, 10), type=<class 'tuple'>, choices=None, help='occlusion size', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Csx_IXrMdMj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93KW-VGjMzun",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title ノイズのタイプ light / occl / blackout\n",
        "#@body {light,occl,blackout} weight_diff weight_nc step seeds\n",
        "#                   grad_iterations threshold\n",
        "noise_type = \"light\" #@param [\"light\", \"occl\", \"blackout\"]\n",
        "weight_diff = \"0.1\" #@param {type:\"string\"}\n",
        "weight_nc = \"0.1\" #@param {type:\"string\"}\n",
        "step = \"1\" #@param {type:\"string\"}\n",
        "seeds = \"1000\" #@param {type:\"string\"}\n",
        "grad_iterations = \"100\" #@param {type:\"string\"}\n",
        "threshold = \"0.1\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk80fCuPts9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = parser.parse_args([noise_type, weight_diff, weight_nc, step, seeds, grad_iterations, threshold])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tTYrllplJNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x.reshape(x.shape[1], x.shape[2])  # original shape (1,img_rows, img_cols,1)\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "def constraint_occl(gradients, start_point, rect_shape):\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def constraint_light(gradients):\n",
        "    new_grads = np.ones_like(gradients)\n",
        "    grad_mean = np.mean(gradients)\n",
        "    return grad_mean * new_grads\n",
        "\n",
        "\n",
        "def constraint_black(gradients, rect_shape=(6, 6)):\n",
        "    start_point = (\n",
        "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    if np.mean(patch) < 0:\n",
        "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def init_coverage_tables(model1, model2, model3):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    model_layer_dict2 = defaultdict(bool)\n",
        "    model_layer_dict3 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    init_dict(model2, model_layer_dict2)\n",
        "    init_dict(model3, model_layer_dict3)\n",
        "    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n",
        "\n",
        "\n",
        "def init_dict(model, model_layer_dict):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_dict[(layer.name, index)] = False\n",
        "\n",
        "\n",
        "def neuron_to_cover(model_layer_dict):\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "    else:\n",
        "        layer_name, index = random.choice(model_layer_dict.keys())\n",
        "    return layer_name, index\n",
        "\n",
        "\n",
        "def neuron_covered(model_layer_dict):\n",
        "    covered_neurons = len([v for v in model_layer_dict.values() if v])\n",
        "    total_neurons = len(model_layer_dict)\n",
        "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n",
        "\n",
        "\n",
        "def update_coverage(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n",
        "                model_layer_dict[(layer_names[i], num_neuron)] = True\n",
        "\n",
        "\n",
        "def full_coverage(model_layer_dict):\n",
        "    if False in model_layer_dict.values():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
        "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
        "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
        "    X_scaled = X_std * (rmax - rmin) + rmin\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def fired(model, layer_name, index, input_data, threshold=0):\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
        "    scaled = scale(intermediate_layer_output)\n",
        "    if np.mean(scaled[..., index]) > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def diverged(predictions1, predictions2, predictions3, target):\n",
        "    #     if predictions2 == predictions3 == target and predictions1 != target:\n",
        "    if not predictions1 == predictions2 == predictions3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "#add\n",
        "def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n",
        "    return [os.path.join(root, f)\n",
        "            for root, _, files in os.walk(directory) for f in files\n",
        "            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoRphb5Fl0JT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "1374db12-cd96-4463-efb8-ec6667b7d819"
      },
      "source": [
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "# the data, shuffled and split between train and test sets\n",
        "(_, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "\n",
        "# define input tensor as a placeholder\n",
        "input_tensor = Input(shape=input_shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDDT47Er6u23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "outputId": "5d6e25cb-2d3e-4009-f936-cc02ca4d0984"
      },
      "source": [
        "'''\n",
        "LeNet-1\n",
        "'''\n",
        "def Model1(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        print(x_train.shape)\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(4, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(12, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "#        model.save_weights('./Model1.h5')\n",
        "        model.save_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "#        model.load_weights('./Model1.h5')\n",
        "        model.load_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        print(bcolors.OKBLUE + 'Model1 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model1(train=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 4s 66us/step - loss: 0.4014 - acc: 0.8805 - val_loss: 0.1366 - val_acc: 0.9603\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.1187 - acc: 0.9643 - val_loss: 0.0905 - val_acc: 0.9704\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 1s 24us/step - loss: 0.0907 - acc: 0.9730 - val_loss: 0.0674 - val_acc: 0.9790\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0757 - acc: 0.9778 - val_loss: 0.0610 - val_acc: 0.9807\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0664 - acc: 0.9803 - val_loss: 0.0538 - val_acc: 0.9825\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0587 - acc: 0.9824 - val_loss: 0.0722 - val_acc: 0.9779\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0539 - acc: 0.9834 - val_loss: 0.0469 - val_acc: 0.9842\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0497 - acc: 0.9852 - val_loss: 0.0457 - val_acc: 0.9852\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0464 - acc: 0.9862 - val_loss: 0.0437 - val_acc: 0.9848\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 1s 23us/step - loss: 0.0431 - acc: 0.9867 - val_loss: 0.0435 - val_acc: 0.9855\n",
            "\n",
            "\n",
            "Overall Test score: 0.043497654444968795\n",
            "Overall Test accuracy: 0.9855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU88awZf68JO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "683cfcac-499a-43dd-e274-0d219d472c77"
      },
      "source": [
        "'''\n",
        "LeNet-4\n",
        "'''\n",
        "def Model2(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(84, activation='relu', name='fc1')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model2.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model2.h5')\n",
        "        print(bcolors.OKBLUE + 'Model2 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model2(train=True)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.3970 - acc: 0.8746 - val_loss: 0.1317 - val_acc: 0.9552\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0827 - acc: 0.9746 - val_loss: 0.0864 - val_acc: 0.9708\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0594 - acc: 0.9815 - val_loss: 0.0623 - val_acc: 0.9786\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0472 - acc: 0.9858 - val_loss: 0.0544 - val_acc: 0.9818\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0401 - acc: 0.9874 - val_loss: 0.0389 - val_acc: 0.9870\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0340 - acc: 0.9891 - val_loss: 0.0396 - val_acc: 0.9866\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0292 - acc: 0.9907 - val_loss: 0.0355 - val_acc: 0.9886\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0259 - acc: 0.9917 - val_loss: 0.0328 - val_acc: 0.9897\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0231 - acc: 0.9930 - val_loss: 0.0324 - val_acc: 0.9897\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.0202 - acc: 0.9936 - val_loss: 0.0338 - val_acc: 0.9886\n",
            "\n",
            "\n",
            "Overall Test score: 0.03375893767081361\n",
            "Overall Test accuracy: 0.9886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufs67uRN7KYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "dd51d816-cdff-4156-9870-733516f9fe87"
      },
      "source": [
        "'''\n",
        "LeNet-5\n",
        "'''\n",
        "def Model3(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 10\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(120, activation='relu', name='fc1')(x)\n",
        "    x = Dense(84, activation='relu', name='fc2')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model3.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model3.h5')\n",
        "        print(bcolors.OKBLUE + 'Model3 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model3(train=True)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 2s 37us/step - loss: 0.3930 - acc: 0.8794 - val_loss: 0.1262 - val_acc: 0.9622\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0836 - acc: 0.9740 - val_loss: 0.0597 - val_acc: 0.9811\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0579 - acc: 0.9818 - val_loss: 0.0831 - val_acc: 0.9755\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0448 - acc: 0.9857 - val_loss: 0.0459 - val_acc: 0.9848\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0369 - acc: 0.9887 - val_loss: 0.0617 - val_acc: 0.9803\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0307 - acc: 0.9903 - val_loss: 0.0318 - val_acc: 0.9901\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0252 - acc: 0.9919 - val_loss: 0.0337 - val_acc: 0.9876\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0221 - acc: 0.9930 - val_loss: 0.0418 - val_acc: 0.9870\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.0190 - acc: 0.9940 - val_loss: 0.0402 - val_acc: 0.9873\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0154 - acc: 0.9950 - val_loss: 0.0351 - val_acc: 0.9889\n",
            "\n",
            "\n",
            "Overall Test score: 0.03510084971156248\n",
            "Overall Test accuracy: 0.9889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9QiwlW7WWR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "597a0049-fd47-445e-de22-0fc834c04033"
      },
      "source": [
        "model1 = Model1(input_tensor=input_tensor)\n",
        "model2 = Model2(input_tensor=input_tensor)\n",
        "model3 = Model3(input_tensor=input_tensor)\n",
        "model_layer_dict1, model_layer_dict2, model_layer_dict3 = init_coverage_tables(model1, model2, model3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[94mModel1 loaded\u001b[0m\n",
            "\u001b[94mModel2 loaded\u001b[0m\n",
            "\u001b[94mModel3 loaded\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD6cHroNl43Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start gen inputs\n",
        "# img_paths = list_pictures(data_imagenet_seeds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfdTZi4mUPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f1c60df-ba2a-40e0-987d-3565bfcb44c0"
      },
      "source": [
        "%%time\n",
        "for _ in range(args.seeds):\n",
        "    gen_img = np.expand_dims(random.choice(x_test), axis=0)\n",
        "    orig_img = gen_img.copy()\n",
        "    # first check if input already induces differences\n",
        "    label1, label2, label3 = np.argmax(model1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(\n",
        "        model3.predict(gen_img)[0])\n",
        "\n",
        "    if not label1 == label2 == label3:\n",
        "        print(bcolors.OKGREEN + '{}/{}. input already causes different outputs: {}, {}, {}'.format(_, args.seeds, label1, label2,\n",
        "                                                                                            label3) + bcolors.ENDC)\n",
        "\n",
        "        update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "        update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "        update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "        print(bcolors.OKGREEN + 'covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'\n",
        "              % (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                 neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                 neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "        averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                       neuron_covered(model_layer_dict3)[0]) / float(\n",
        "            neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "            neuron_covered(model_layer_dict3)[\n",
        "                1])\n",
        "        print(bcolors.OKGREEN + '%d/%d. averaged covered neurons %.3f' % (_, args.seeds, averaged_nc) + bcolors.ENDC)\n",
        "\n",
        "        gen_img_deprocessed = deprocess_image(gen_img)\n",
        "\n",
        "        # save the result to disk\n",
        "        outputfilepath0 = os.path.join(output_dir, 'already_differ_' + decode_label(pred1) + '_' + decode_label(pred2) + '_' + decode_label(pred3) + '.png')\n",
        "        imageio.imwrite(outputfilepath0, gen_img_deprocessed)\n",
        "        continue\n",
        "\n",
        "    # if all label agrees\n",
        "    orig_label = label1\n",
        "    layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n",
        "    layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n",
        "    layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n",
        "\n",
        "    # construct joint loss function\n",
        "    if args.target_model == 0:\n",
        "        loss1 = -args.weight_diff * K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "    elif args.target_model == 1:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = -args.weight_diff * K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "    elif args.target_model == 2:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "    loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n",
        "    loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n",
        "    loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n",
        "    layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron)\n",
        "\n",
        "    # for adversarial image generation\n",
        "    final_loss = K.mean(layer_output)\n",
        "\n",
        "    # we compute the gradient of the input picture wrt this loss\n",
        "    grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "\n",
        "    # this function returns the loss and grads given the input picture\n",
        "    iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n",
        "\n",
        "    # we run gradient ascent for 20 steps\n",
        "    for iters in range(args.grad_iterations):\n",
        "        loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = iterate(\n",
        "            [gen_img])\n",
        "        if args.transformation == 'light':\n",
        "            grads_value = constraint_light(grads_value)  # constraint the gradients value\n",
        "        elif args.transformation == 'occl':\n",
        "            grads_value = constraint_occl(grads_value, args.start_point,\n",
        "                                          args.occlusion_size)  # constraint the gradients value\n",
        "        elif args.transformation == 'blackout':\n",
        "            grads_value = constraint_black(grads_value)  # constraint the gradients value\n",
        "\n",
        "        gen_img += grads_value * args.step\n",
        "        predictions1 = np.argmax(model1.predict(gen_img)[0])\n",
        "        predictions2 = np.argmax(model2.predict(gen_img)[0])\n",
        "        predictions3 = np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "        if not predictions1 == predictions2 == predictions3:\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "            print(bcolors.OKGREEN + 'covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'\n",
        "                  % (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                     neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                     neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[\n",
        "                    1])\n",
        "            print(bcolors.OKGREEN + 'averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            outputfilepath = os.path.join(output_dir, args.transformation + '_' + decode_label(pred1) + '_' + decode_label(pred2) + '_' + decode_label(pred3) + '.png')\n",
        "            print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "            outputfilepath2 = os.path.join(output_dir, args.transformation + '_' + decode_label(pred1) + '_' + decode_label(pred2) + '_' + decode_label(pred3) + '_orig.png')\n",
        "            print(outputfilepath2)\n",
        "            imageio.imwrite(outputfilepath2, orig_img_deprocessed)\n",
        "            break\n",
        "        #add\n",
        "        if iters == (args.grad_iterations-1):\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[\n",
        "                    1])\n",
        "            print('%d/%d. test suite was not found: averaged covered neurons %.3f' % (_, args.seeds, averaged_nc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0/1000. test suite was not found: averaged covered neurons 0.387\n",
            "1/1000. test suite was not found: averaged covered neurons 0.468\n",
            "2/1000. test suite was not found: averaged covered neurons 0.596\n",
            "3/1000. test suite was not found: averaged covered neurons 0.628\n",
            "4/1000. test suite was not found: averaged covered neurons 0.701\n",
            "5/1000. test suite was not found: averaged covered neurons 0.724\n",
            "6/1000. test suite was not found: averaged covered neurons 0.754\n",
            "7/1000. test suite was not found: averaged covered neurons 0.793\n",
            "8/1000. test suite was not found: averaged covered neurons 0.803\n",
            "9/1000. test suite was not found: averaged covered neurons 0.818\n",
            "10/1000. test suite was not found: averaged covered neurons 0.829\n",
            "11/1000. test suite was not found: averaged covered neurons 0.835\n",
            "12/1000. test suite was not found: averaged covered neurons 0.842\n",
            "13/1000. test suite was not found: averaged covered neurons 0.844\n",
            "14/1000. test suite was not found: averaged covered neurons 0.848\n",
            "15/1000. test suite was not found: averaged covered neurons 0.848\n",
            "16/1000. test suite was not found: averaged covered neurons 0.848\n",
            "17/1000. test suite was not found: averaged covered neurons 0.850\n",
            "18/1000. test suite was not found: averaged covered neurons 0.853\n",
            "19/1000. test suite was not found: averaged covered neurons 0.857\n",
            "20/1000. test suite was not found: averaged covered neurons 0.863\n",
            "21/1000. test suite was not found: averaged covered neurons 0.863\n",
            "22/1000. test suite was not found: averaged covered neurons 0.863\n",
            "23/1000. test suite was not found: averaged covered neurons 0.863\n",
            "24/1000. test suite was not found: averaged covered neurons 0.865\n",
            "25/1000. test suite was not found: averaged covered neurons 0.868\n",
            "26/1000. test suite was not found: averaged covered neurons 0.868\n",
            "27/1000. test suite was not found: averaged covered neurons 0.868\n",
            "28/1000. test suite was not found: averaged covered neurons 0.868\n",
            "29/1000. test suite was not found: averaged covered neurons 0.868\n",
            "30/1000. test suite was not found: averaged covered neurons 0.868\n",
            "31/1000. test suite was not found: averaged covered neurons 0.868\n",
            "32/1000. test suite was not found: averaged covered neurons 0.868\n",
            "33/1000. test suite was not found: averaged covered neurons 0.868\n",
            "34/1000. test suite was not found: averaged covered neurons 0.868\n",
            "35/1000. test suite was not found: averaged covered neurons 0.870\n",
            "36/1000. test suite was not found: averaged covered neurons 0.870\n",
            "37/1000. test suite was not found: averaged covered neurons 0.872\n",
            "38/1000. test suite was not found: averaged covered neurons 0.872\n",
            "39/1000. test suite was not found: averaged covered neurons 0.872\n",
            "40/1000. test suite was not found: averaged covered neurons 0.872\n",
            "41/1000. test suite was not found: averaged covered neurons 0.872\n",
            "42/1000. test suite was not found: averaged covered neurons 0.872\n",
            "43/1000. test suite was not found: averaged covered neurons 0.872\n",
            "44/1000. test suite was not found: averaged covered neurons 0.872\n",
            "45/1000. test suite was not found: averaged covered neurons 0.872\n",
            "46/1000. test suite was not found: averaged covered neurons 0.872\n",
            "47/1000. test suite was not found: averaged covered neurons 0.872\n",
            "48/1000. test suite was not found: averaged covered neurons 0.872\n",
            "49/1000. test suite was not found: averaged covered neurons 0.874\n",
            "50/1000. test suite was not found: averaged covered neurons 0.874\n",
            "51/1000. test suite was not found: averaged covered neurons 0.874\n",
            "52/1000. test suite was not found: averaged covered neurons 0.874\n",
            "53/1000. test suite was not found: averaged covered neurons 0.874\n",
            "54/1000. test suite was not found: averaged covered neurons 0.874\n",
            "55/1000. test suite was not found: averaged covered neurons 0.874\n",
            "56/1000. test suite was not found: averaged covered neurons 0.874\n",
            "57/1000. test suite was not found: averaged covered neurons 0.878\n",
            "58/1000. test suite was not found: averaged covered neurons 0.878\n",
            "59/1000. test suite was not found: averaged covered neurons 0.878\n",
            "60/1000. test suite was not found: averaged covered neurons 0.878\n",
            "61/1000. test suite was not found: averaged covered neurons 0.878\n",
            "62/1000. test suite was not found: averaged covered neurons 0.878\n",
            "63/1000. test suite was not found: averaged covered neurons 0.878\n",
            "64/1000. test suite was not found: averaged covered neurons 0.878\n",
            "65/1000. test suite was not found: averaged covered neurons 0.878\n",
            "66/1000. test suite was not found: averaged covered neurons 0.878\n",
            "67/1000. test suite was not found: averaged covered neurons 0.878\n",
            "68/1000. test suite was not found: averaged covered neurons 0.880\n",
            "69/1000. test suite was not found: averaged covered neurons 0.880\n",
            "70/1000. test suite was not found: averaged covered neurons 0.880\n",
            "71/1000. test suite was not found: averaged covered neurons 0.880\n",
            "72/1000. test suite was not found: averaged covered neurons 0.880\n",
            "73/1000. test suite was not found: averaged covered neurons 0.880\n",
            "74/1000. test suite was not found: averaged covered neurons 0.880\n",
            "75/1000. test suite was not found: averaged covered neurons 0.880\n",
            "76/1000. test suite was not found: averaged covered neurons 0.880\n",
            "77/1000. test suite was not found: averaged covered neurons 0.880\n",
            "78/1000. test suite was not found: averaged covered neurons 0.880\n",
            "79/1000. test suite was not found: averaged covered neurons 0.880\n",
            "80/1000. test suite was not found: averaged covered neurons 0.880\n",
            "81/1000. test suite was not found: averaged covered neurons 0.882\n",
            "82/1000. test suite was not found: averaged covered neurons 0.885\n",
            "83/1000. test suite was not found: averaged covered neurons 0.885\n",
            "84/1000. test suite was not found: averaged covered neurons 0.885\n",
            "85/1000. test suite was not found: averaged covered neurons 0.885\n",
            "86/1000. test suite was not found: averaged covered neurons 0.885\n",
            "87/1000. test suite was not found: averaged covered neurons 0.885\n",
            "88/1000. test suite was not found: averaged covered neurons 0.885\n",
            "89/1000. test suite was not found: averaged covered neurons 0.885\n",
            "90/1000. test suite was not found: averaged covered neurons 0.885\n",
            "91/1000. test suite was not found: averaged covered neurons 0.885\n",
            "92/1000. test suite was not found: averaged covered neurons 0.885\n",
            "93/1000. test suite was not found: averaged covered neurons 0.885\n",
            "94/1000. test suite was not found: averaged covered neurons 0.885\n",
            "95/1000. test suite was not found: averaged covered neurons 0.885\n",
            "96/1000. test suite was not found: averaged covered neurons 0.885\n",
            "97/1000. test suite was not found: averaged covered neurons 0.887\n",
            "98/1000. test suite was not found: averaged covered neurons 0.887\n",
            "99/1000. test suite was not found: averaged covered neurons 0.887\n",
            "100/1000. test suite was not found: averaged covered neurons 0.887\n",
            "101/1000. test suite was not found: averaged covered neurons 0.887\n",
            "102/1000. test suite was not found: averaged covered neurons 0.887\n",
            "103/1000. test suite was not found: averaged covered neurons 0.887\n",
            "104/1000. test suite was not found: averaged covered neurons 0.891\n",
            "105/1000. test suite was not found: averaged covered neurons 0.891\n",
            "106/1000. test suite was not found: averaged covered neurons 0.891\n",
            "107/1000. test suite was not found: averaged covered neurons 0.891\n",
            "108/1000. test suite was not found: averaged covered neurons 0.891\n",
            "109/1000. test suite was not found: averaged covered neurons 0.891\n",
            "110/1000. test suite was not found: averaged covered neurons 0.891\n",
            "111/1000. test suite was not found: averaged covered neurons 0.891\n",
            "112/1000. test suite was not found: averaged covered neurons 0.891\n",
            "113/1000. test suite was not found: averaged covered neurons 0.891\n",
            "114/1000. test suite was not found: averaged covered neurons 0.891\n",
            "115/1000. test suite was not found: averaged covered neurons 0.891\n",
            "116/1000. test suite was not found: averaged covered neurons 0.891\n",
            "117/1000. test suite was not found: averaged covered neurons 0.891\n",
            "118/1000. test suite was not found: averaged covered neurons 0.891\n",
            "119/1000. test suite was not found: averaged covered neurons 0.893\n",
            "120/1000. test suite was not found: averaged covered neurons 0.893\n",
            "121/1000. test suite was not found: averaged covered neurons 0.893\n",
            "122/1000. test suite was not found: averaged covered neurons 0.893\n",
            "123/1000. test suite was not found: averaged covered neurons 0.893\n",
            "124/1000. test suite was not found: averaged covered neurons 0.893\n",
            "125/1000. test suite was not found: averaged covered neurons 0.893\n",
            "126/1000. test suite was not found: averaged covered neurons 0.893\n",
            "127/1000. test suite was not found: averaged covered neurons 0.893\n",
            "128/1000. test suite was not found: averaged covered neurons 0.893\n",
            "129/1000. test suite was not found: averaged covered neurons 0.893\n",
            "130/1000. test suite was not found: averaged covered neurons 0.893\n",
            "131/1000. test suite was not found: averaged covered neurons 0.893\n",
            "132/1000. test suite was not found: averaged covered neurons 0.893\n",
            "133/1000. test suite was not found: averaged covered neurons 0.893\n",
            "134/1000. test suite was not found: averaged covered neurons 0.893\n",
            "135/1000. test suite was not found: averaged covered neurons 0.893\n",
            "136/1000. test suite was not found: averaged covered neurons 0.893\n",
            "137/1000. test suite was not found: averaged covered neurons 0.893\n",
            "138/1000. test suite was not found: averaged covered neurons 0.893\n",
            "139/1000. test suite was not found: averaged covered neurons 0.893\n",
            "140/1000. test suite was not found: averaged covered neurons 0.893\n",
            "141/1000. test suite was not found: averaged covered neurons 0.893\n",
            "142/1000. test suite was not found: averaged covered neurons 0.893\n",
            "143/1000. test suite was not found: averaged covered neurons 0.893\n",
            "144/1000. test suite was not found: averaged covered neurons 0.893\n",
            "145/1000. test suite was not found: averaged covered neurons 0.893\n",
            "146/1000. test suite was not found: averaged covered neurons 0.893\n",
            "147/1000. test suite was not found: averaged covered neurons 0.893\n",
            "148/1000. test suite was not found: averaged covered neurons 0.893\n",
            "149/1000. test suite was not found: averaged covered neurons 0.893\n",
            "150/1000. test suite was not found: averaged covered neurons 0.893\n",
            "151/1000. test suite was not found: averaged covered neurons 0.893\n",
            "152/1000. test suite was not found: averaged covered neurons 0.893\n",
            "153/1000. test suite was not found: averaged covered neurons 0.893\n",
            "154/1000. test suite was not found: averaged covered neurons 0.893\n",
            "155/1000. test suite was not found: averaged covered neurons 0.893\n",
            "156/1000. test suite was not found: averaged covered neurons 0.893\n",
            "157/1000. test suite was not found: averaged covered neurons 0.893\n",
            "158/1000. test suite was not found: averaged covered neurons 0.893\n",
            "159/1000. test suite was not found: averaged covered neurons 0.893\n",
            "160/1000. test suite was not found: averaged covered neurons 0.893\n",
            "161/1000. test suite was not found: averaged covered neurons 0.893\n",
            "162/1000. test suite was not found: averaged covered neurons 0.893\n",
            "163/1000. test suite was not found: averaged covered neurons 0.893\n",
            "164/1000. test suite was not found: averaged covered neurons 0.893\n",
            "165/1000. test suite was not found: averaged covered neurons 0.893\n",
            "166/1000. test suite was not found: averaged covered neurons 0.893\n",
            "\u001b[92m167/1000. input already causes different outputs: 1, 1, 6\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a71yjeqUDVRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}