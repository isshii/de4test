{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "DeepXplore_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshii/de4test/blob/test_ozawa/DeepXplore/DeepXplore_03_13_coverage_trace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVFjZpUIRF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 共通で使うパスなどの定義\n",
        "# 共通の変数設定\n",
        "# 共通フォルダパス\n",
        "\n",
        "import datetime\n",
        "import pytz\n",
        "dt_now = datetime.datetime.now(pytz.timezone('Asia/Tokyo'))\n",
        "dt_str = str(dt_now.strftime('%Y%m%d_%H%M'))\n",
        "\n",
        "data_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data\"\n",
        "#data_imagenet = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet\"\n",
        "data_mnist = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/MNIST\"\n",
        "#data_imagenet_seeds = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/data/ImageNet/seeds\"\n",
        "model_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/model\"\n",
        "#output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output\"\n",
        "output_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/output/\" + dt_str\n",
        "tmp_dir = \"/content/gdrive/My Drive/ColabNotebooks/test4ai/tmp\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hv53U_OdAFYb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4lP4POGn5Xj",
        "colab_type": "code",
        "outputId": "74763f7f-4261-4dd2-a116-41e67e660100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "# Goggle Drive つなぐ\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3d3Suv3Q5Db",
        "colab_type": "code",
        "outputId": "752e9021-3346-4d6b-ba99-cce0e8ca6364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# outputフォルダ内容物のクリーンアップ削除\n",
        "!mkdir \"$output_dir\"\n",
        "!rm \"$output_dir\"/*"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200207_2033/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhdHYnLT0Nbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST')\n",
        "\n",
        "class bcolors:\n",
        "    HEADER = '\\033[95m'\n",
        "    OKBLUE = '\\033[94m'\n",
        "    OKGREEN = '\\033[92m'\n",
        "    WARNING = '\\033[93m'\n",
        "    FAIL = '\\033[91m'\n",
        "    ENDC = '\\033[0m'\n",
        "    BOLD = '\\033[1m'\n",
        "    UNDERLINE = '\\033[4m'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDY8sDVBqxmK",
        "colab_type": "code",
        "outputId": "91f24d60-4c72-45d1-b5d9-eecc7a5937ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        }
      },
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "pd.options.display.float_format = '{:.3f}'.format\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input\n",
        "import imageio\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Input, Dense, Activation, Flatten\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "import os\n",
        "\n",
        "# TensorFlowでGPUを使っているかのチェック：\n",
        "# \"device_type: \"GPU\" \" があればOK\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 1157001593669305694, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 17218810003021842865\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 11932830681606044812\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15956161332\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 2712451559209043255\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWQ2M_8glJNM",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "4f7bf524-c043-49c5-ded1-565ff2f5e4e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "# DeepXplore のパラメータ設定部\n",
        "# read the parameter\n",
        "# argument parsing\n",
        "parser = argparse.ArgumentParser(description='Main function for difference-inducing input generation in MNIST dataset')\n",
        "parser.add_argument('transformation', help=\"realistic transformation type\", choices=['light', 'occl', 'blackout'])\n",
        "parser.add_argument('weight_diff', help=\"weight hyperparm to control differential behavior\", type=float)\n",
        "parser.add_argument('weight_nc', help=\"weight hyperparm to control neuron coverage\", type=float)\n",
        "parser.add_argument('step', help=\"step size of gradient descent\", type=float)\n",
        "parser.add_argument('seeds', help=\"number of seeds of input\", type=int)\n",
        "parser.add_argument('grad_iterations', help=\"number of iterations of gradient descent\", type=int)\n",
        "parser.add_argument('threshold', help=\"threshold for determining neuron activated\", type=float)\n",
        "parser.add_argument('-t', '--target_model', help=\"target model that we want it predicts differently\",\n",
        "                    choices=[0, 1, 2], default=0, type=int)\n",
        "parser.add_argument('-sp', '--start_point', help=\"occlusion upper left corner coordinate\", default=(0, 0), type=tuple)\n",
        "parser.add_argument('-occl_size', '--occlusion_size', help=\"occlusion size\", default=(10, 10), type=tuple)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-occl_size', '--occlusion_size'], dest='occlusion_size', nargs=None, const=None, default=(10, 10), type=<class 'tuple'>, choices=None, help='occlusion size', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93KW-VGjMzun",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title ノイズのタイプ light / occl / blackout\n",
        "#@body {light,occl,blackout} weight_diff weight_nc step seeds\n",
        "#                   grad_iterations threshold\n",
        "noise_type = \"light\" #@param [\"light\", \"occl\", \"blackout\"]\n",
        "weight_diff = \"0.1\" #@param {type:\"string\"}\n",
        "weight_nc = \"0.1\" #@param {type:\"string\"}\n",
        "step = \"20\" #@param {type:\"string\"}　#50だとほぼ白飛び\n",
        "seeds = \"5\" #@param {type:\"string\"}\n",
        "grad_iterations = \"10\" #@param {type:\"string\"}\n",
        "threshold = \"0.1\" #@param {type:\"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szk80fCuPts9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = parser.parse_args([noise_type, weight_diff, weight_nc, step, seeds, grad_iterations, threshold])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tTYrllplJNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x.reshape(x.shape[1], x.shape[2])  # original shape (1,img_rows, img_cols,1)\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "def constraint_occl(gradients, start_point, rect_shape):\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def constraint_light(gradients):\n",
        "    new_grads = np.ones_like(gradients)\n",
        "    grad_mean = np.mean(gradients)\n",
        "    return grad_mean * new_grads\n",
        "\n",
        "\n",
        "def constraint_black(gradients, rect_shape=(6, 6)):\n",
        "    start_point = (\n",
        "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    if np.mean(patch) < 0:\n",
        "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def init_coverage_tables(model1, model2, model3):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    model_layer_dict2 = defaultdict(bool)\n",
        "    model_layer_dict3 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    init_dict(model2, model_layer_dict2)\n",
        "    init_dict(model3, model_layer_dict3)\n",
        "    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n",
        "\n",
        "\n",
        "def init_dict(model, model_layer_dict):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_dict[(layer.name, index)] = False\n",
        "\n",
        "\n",
        "def neuron_to_cover(model_layer_dict):\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "    else:\n",
        "        layer_name, index = random.choice(model_layer_dict.keys())\n",
        "    return layer_name, index\n",
        "\n",
        "\n",
        "def neuron_covered(model_layer_dict):\n",
        "    covered_neurons = len([v for v in model_layer_dict.values() if v])\n",
        "    total_neurons = len(model_layer_dict)\n",
        "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n",
        "\n",
        "\n",
        "def update_coverage(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n",
        "                model_layer_dict[(layer_names[i], num_neuron)] = True\n",
        "\n",
        "\n",
        "def full_coverage(model_layer_dict):\n",
        "    if False in model_layer_dict.values():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
        "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
        "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
        "    X_scaled = X_std * (rmax - rmin) + rmin\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def fired(model, layer_name, index, input_data, threshold=0):\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
        "    scaled = scale(intermediate_layer_output)\n",
        "    if np.mean(scaled[..., index]) > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def diverged(predictions1, predictions2, predictions3, target):\n",
        "    #     if predictions2 == predictions3 == target and predictions1 != target:\n",
        "    if not predictions1 == predictions2 == predictions3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "#add\n",
        "def list_pictures(directory, ext='jpg|jpeg|bmp|png|ppm'):\n",
        "    return [os.path.join(root, f)\n",
        "            for root, _, files in os.walk(directory) for f in files\n",
        "            if re.match(r'([\\w]+\\.(?:' + ext + '))', f.lower())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDDT47Er6u23",
        "colab_type": "code",
        "outputId": "b1271f28-921d-4694-81d4-22fe8aea5f1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        }
      },
      "source": [
        "'''\n",
        "LeNet-1\n",
        "'''\n",
        "def Model1(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        print(x_train.shape)\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(4, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(12, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "#        model.save_weights('./Model1.h5')\n",
        "        model.save_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "#        model.load_weights('./Model1.h5')\n",
        "        model.load_weights('/content/gdrive/My Drive/ColabNotebooks/test4ai/model/MNIST/Model1.h5')\n",
        "        print(bcolors.OKBLUE + 'Model1 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model1(train=True)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.4183 - acc: 0.8755 - val_loss: 0.1356 - val_acc: 0.9581\n",
            "\n",
            "\n",
            "Overall Test score: 0.13559644272699953\n",
            "Overall Test accuracy: 0.9581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU88awZf68JO",
        "colab_type": "code",
        "outputId": "bc55b90a-7153-4db2-fad9-c6f86710ab53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "'''\n",
        "LeNet-4\n",
        "'''\n",
        "def Model2(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(84, activation='relu', name='fc1')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model2.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model2.h5')\n",
        "        print(bcolors.OKBLUE + 'Model2 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model2(train=True)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 2s 28us/step - loss: 0.3600 - acc: 0.8878 - val_loss: 0.0874 - val_acc: 0.9730\n",
            "\n",
            "\n",
            "Overall Test score: 0.08737171518616378\n",
            "Overall Test accuracy: 0.973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufs67uRN7KYL",
        "colab_type": "code",
        "outputId": "faa467dc-689a-4d64-a1e3-9818caa244c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "'''\n",
        "LeNet-5\n",
        "'''\n",
        "def Model3(input_tensor=None, train=False):\n",
        "    nb_classes = 10\n",
        "    # convolution kernel size\n",
        "    kernel_size = (5, 5)\n",
        "\n",
        "    if train:\n",
        "        batch_size = 256\n",
        "        nb_epoch = 1\n",
        "\n",
        "        # input image dimensions\n",
        "        img_rows, img_cols = 28, 28\n",
        "\n",
        "        # the data, shuffled and split between train and test sets\n",
        "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "        input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        x_train /= 255\n",
        "        x_test /= 255\n",
        "\n",
        "        # convert class vectors to binary class matrices\n",
        "        y_train = to_categorical(y_train, nb_classes)\n",
        "        y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "        input_tensor = Input(shape=input_shape)\n",
        "    elif input_tensor is None:\n",
        "        print(bcolors.FAIL + 'you have to proved input_tensor when testing')\n",
        "        exit()\n",
        "\n",
        "    # block1\n",
        "    x = Convolution2D(6, kernel_size, activation='relu', padding='same', name='block1_conv1')(input_tensor)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block1_pool1')(x)\n",
        "\n",
        "    # block2\n",
        "    x = Convolution2D(16, kernel_size, activation='relu', padding='same', name='block2_conv1')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), name='block2_pool1')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        "    x = Dense(120, activation='relu', name='fc1')(x)\n",
        "    x = Dense(84, activation='relu', name='fc2')(x)\n",
        "    x = Dense(nb_classes, name='before_softmax')(x)\n",
        "    x = Activation('softmax', name='predictions')(x)\n",
        "\n",
        "    model = Model(input_tensor, x)\n",
        "\n",
        "    if train:\n",
        "        # compiling\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "\n",
        "        # trainig\n",
        "        model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=nb_epoch, verbose=1)\n",
        "        # save model\n",
        "        model.save_weights('./Model3.h5')\n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        print('\\n')\n",
        "        print('Overall Test score:', score[0])\n",
        "        print('Overall Test accuracy:', score[1])\n",
        "    else:\n",
        "        model.load_weights('./Model3.h5')\n",
        "        print(bcolors.OKBLUE + 'Model3 loaded' + bcolors.ENDC)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Model3(train=True)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.3895 - acc: 0.8737 - val_loss: 0.1186 - val_acc: 0.9634\n",
            "\n",
            "\n",
            "Overall Test score: 0.1186350061841309\n",
            "Overall Test accuracy: 0.9634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9QiwlW7WWR",
        "colab_type": "code",
        "outputId": "048d8346-092c-4326-bab3-00d6e7fb5796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# input MNIST image dimensions \n",
        "img_rows, img_cols = 28, 28\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "#define the model instance\n",
        "model1 = Model1(input_tensor=input_tensor)\n",
        "model2 = Model2(input_tensor=input_tensor)\n",
        "model3 = Model3(input_tensor=input_tensor)\n",
        "\n",
        "#define the dictionary of neuron coverage\n",
        "model_layer_dict1, model_layer_dict2, model_layer_dict3 = init_coverage_tables(model1, model2, model3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[94mModel1 loaded\u001b[0m\n",
            "\u001b[94mModel2 loaded\u001b[0m\n",
            "\u001b[94mModel3 loaded\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MS5H74Q4XA",
        "colab_type": "text"
      },
      "source": [
        "### 入力するデータの選別(1/2)\n",
        "\n",
        "０～９の数字をそれぞれ束ねてデータを準備する\n",
        "[00...011...1......99...9]。\n",
        "\n",
        "* test_per_fig_x: \n",
        "    各数字に対する画像データを格納\n",
        "* test_per_fig_y: \n",
        "    各数字に対する教師ラベルを格納\n",
        "* tests_x: \n",
        "    test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: \n",
        "    tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brFIdSfL8nHP",
        "colab_type": "code",
        "outputId": "e18bc754-e177-49e0-8e53-fec9a0ab1667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# load the MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "num_fig = 10\n",
        "test_per_fig_x = np.array([])\n",
        "test_per_fig_y = np.array([])\n",
        "tests_x = np.array([])\n",
        "tests_y = np.array([])\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "#x_train = x_train.astype('float')\n",
        "#x_test = x_test.astype('float')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "length = int(args.seeds)\n",
        "#length = int(args.seeds)\n",
        "for i in range(num_fig):\n",
        "  cond = [(x==i) for x in y_test]\n",
        "  test_per_fig_x = x_test[cond]\n",
        "  test_per_fig_y = y_test[cond]\n",
        "#  np.set_printoptions(formatter={'int': '{:07d}'.format})\n",
        "  print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "  tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "  tests_y = np.append(tests_y, test_per_fig_y[:length])\n",
        "#  conds = [conds, cond]\n",
        "print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "\n",
        "###### MNIST data, shuffled and split by train and test sets\n",
        "#####(_, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "###### modify the numpy data for the Keras model\n",
        "#####x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "#####input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "#tests_x.shape\n",
        "tests_x = tests_x.astype('float32')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure: 0 , shape: (980, 28, 28) , deviation: -20\n",
            "figure: 1 , shape: (1135, 28, 28) , deviation: 135\n",
            "figure: 2 , shape: (1032, 28, 28) , deviation: 32\n",
            "figure: 3 , shape: (1010, 28, 28) , deviation: 10\n",
            "figure: 4 , shape: (982, 28, 28) , deviation: -18\n",
            "figure: 5 , shape: (892, 28, 28) , deviation: -108\n",
            "figure: 6 , shape: (958, 28, 28) , deviation: -42\n",
            "figure: 7 , shape: (1028, 28, 28) , deviation: 28\n",
            "figure: 8 , shape: (974, 28, 28) , deviation: -26\n",
            "figure: 9 , shape: (1009, 28, 28) , deviation: 9\n",
            "check! 5.0 = 5 equal?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUOK_EjJW2KT",
        "colab_type": "text"
      },
      "source": [
        "## 入力するデータの選別(2/2)\n",
        "０～９の数字をそれぞれ束ねてデータを準備する [00...000]。\n",
        "\n",
        "* test_per_fig_x: 各数字に対する画像データを格納\n",
        "* test_per_fig_y: 各数字に対する教師ラベルを格納\n",
        "* tests_x: test_per_fig_xを数字ごとに格納：deepXploreコアコードのseedsに使う\n",
        "* tests_y: tests_xの正解ラベル"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "code",
        "id": "SLee__9UAD1Y",
        "colab": {}
      },
      "source": [
        "#@title デフォルトのタイトル テキスト\n",
        "#load the MNIST \n",
        "\n",
        "def create_data(start_fig, num_fi, length):\n",
        "  (datax_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  test_per_fig_x = np.array([])\n",
        "  test_per_fig_y = np.array([])\n",
        "  tests_x = np.array([])\n",
        "  tests_y = np.array([])\n",
        "\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test  = x_test.astype('float32')\n",
        "  x_train = x_train.astype('float')\n",
        "  x_test = x_test.astype('float')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "  #length = int(args.seeds/num_fig)\n",
        "  #length = int(args.seeds)\n",
        "  for i in range(start_fig, start_fig+num_fig):\n",
        "  #i=0\n",
        "    cond = [(x==i) for x in y_test]\n",
        "    test_per_fig_x = x_test[cond]\n",
        "    test_per_fig_y = y_test[cond]\n",
        "    print(\"figure:\", i, \", shape:\", test_per_fig_x.shape, \", deviation:\",test_per_fig_x.shape[0]-1000)\n",
        "    tests_x = np.append(tests_x, test_per_fig_x[:length])\n",
        "    tests_y = np.append(tests_y, test_per_fig_y[:length])  \n",
        "\n",
        "  #conds = [conds, cond]\n",
        "  print(\"check!\", tests_x.shape[0]/img_rows/img_cols/num_fig, \"=\", length, \"equal?\")\n",
        "  tests_x = tests_x.reshape(-1,img_rows, img_cols,1)\n",
        "  tests_x.shape\n",
        "  tests_x = tests_x.astype('float32')\n",
        "  return tests_x, tests_y, length\n",
        "  tests_x, tests_y, length = create_data(0, 10, int(args.seeds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyo2SSSz8vkT",
        "colab_type": "code",
        "outputId": "7aa01a12-00aa-47be-e008-238fce6cbcff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "print(tests_x.shape)\n",
        "print(tests_y)\n",
        "for i in range(0,1):\n",
        "  print(i)\n",
        "print(length)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50, 28, 28, 1)\n",
            "[0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 3. 3. 3. 3. 3. 4. 4. 4. 4.\n",
            " 4. 5. 5. 5. 5. 5. 6. 6. 6. 6. 6. 7. 7. 7. 7. 7. 8. 8. 8. 8. 8. 9. 9. 9.\n",
            " 9. 9.]\n",
            "0\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLlKIMxG9kEA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkfdTZi4mUPf",
        "colab_type": "code",
        "outputId": "49a3fb55-ca1f-4c38-9392-530812767130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "def neuron_output(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "    \n",
        "    out_list = []\n",
        "    out_list_scale = []\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            out_list.append(np.mean(intermediate_layer_output[..., num_neuron]))\n",
        "            out_list_scale.append(np.mean(scaled[..., num_neuron]))\n",
        "\n",
        "    return out_list, out_list_scale\n",
        "\n",
        "\n",
        "def deepXplore(model_layer_dict1, model_layer_dict2, model_layer_dict3, tests_x, model1, model2, model3, num_fig, start_fig, length):\n",
        "  #print(\"test01\")\n",
        "  count_already = 0\n",
        "  count_found = 0\n",
        "  count_not_found = 0\n",
        "  temp_per_nc1 = np.array([])\n",
        "  temp_per_nc2 = np.array([])\n",
        "  temp_per_nc3 = np.array([])\n",
        "  temp_num_nc1 = np.array([])\n",
        "  temp_num_nc2 = np.array([])\n",
        "  temp_num_nc3 = np.array([])\n",
        "  #print(\"test10\")\n",
        "\n",
        "  #for each neuron\n",
        "  num_neurons1 = neuron_covered(model_layer_dict1)[1]\n",
        "  num_neurons2 = neuron_covered(model_layer_dict2)[1]\n",
        "  num_neurons3 = neuron_covered(model_layer_dict3)[1]\n",
        "  #print(\"test11\")\n",
        "\n",
        "  column_tmp1 = list(model_layer_dict1.keys())\n",
        "  column_tmp2 = list(model_layer_dict2.keys())\n",
        "  column_tmp3 = list(model_layer_dict3.keys())\n",
        "  #print(\"test12\")\n",
        "  df1 = pd.DataFrame(columns=column_tmp1)\n",
        "  df2 = pd.DataFrame(columns=column_tmp2)\n",
        "  df3 = pd.DataFrame(columns=column_tmp3)\n",
        "  df1_scale = pd.DataFrame(columns=column_tmp1)\n",
        "  df2_scale = pd.DataFrame(columns=column_tmp2)\n",
        "  df3_scale = pd.DataFrame(columns=column_tmp3)\n",
        "  df1_trace = pd.DataFrame(columns=column_tmp1)\n",
        "  df2_trace = pd.DataFrame(columns=column_tmp2)\n",
        "  df3_trace = pd.DataFrame(columns=column_tmp3)\n",
        "  df2 = pd.DataFrame(columns=column_tmp2)\n",
        "  df3 = pd.DataFrame(columns=column_tmp3)\n",
        "  tmp_list = [\"already_diff\", \"found\", \"not_found\", \"layer1\", \"index1\", \"layer2\", \"index2\", \"layer3\", \"index3\"]\n",
        "  bug_result = pd.DataFrame(columns=tmp_list)\n",
        "  trial = 1\n",
        "  #print(\"test13\")\n",
        "  #print(\"test02\")\n",
        "  for index_fig in range(num_fig):\n",
        "    index_fig = index_fig + start_fig\n",
        "    print(\"figure\"+str(index_fig))\n",
        "    for _ in range(length):\n",
        "      #gen_img = np.expand_dims(random.choice(tests_x), axis=0)\n",
        "      gen_img = np.expand_dims(tests_x[(length*index_fig + _)], axis=0)\n",
        "      orig_img = gen_img.copy()\n",
        "      # first check if input already induces differences\n",
        "      label1, label2, label3 = np.argmax(model1.predict(gen_img)[0]), np.argmax(model2.predict(gen_img)[0]), np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "      if not label1 == label2 == label3:\n",
        "          count_already += 1\n",
        "          print(bcolors.OKGREEN + '   {}/{}. input already causes different outputs ({},{},{}) at({}, {}, {}): '.format(_, length, label1, label2, label3, count_already, count_found, count_not_found) + bcolors.ENDC)        \n",
        "\n",
        "          update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "\n",
        "          temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "          temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "          temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "          temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "          temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "          temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "                               \n",
        "          print(bcolors.OKGREEN + '     covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f'% (len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  neuron_covered(model_layer_dict2)[2], len(model_layer_dict3), neuron_covered(model_layer_dict3)[2]) + bcolors.ENDC)\n",
        "          averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                       neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +neuron_covered(model_layer_dict3)[1])\n",
        "          print(bcolors.OKGREEN + '     averaged covered neurons %.3f' % (averaged_nc) + bcolors.ENDC)\n",
        "\n",
        "          gen_img_deprocessed = deprocess_image(gen_img)\n",
        "\n",
        "          # save the result to disk\n",
        "          outputfilepath0 = os.path.join(output_dir, 'already_differ_' + str(label1) + '_' + str(label2) + '_' + str(label3) +'_['+ str(_) +  '].png')\n",
        "          imageio.imwrite(outputfilepath0, gen_img_deprocessed)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "          temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df1 = df1.append(temp)\n",
        "          df1_scale = df1_scale.append(temp_scale)\n",
        "##############################\n",
        "          df1_trace = df1_trace.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "          temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df2 = df2.append(temp)\n",
        "          df2_scale = df2_scale.append(temp_scale)\n",
        "##############################\n",
        "          df2_trace = df2_trace.append(temp_scale)\n",
        "\n",
        "          temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "          temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          df3 = df3.append(temp)\n",
        "          df3_scale = df3_scale.append(temp_scale)\n",
        "##############################\n",
        "          df3_trace = df3_trace.append(temp_scale)\n",
        "\n",
        "          #print(\"test10\")\n",
        "          temp = [1, 0, 0, None, None, None, None, None, None]\n",
        "          #print(\"test11\")\n",
        "          temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "          #print(\"test12\")\n",
        "          bug_result = bug_result.append(temp)\n",
        "          #print(\"test13\")\n",
        "          trial += 1\n",
        "          continue\n",
        "\n",
        "      # if all label agrees\n",
        "      orig_label = label1\n",
        "      layer_name1, index1 = neuron_to_cover(model_layer_dict1)\n",
        "      layer_name2, index2 = neuron_to_cover(model_layer_dict2)\n",
        "      layer_name3, index3 = neuron_to_cover(model_layer_dict3)\n",
        "\n",
        "\n",
        "\n",
        "      # construct joint loss function\n",
        "      if args.target_model == 0:\n",
        "        loss1 = -args.weight_diff * K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 1:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = -args.weight_diff * K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      elif args.target_model == 2:\n",
        "        loss1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss2 = K.mean(model2.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss3 = -args.weight_diff * K.mean(model3.get_layer('before_softmax').output[..., orig_label])\n",
        "      loss1_neuron = K.mean(model1.get_layer(layer_name1).output[..., index1])\n",
        "      loss2_neuron = K.mean(model2.get_layer(layer_name2).output[..., index2])\n",
        "      loss3_neuron = K.mean(model3.get_layer(layer_name3).output[..., index3])\n",
        "      layer_output = (loss1 + loss2 + loss3) + args.weight_nc * (loss1_neuron + loss2_neuron + loss3_neuron)\n",
        "\n",
        "      # for adversarial image generation\n",
        "      final_loss = K.mean(layer_output)\n",
        "\n",
        "      # we compute the gradient of the input picture wrt this loss\n",
        "      grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "\n",
        "      # this function returns the loss and grads given the input picture\n",
        "      iterate = K.function([input_tensor], [loss1, loss2, loss3, loss1_neuron, loss2_neuron, loss3_neuron, grads])\n",
        "\n",
        "      #print(\"test03\")\n",
        "      # we run gradient ascent for some steps\n",
        "      for iters in range(args.grad_iterations):\n",
        "        loss_value1, loss_value2, loss_value3, loss_neuron1, loss_neuron2, loss_neuron3, grads_value = iterate([gen_img])\n",
        "        if args.transformation == 'light':\n",
        "            grads_value = constraint_light(grads_value)  # constraint the gradients value\n",
        "        elif args.transformation == 'occl':\n",
        "            grads_value = constraint_occl(grads_value, args.start_point,\n",
        "                                          args.occlusion_size)  # constraint the gradients value\n",
        "        elif args.transformation == 'blackout':\n",
        "            grads_value = constraint_black(grads_value)  # constraint the gradients value\n",
        "\n",
        "        gen_img += grads_value * args.step\n",
        "        predictions1 = np.argmax(model1.predict(gen_img)[0])\n",
        "        predictions2 = np.argmax(model2.predict(gen_img)[0])\n",
        "        predictions3 = np.argmax(model3.predict(gen_img)[0])\n",
        "\n",
        "##############################\n",
        "        temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "        #print(temp)\n",
        "        temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "        temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial) + \"_\" + str(iters))\n",
        "        df1_trace = df1_trace.append(temp_scale)\n",
        "\n",
        "        temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "        temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "        temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial) + \"_\" + str(iters))\n",
        "        df2_trace = df2_trace.append(temp_scale)\n",
        "\n",
        "        temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "        temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "        temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial) + \"_\" + str(iters))\n",
        "        df3_trace = df3_trace.append(temp_scale)\n",
        "\n",
        "        #print(\"test04\")\n",
        "        if not predictions1 == predictions2 == predictions3:\n",
        "            #print(\"test04-01\")\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            count_found += 1\n",
        "            print(bcolors.OKBLUE + '%4d/%d. found at %d! covered neurons percentage %d neurons %.3f, %d neurons %.3f, %d neurons %.3f at (%d, %d, %d)'\n",
        "#                  % (_, args.seeds, iters, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                  % (_, length, iters + 1, len(model_layer_dict1), neuron_covered(model_layer_dict1)[2], len(model_layer_dict2),\n",
        "                     neuron_covered(model_layer_dict2)[2], len(model_layer_dict3),\n",
        "                     neuron_covered(model_layer_dict3)[2], count_already, count_found, count_not_found) + bcolors.ENDC)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(\n",
        "                neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            print(bcolors.OKBLUE + '     averaged covered neurons %.3f' % averaged_nc + bcolors.ENDC)\n",
        "\n",
        "            #print(\"test04-02\")\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            #print(\"test04-03\")\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  '].png')\n",
        "            #print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "#            outputfilepath2 = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '_orig.png')\n",
        "            outputfilepath2 = os.path.join(output_dir, args.transformation + '_from' + str(label1) + '_to' + str(predictions1) + '_'  + str(predictions2) + '_' + str(predictions3) +'_['+ str(_) +  ']_orig.png')\n",
        "            #print(outputfilepath2)\n",
        "            imageio.imwrite(outputfilepath2, orig_img_deprocessed)\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "            #print(\"test04-04\")\n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "     \n",
        "            #print(\"test04-05\")\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            #print(\"test04-06\")\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "            #print(\"test04-07\")\n",
        "            temp = [0, iters+1, 0, layer_name1, index1, layer_name2, index2,layer_name3, index3]\n",
        "            temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            bug_result = bug_result.append(temp)\n",
        "            trial += 1\n",
        "            break\n",
        "          \n",
        "          #add\n",
        "        #print(\"test05\")\n",
        "        if iters == (args.grad_iterations-1):\n",
        "            update_coverage(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            update_coverage(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            update_coverage(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            averaged_nc = (neuron_covered(model_layer_dict1)[0] + neuron_covered(model_layer_dict2)[0] +\n",
        "                           neuron_covered(model_layer_dict3)[0]) / float(neuron_covered(model_layer_dict1)[1] + neuron_covered(model_layer_dict2)[1] +\n",
        "                neuron_covered(model_layer_dict3)[1])\n",
        "            count_not_found += 1\n",
        "#              print('%4d/%d. test suite was not found: averaged covered neurons %.3f at %d/%d' % (_, args.seeds, averaged_nc, count_not_found, count_already + count_found + count_not_found))\n",
        "            print('%4d/%d. test suite was not found: averaged covered neurons %.3f at (%d, %d, %d)' % (_, length, averaged_nc, count_already, count_found, count_not_found))\n",
        "\n",
        "            gen_img_deprocessed = deprocess_image(gen_img)\n",
        "            #orig_img_deprocessed = deprocess_image(orig_img)\n",
        "\n",
        "            # save the result to disk\n",
        "            #outputfilepath = os.path.join(output_dir, args.transformation + '_' + str(label1) + '_' + str(label2) + '_' + str(label3) + '.png')\n",
        "            outputfilepath = os.path.join(output_dir, 'not_found_' + str(label1)+'_['+ str(_) + '].png')\n",
        "#           print(outputfilepath)\n",
        "            imageio.imwrite(outputfilepath, gen_img_deprocessed)\n",
        "\n",
        "            \n",
        "            temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "            temp = pd.Series(temp, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df1 = df1.append(temp)\n",
        "            df1_scale = df1_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model2, model_layer_dict2, args.threshold)\n",
        "            temp = pd.Series(temp, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df2.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            df2 = df2.append(temp)\n",
        "            df2_scale = df2_scale.append(temp_scale)\n",
        "\n",
        "            temp, temp_scale = neuron_output(gen_img, model3, model_layer_dict3, args.threshold)\n",
        "            temp = pd.Series(temp, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            temp_scale = pd.Series(temp_scale, index=df3.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "\n",
        "            df3 = df3.append(temp)\n",
        "            df3_scale = df3_scale.append(temp_scale)\n",
        "\n",
        "\n",
        "            temp = [0, 0, 1, layer_name1, index1, layer_name2, index2,layer_name3, index3]\n",
        "            temp = pd.Series(temp, index=bug_result.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "            bug_result = bug_result.append(temp)\n",
        "\n",
        "            trial += 1\n",
        "            \n",
        "            #break\n",
        "\n",
        "            temp_per_nc1=np.append(temp_per_nc1,  neuron_covered(model_layer_dict1)[2])\n",
        "            temp_per_nc2=np.append(temp_per_nc2,  neuron_covered(model_layer_dict2)[2])\n",
        "            temp_per_nc3=np.append(temp_per_nc3,  neuron_covered(model_layer_dict3)[2])\n",
        "            temp_num_nc1=np.append(temp_num_nc1,  neuron_covered(model_layer_dict1)[0])\n",
        "            temp_num_nc2=np.append(temp_num_nc2,  neuron_covered(model_layer_dict2)[0])\n",
        "            temp_num_nc3=np.append(temp_num_nc3,  neuron_covered(model_layer_dict3)[0])\n",
        "\n",
        "  #print(\"test06\")\n",
        "  temp_per_nc1=temp_per_nc1.reshape(num_fig, length)\n",
        "  temp_per_nc2=temp_per_nc2.reshape(num_fig, length)\n",
        "  temp_per_nc3=temp_per_nc3.reshape(num_fig, length)\n",
        "  temp_num_nc1=temp_num_nc1.reshape(num_fig, length)\n",
        "  temp_num_nc2=temp_num_nc2.reshape(num_fig, length)\n",
        "  temp_num_nc3=temp_num_nc3.reshape(num_fig, length)\n",
        "\n",
        "  #print(\"test07\")\n",
        "#  return df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result\n",
        "  return df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result, df1_trace"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 6.91 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHi1ffJuWH5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "4feacddb-96ed-4c03-cdbe-780a87608ceb"
      },
      "source": [
        "print(length, num_fig)\n",
        "print(output_dir)\n",
        "column_tmp1 = list(model_layer_dict1.keys())\n",
        "df1 = pd.DataFrame(columns=column_tmp1)\n",
        "df1.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 10\n",
            "/content/gdrive/My Drive/ColabNotebooks/test4ai/output/20200207_2033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(block1_conv1, 0)</th>\n",
              "      <th>(block1_conv1, 1)</th>\n",
              "      <th>(block1_conv1, 2)</th>\n",
              "      <th>(block1_conv1, 3)</th>\n",
              "      <th>(block1_pool1, 0)</th>\n",
              "      <th>(block1_pool1, 1)</th>\n",
              "      <th>(block1_pool1, 2)</th>\n",
              "      <th>(block1_pool1, 3)</th>\n",
              "      <th>(block2_conv1, 0)</th>\n",
              "      <th>(block2_conv1, 1)</th>\n",
              "      <th>(block2_conv1, 2)</th>\n",
              "      <th>(block2_conv1, 3)</th>\n",
              "      <th>(block2_conv1, 4)</th>\n",
              "      <th>(block2_conv1, 5)</th>\n",
              "      <th>(block2_conv1, 6)</th>\n",
              "      <th>(block2_conv1, 7)</th>\n",
              "      <th>(block2_conv1, 8)</th>\n",
              "      <th>(block2_conv1, 9)</th>\n",
              "      <th>(block2_conv1, 10)</th>\n",
              "      <th>(block2_conv1, 11)</th>\n",
              "      <th>(block2_pool1, 0)</th>\n",
              "      <th>(block2_pool1, 1)</th>\n",
              "      <th>(block2_pool1, 2)</th>\n",
              "      <th>(block2_pool1, 3)</th>\n",
              "      <th>(block2_pool1, 4)</th>\n",
              "      <th>(block2_pool1, 5)</th>\n",
              "      <th>(block2_pool1, 6)</th>\n",
              "      <th>(block2_pool1, 7)</th>\n",
              "      <th>(block2_pool1, 8)</th>\n",
              "      <th>(block2_pool1, 9)</th>\n",
              "      <th>(block2_pool1, 10)</th>\n",
              "      <th>(block2_pool1, 11)</th>\n",
              "      <th>(before_softmax, 0)</th>\n",
              "      <th>(before_softmax, 1)</th>\n",
              "      <th>(before_softmax, 2)</th>\n",
              "      <th>(before_softmax, 3)</th>\n",
              "      <th>(before_softmax, 4)</th>\n",
              "      <th>(before_softmax, 5)</th>\n",
              "      <th>(before_softmax, 6)</th>\n",
              "      <th>(before_softmax, 7)</th>\n",
              "      <th>(before_softmax, 8)</th>\n",
              "      <th>(before_softmax, 9)</th>\n",
              "      <th>(predictions, 0)</th>\n",
              "      <th>(predictions, 1)</th>\n",
              "      <th>(predictions, 2)</th>\n",
              "      <th>(predictions, 3)</th>\n",
              "      <th>(predictions, 4)</th>\n",
              "      <th>(predictions, 5)</th>\n",
              "      <th>(predictions, 6)</th>\n",
              "      <th>(predictions, 7)</th>\n",
              "      <th>(predictions, 8)</th>\n",
              "      <th>(predictions, 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [(block1_conv1, 0), (block1_conv1, 1), (block1_conv1, 2), (block1_conv1, 3), (block1_pool1, 0), (block1_pool1, 1), (block1_pool1, 2), (block1_pool1, 3), (block2_conv1, 0), (block2_conv1, 1), (block2_conv1, 2), (block2_conv1, 3), (block2_conv1, 4), (block2_conv1, 5), (block2_conv1, 6), (block2_conv1, 7), (block2_conv1, 8), (block2_conv1, 9), (block2_conv1, 10), (block2_conv1, 11), (block2_pool1, 0), (block2_pool1, 1), (block2_pool1, 2), (block2_pool1, 3), (block2_pool1, 4), (block2_pool1, 5), (block2_pool1, 6), (block2_pool1, 7), (block2_pool1, 8), (block2_pool1, 9), (block2_pool1, 10), (block2_pool1, 11), (before_softmax, 0), (before_softmax, 1), (before_softmax, 2), (before_softmax, 3), (before_softmax, 4), (before_softmax, 5), (before_softmax, 6), (before_softmax, 7), (before_softmax, 8), (before_softmax, 9), (predictions, 0), (predictions, 1), (predictions, 2), (predictions, 3), (predictions, 4), (predictions, 5), (predictions, 6), (predictions, 7), (predictions, 8), (predictions, 9)]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSSBfoAZreWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4422c512-bb09-46de-ff8a-799e9cd356ba"
      },
      "source": [
        "index_fig = 0\n",
        "trial = 0\n",
        "gen_img = np.expand_dims(tests_x[(length*index_fig + 0)], axis=0)\n",
        "temp, temp_scale = neuron_output(gen_img, model1, model_layer_dict1, args.threshold)\n",
        "temp_scale = pd.Series(temp_scale, index=df1.columns, name=str(index_fig) + \"_\" + str(trial))\n",
        "print(type(temp_scale))\n",
        "temp_scale.name"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0_0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yymgs3ldNiDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de6e1f19-e2e6-4c73-c2d8-41c21ddf3ef8"
      },
      "source": [
        "%%time\n",
        "#num_fig = 1\n",
        "#start_fig = 0\n",
        "#deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "\n",
        "num_fig = 1\n",
        "for start_fig in range(10):\n",
        "#for start_fig in range(2):\n",
        "#  df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result, df1_trace, df2_trace. df3_trace= deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "  df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result, df1_trace= deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "#  df1, df2, df3, df1_scale, df2_scale, df3_scale, bug_result= deepXplore(model_layer_dict1=model_layer_dict1, model_layer_dict2=model_layer_dict2, model_layer_dict3=model_layer_dict3, tests_x=tests_x, model1=model1, model2=model2, model3=model3, num_fig = num_fig, start_fig=start_fig, length=length)\n",
        "  df1_scale.to_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")\n",
        "  bug_result.to_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_index.csv\")\n",
        "  df1_trace.to_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_trace.csv\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "figure0\n",
            "\u001b[94m   0/5. found at 2! covered neurons percentage 52 neurons 0.596, 148 neurons 0.446, 268 neurons 0.414 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.444\u001b[0m\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.596, 148 neurons 0.473, 268 neurons 0.474 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.487\u001b[0m\n",
            "\u001b[94m   2/5. found at 3! covered neurons percentage 52 neurons 0.808, 148 neurons 0.655, 268 neurons 0.713 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.705\u001b[0m\n",
            "\u001b[94m   3/5. found at 3! covered neurons percentage 52 neurons 0.808, 148 neurons 0.669, 268 neurons 0.724 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.716\u001b[0m\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.827, 148 neurons 0.709, 268 neurons 0.743 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.741\u001b[0m\n",
            "figure1\n",
            "\u001b[94m   0/5. found at 2! covered neurons percentage 52 neurons 0.827, 148 neurons 0.716, 268 neurons 0.750 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.748\u001b[0m\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.846, 148 neurons 0.736, 268 neurons 0.754 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.759\u001b[0m\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.846, 148 neurons 0.736, 268 neurons 0.754 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.759\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.865, 148 neurons 0.743, 268 neurons 0.817 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.799\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.865, 148 neurons 0.743, 268 neurons 0.817 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.799\u001b[0m\n",
            "figure2\n",
            "\u001b[94m   0/5. found at 3! covered neurons percentage 52 neurons 0.865, 148 neurons 0.743, 268 neurons 0.817 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.799\u001b[0m\n",
            "\u001b[94m   1/5. found at 3! covered neurons percentage 52 neurons 0.865, 148 neurons 0.743, 268 neurons 0.821 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.801\u001b[0m\n",
            "\u001b[94m   2/5. found at 3! covered neurons percentage 52 neurons 0.865, 148 neurons 0.743, 268 neurons 0.821 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.801\u001b[0m\n",
            "\u001b[94m   3/5. found at 3! covered neurons percentage 52 neurons 0.865, 148 neurons 0.743, 268 neurons 0.825 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.803\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.865, 148 neurons 0.750, 268 neurons 0.825 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.806\u001b[0m\n",
            "figure3\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.885, 148 neurons 0.777, 268 neurons 0.843 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.827\u001b[0m\n",
            "\u001b[94m   1/5. found at 5! covered neurons percentage 52 neurons 0.885, 148 neurons 0.777, 268 neurons 0.847 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.829\u001b[0m\n",
            "\u001b[94m   2/5. found at 3! covered neurons percentage 52 neurons 0.885, 148 neurons 0.777, 268 neurons 0.847 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.829\u001b[0m\n",
            "\u001b[94m   3/5. found at 1! covered neurons percentage 52 neurons 0.885, 148 neurons 0.784, 268 neurons 0.847 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.831\u001b[0m\n",
            "\u001b[94m   4/5. found at 3! covered neurons percentage 52 neurons 0.885, 148 neurons 0.784, 268 neurons 0.847 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.831\u001b[0m\n",
            "figure4\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.904, 148 neurons 0.804, 268 neurons 0.854 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.844\u001b[0m\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.804, 268 neurons 0.854 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.844\u001b[0m\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.804, 268 neurons 0.854 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.844\u001b[0m\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.804, 268 neurons 0.854 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.844\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.904, 148 neurons 0.804, 268 neurons 0.854 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.844\u001b[0m\n",
            "figure5\n",
            "\u001b[94m   0/5. found at 7! covered neurons percentage 52 neurons 0.904, 148 neurons 0.804, 268 neurons 0.862 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.848\u001b[0m\n",
            "\u001b[94m   1/5. found at 3! covered neurons percentage 52 neurons 0.904, 148 neurons 0.804, 268 neurons 0.862 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.848\u001b[0m\n",
            "\u001b[94m   2/5. found at 3! covered neurons percentage 52 neurons 0.904, 148 neurons 0.804, 268 neurons 0.862 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.848\u001b[0m\n",
            "\u001b[94m   3/5. found at 3! covered neurons percentage 52 neurons 0.904, 148 neurons 0.804, 268 neurons 0.862 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.848\u001b[0m\n",
            "\u001b[94m   4/5. found at 3! covered neurons percentage 52 neurons 0.904, 148 neurons 0.804, 268 neurons 0.862 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.848\u001b[0m\n",
            "figure6\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.818, 268 neurons 0.869 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.859\u001b[0m\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.923, 148 neurons 0.818, 268 neurons 0.869 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.859\u001b[0m\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.818, 268 neurons 0.869 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.859\u001b[0m\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.818, 268 neurons 0.869 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.859\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.818, 268 neurons 0.869 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.859\u001b[0m\n",
            "figure7\n",
            "\u001b[94m   0/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.818, 268 neurons 0.869 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.859\u001b[0m\n",
            "\u001b[94m   1/5. found at 2! covered neurons percentage 52 neurons 0.923, 148 neurons 0.818, 268 neurons 0.869 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.859\u001b[0m\n",
            "\u001b[94m   2/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.818, 268 neurons 0.877 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.865\u001b[0m\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.818, 268 neurons 0.877 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.865\u001b[0m\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.942, 148 neurons 0.824, 268 neurons 0.881 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.870\u001b[0m\n",
            "figure8\n",
            "\u001b[94m   0/5. found at 4! covered neurons percentage 52 neurons 0.942, 148 neurons 0.824, 268 neurons 0.881 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.870\u001b[0m\n",
            "   1/5. test suite was not found: averaged covered neurons 0.874 at (0, 1, 1)\n",
            "\u001b[94m   2/5. found at 3! covered neurons percentage 52 neurons 0.942, 148 neurons 0.838, 268 neurons 0.881 at (0, 2, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.874\u001b[0m\n",
            "\u001b[94m   3/5. found at 3! covered neurons percentage 52 neurons 0.942, 148 neurons 0.838, 268 neurons 0.881 at (0, 3, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.874\u001b[0m\n",
            "\u001b[94m   4/5. found at 2! covered neurons percentage 52 neurons 0.942, 148 neurons 0.838, 268 neurons 0.881 at (0, 4, 1)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.874\u001b[0m\n",
            "figure9\n",
            "\u001b[94m   0/5. found at 1! covered neurons percentage 52 neurons 0.962, 148 neurons 0.858, 268 neurons 0.884 at (0, 1, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.885\u001b[0m\n",
            "\u001b[94m   1/5. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.858, 268 neurons 0.884 at (0, 2, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.887\u001b[0m\n",
            "\u001b[94m   2/5. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.858, 268 neurons 0.884 at (0, 3, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.887\u001b[0m\n",
            "\u001b[94m   3/5. found at 2! covered neurons percentage 52 neurons 0.981, 148 neurons 0.865, 268 neurons 0.884 at (0, 4, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.889\u001b[0m\n",
            "\u001b[94m   4/5. found at 1! covered neurons percentage 52 neurons 0.981, 148 neurons 0.865, 268 neurons 0.888 at (0, 5, 0)\u001b[0m\n",
            "\u001b[94m     averaged covered neurons 0.891\u001b[0m\n",
            "CPU times: user 4min 36s, sys: 1.31 s, total: 4min 38s\n",
            "Wall time: 4min 38s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpsI1IOM-hUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "ceff0a64-332a-4406-b58e-6310cf95bcab"
      },
      "source": [
        "df2_scale = pd.DataFrame()\n",
        "df3_scale = pd.DataFrame()\n",
        "df4_scale = pd.DataFrame()\n",
        "for start_fig in range(10):\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_neuron.csv\")\n",
        "  df2_scale = pd.concat([df2_scale, df1_scale])\n",
        "\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_index.csv\")\n",
        "  df3_scale = pd.concat([df3_scale, df1_scale])\n",
        "\n",
        "  df1_scale = pd.read_csv(output_dir+ \"/../01_fig\" + str(start_fig) + \"_seed\" + str(length) +\"_trace.csv\")\n",
        "  df4_scale = pd.concat([df4_scale, df1_scale])\n",
        "\n",
        "tmp_list = list(df2_scale.iloc[:,0])\n",
        "df2_scale.index = tmp_list\n",
        "print(df2_scale.iloc[:,1:].head())\n",
        "\n",
        "tmp_list = list(df3_scale.iloc[:,0])\n",
        "df3_scale.index = tmp_list\n",
        "print(df3_scale.iloc[:,1:].head())\n",
        "\n",
        "tmp_list = list(df4_scale.iloc[:,0])\n",
        "df4_scale.index = tmp_list\n",
        "print(df4_scale.iloc[:,1:].head())\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     ('block1_conv1', 0)  ...  ('predictions', 9)\n",
            "0_1                0.124  ...               0.000\n",
            "0_2                0.114  ...               0.000\n",
            "0_3                0.094  ...               0.000\n",
            "0_4                0.096  ...               0.000\n",
            "0_5                0.079  ...               0.000\n",
            "\n",
            "[5 rows x 52 columns]\n",
            "     already_diff  found  not_found  ... index2        layer3 index3\n",
            "0_1             0      2          0  ...      1  block2_pool1      1\n",
            "0_2             0      2          0  ...      7  block1_pool1      5\n",
            "0_3             0      3          0  ...      3           fc1     48\n",
            "0_4             0      3          0  ...     23           fc1     28\n",
            "0_5             0      1          0  ...      9           fc2     78\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "       ('block1_conv1', 0)  ...  ('predictions', 9)\n",
            "0_1_0                0.071  ...               0.000\n",
            "0_1_1                0.124  ...               0.000\n",
            "0_2_0                0.063  ...               0.000\n",
            "0_2_1                0.114  ...               0.000\n",
            "0_3_0                0.079  ...               0.000\n",
            "\n",
            "[5 rows x 52 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QtyLAZiPeKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "6c6afa8f-c2cf-477b-bf67-cc22ac8537b6"
      },
      "source": [
        "df2_scale.iloc[0:6,1:]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>('block1_conv1', 0)</th>\n",
              "      <th>('block1_conv1', 1)</th>\n",
              "      <th>('block1_conv1', 2)</th>\n",
              "      <th>('block1_conv1', 3)</th>\n",
              "      <th>('block1_pool1', 0)</th>\n",
              "      <th>('block1_pool1', 1)</th>\n",
              "      <th>('block1_pool1', 2)</th>\n",
              "      <th>('block1_pool1', 3)</th>\n",
              "      <th>('block2_conv1', 0)</th>\n",
              "      <th>('block2_conv1', 1)</th>\n",
              "      <th>('block2_conv1', 2)</th>\n",
              "      <th>('block2_conv1', 3)</th>\n",
              "      <th>('block2_conv1', 4)</th>\n",
              "      <th>('block2_conv1', 5)</th>\n",
              "      <th>('block2_conv1', 6)</th>\n",
              "      <th>('block2_conv1', 7)</th>\n",
              "      <th>('block2_conv1', 8)</th>\n",
              "      <th>('block2_conv1', 9)</th>\n",
              "      <th>('block2_conv1', 10)</th>\n",
              "      <th>('block2_conv1', 11)</th>\n",
              "      <th>('block2_pool1', 0)</th>\n",
              "      <th>('block2_pool1', 1)</th>\n",
              "      <th>('block2_pool1', 2)</th>\n",
              "      <th>('block2_pool1', 3)</th>\n",
              "      <th>('block2_pool1', 4)</th>\n",
              "      <th>('block2_pool1', 5)</th>\n",
              "      <th>('block2_pool1', 6)</th>\n",
              "      <th>('block2_pool1', 7)</th>\n",
              "      <th>('block2_pool1', 8)</th>\n",
              "      <th>('block2_pool1', 9)</th>\n",
              "      <th>('block2_pool1', 10)</th>\n",
              "      <th>('block2_pool1', 11)</th>\n",
              "      <th>('before_softmax', 0)</th>\n",
              "      <th>('before_softmax', 1)</th>\n",
              "      <th>('before_softmax', 2)</th>\n",
              "      <th>('before_softmax', 3)</th>\n",
              "      <th>('before_softmax', 4)</th>\n",
              "      <th>('before_softmax', 5)</th>\n",
              "      <th>('before_softmax', 6)</th>\n",
              "      <th>('before_softmax', 7)</th>\n",
              "      <th>('before_softmax', 8)</th>\n",
              "      <th>('before_softmax', 9)</th>\n",
              "      <th>('predictions', 0)</th>\n",
              "      <th>('predictions', 1)</th>\n",
              "      <th>('predictions', 2)</th>\n",
              "      <th>('predictions', 3)</th>\n",
              "      <th>('predictions', 4)</th>\n",
              "      <th>('predictions', 5)</th>\n",
              "      <th>('predictions', 6)</th>\n",
              "      <th>('predictions', 7)</th>\n",
              "      <th>('predictions', 8)</th>\n",
              "      <th>('predictions', 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_1</th>\n",
              "      <td>0.124</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.195</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.178</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.272</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.823</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.970</td>\n",
              "      <td>0.661</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.604</td>\n",
              "      <td>0.498</td>\n",
              "      <td>0.410</td>\n",
              "      <td>0.382</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_2</th>\n",
              "      <td>0.114</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.264</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.197</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.162</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.131</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.303</td>\n",
              "      <td>0.319</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.336</td>\n",
              "      <td>0.160</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.272</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.661</td>\n",
              "      <td>0.023</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.589</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.957</td>\n",
              "      <td>0.495</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.602</td>\n",
              "      <td>0.243</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_3</th>\n",
              "      <td>0.094</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.477</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.508</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.246</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.282</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.303</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.291</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.101</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.265</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_4</th>\n",
              "      <td>0.096</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.553</td>\n",
              "      <td>0.495</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.593</td>\n",
              "      <td>0.529</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.269</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.152</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.332</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.308</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.910</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.608</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.197</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_5</th>\n",
              "      <td>0.079</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.383</td>\n",
              "      <td>0.338</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.231</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.271</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.141</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.128</td>\n",
              "      <td>0.707</td>\n",
              "      <td>0.453</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.444</td>\n",
              "      <td>0.366</td>\n",
              "      <td>0.442</td>\n",
              "      <td>0.779</td>\n",
              "      <td>0.377</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_1</th>\n",
              "      <td>0.092</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.521</td>\n",
              "      <td>0.466</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.547</td>\n",
              "      <td>0.493</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.250</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.291</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.306</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.194</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.489</td>\n",
              "      <td>0.706</td>\n",
              "      <td>0.675</td>\n",
              "      <td>0.307</td>\n",
              "      <td>0.231</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.309</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ('block1_conv1', 0)  ...  ('predictions', 9)\n",
              "0_1                0.124  ...               0.000\n",
              "0_2                0.114  ...               0.000\n",
              "0_3                0.094  ...               0.000\n",
              "0_4                0.096  ...               0.000\n",
              "0_5                0.079  ...               0.000\n",
              "1_1                0.092  ...               0.000\n",
              "\n",
              "[6 rows x 52 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ1DCqKX9lTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tmp_list = list(df2_scale.iloc[:,0])\n",
        "#df2_scale = df2_scale.rename(index=tmp_list)\n",
        "type(tmp_list)\n",
        "df2_scale.index = tmp_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiVj9Wjd-rVv",
        "colab_type": "text"
      },
      "source": [
        "以下が、df1_scaleに格納されているニューロンの出力値から発火/非発火の集計をとるアルゴリズム"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4B19EsPFQrak",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "f6b085e3-5623-46ac-eba9-6fd4aaae920a"
      },
      "source": [
        "import pylab as pl\n",
        "\n",
        "thres = 0.1\n",
        "bools = df2_scale.iloc[:,1:] > thres\n",
        "\n",
        "bools.sum().hist()\n",
        "pl.xlabel(\"activation time\")\n",
        "pl.ylabel(\"number of neurons\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'number of neurons')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEGCAYAAABiq/5QAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGmSlrMm7g6-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "6cda5f0e-a73c-4d2c-fdbe-8f7d5fe87cf4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#print(test.index[0],test.index[0][0])\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(12, 8))\n",
        "bools.sum().plot(kind=\"bar\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3e9acbaef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAIwCAYAAACP/wPjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVI1/n9R21KYlSW8agAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPP1-qoLB_b-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "fc465352-0fd3-467a-b6ec-d1b008d64072"
      },
      "source": [
        "bools = df3_scale.iloc[:,1:4] > 0\n",
        "bools.sum().plot(kind=\"bar\")\n",
        "#bools.sum()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3e9ab51fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEpCAYAAACKmHkAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDXKApekRv0/vdnBgJ15NjIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz77SyGDD2L2",
        "colab_type": "text"
      },
      "source": [
        "フィルタ処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z06piFAVDDOK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e966528-8f2b-49b5-d7c4-4c5ecc0db89d"
      },
      "source": [
        "import glob\n",
        "img_list = glob.glob(output_dir + \"/l*].png\")\n",
        "print(img_list[0][79:80])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HqwPQNxDp_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0e3e715e-2e4e-467f-f9a9-e67c911a1c51"
      },
      "source": [
        "from PIL import Image\n",
        "im = np.array(Image.open(img_list[0]))\n",
        "np.max(im), np.min(im)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRueeE7hEUg4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "8e5eaa09-d25e-48e4-cbb9-17179eb29ba6"
      },
      "source": [
        "im_dev =np.array([])\n",
        "im_bug = pd.DataFrame(columns=[\"fig\"])\n",
        "im_all = pd.DataFrame(columns=[\"fig\"])\n",
        "index = 0\n",
        "\n",
        "for i in img_list:\n",
        "  im = np.array(Image.open(i))  \n",
        "  tmp = np.max(im)-np.min(im)\n",
        "  im_dev = np.append(im_dev, tmp)\n",
        "  if tmp > 50:\n",
        "    im_bug.loc[str(index)] = i[79:80]\n",
        "  im_all.loc[str(index)] = i[79:80]\n",
        "  index += 1\n",
        "\n",
        "plt.hist(im_dev)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([36.,  3.,  1.,  1.,  5.,  1.,  0.,  1.,  0.,  1.]),\n",
              " array([  0. ,  16.9,  33.8,  50.7,  67.6,  84.5, 101.4, 118.3, 135.2,\n",
              "        152.1, 169. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD4CAYAAAD4k815AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUS1749kHv90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "c722f434-af81-48c5-ffe9-0d3e0b115ac2"
      },
      "source": [
        "im_all[\"fig\"].value_counts().sort_index().plot(kind=\"bar\")\n",
        "#im_bug[\"fig\"].value_counts().sort_index().plot(kind=\"bar\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3e9aabf208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD1CAYAAAB5n7/BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLUnNGWpJas5QS1JzhlqSmvsfd6mgKFhcUzUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ISWtgVLQTSQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "a314d467-e862-4da6-ee7f-be78d80bb020"
      },
      "source": [
        "im_bug[\"fig\"].value_counts().sort_index().plot(kind=\"bar\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3e9ab26ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD1CAYAAABeMT4pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60e_7IL1JM8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "274e30c9-b3f0-403c-b6e1-9fe5683ea324"
      },
      "source": [
        "im_all"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fig</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fig\n",
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "5    1\n",
              "6    1\n",
              "7    1\n",
              "8    1\n",
              "9    1\n",
              "10   2\n",
              "11   2\n",
              "12   2\n",
              "13   2\n",
              "14   2\n",
              "15   3\n",
              "16   3\n",
              "17   3\n",
              "18   3\n",
              "19   3\n",
              "20   4\n",
              "21   4\n",
              "22   4\n",
              "23   4\n",
              "24   4\n",
              "25   5\n",
              "26   5\n",
              "27   5\n",
              "28   5\n",
              "29   5\n",
              "30   6\n",
              "31   6\n",
              "32   6\n",
              "33   6\n",
              "34   6\n",
              "35   7\n",
              "36   7\n",
              "37   7\n",
              "38   7\n",
              "39   7\n",
              "40   8\n",
              "41   8\n",
              "42   8\n",
              "43   8\n",
              "44   9\n",
              "45   9\n",
              "46   9\n",
              "47   9\n",
              "48   9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGHvvsEkJzXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fig = plt.figure()\n",
        "#ax = fig.add_subplot(1,1,1)\n",
        "#x1 = im_all[\"fig\"].value_counts()\n",
        "#x2 = im_bug[\"fig\"].value_counts()\n",
        "#ax.hist([x1, x2], bins=10, normed=True, color=['red', 'blue', 'green'], label=['x1', 'x2', 'x3'])\n",
        "#ax.set_title('seventh histogram $\\mu1=100,\\ \\sigma1=15,\\ \\mu2=50,\\ \\sigma2=4$')\n",
        "#ax.set_xlabel('x')\n",
        "#ax.set_ylabel('freq')\n",
        "#ax.legend(loc='upper left')\n",
        "#fig.show()\n",
        "#\n",
        "#x1, x2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxkrXvEuLOjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "177091e9-a2fb-4982-b742-31d8b30eaba0"
      },
      "source": [
        "x1 = im_all[\"fig\"].value_counts()\n",
        "x1, type(x1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1    5\n",
              " 7    5\n",
              " 4    5\n",
              " 6    5\n",
              " 9    5\n",
              " 5    5\n",
              " 0    5\n",
              " 2    5\n",
              " 3    5\n",
              " 8    4\n",
              " Name: fig, dtype: int64, pandas.core.series.Series)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ7spEzhOy10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(x1, x2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK8sscq8LTWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "969b97b2-2a4f-4309-f33f-f3856d840dc9"
      },
      "source": [
        "x1.values"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcJirLEsbtHB",
        "colab_type": "text"
      },
      "source": [
        "以下は実際に各画像でどのくらい\"薄い\"画像があるかの分布"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAwybx1JLkn8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "01d2aff2-4052-46da-f12b-54301b7ad71e"
      },
      "source": [
        "im_dev =np.array([])\n",
        "im_bug = pd.DataFrame(columns=[\"filter\", \"org\"])\n",
        "index = 0\n",
        "\n",
        "for i in img_list:\n",
        "  im = np.array(Image.open(i))  \n",
        "  tmp = np.max(im)-np.min(im)\n",
        "  im_dev = np.append(im_dev, tmp)\n",
        "  if tmp > 50:\n",
        "    im_bug.loc[str(index)] = [i[79:80], i[79:80]]\n",
        "  else:\n",
        "    im_bug.loc[str(index)] = [None, i[79:80]]\n",
        "#  im_all.loc[str(index)] = i[79:80]\n",
        "  index += 1\n",
        "\n",
        "plt.hist(im_dev)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([36.,  3.,  1.,  1.,  5.,  1.,  0.,  1.,  0.,  1.]),\n",
              " array([  0. ,  16.9,  33.8,  50.7,  67.6,  84.5, 101.4, 118.3, 135.2,\n",
              "        152.1, 169. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD4CAYAAAD4k815AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ_waU1DMtUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c16afdd-5032-467e-9389-4205e2318eb6"
      },
      "source": [
        "im_bug"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filter</th>\n",
              "      <th>org</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>None</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>None</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>None</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>None</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>None</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>None</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>None</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>None</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>None</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>None</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>None</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>None</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>None</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>None</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>None</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>None</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>None</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>None</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>None</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>None</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>None</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>None</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>None</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   filter org\n",
              "0    None   0\n",
              "1    None   0\n",
              "2    None   0\n",
              "3    None   0\n",
              "4       0   0\n",
              "5    None   1\n",
              "6    None   1\n",
              "7    None   1\n",
              "8       1   1\n",
              "9    None   1\n",
              "10   None   2\n",
              "11   None   2\n",
              "12   None   2\n",
              "13   None   2\n",
              "14   None   2\n",
              "15      3   3\n",
              "16   None   3\n",
              "17   None   3\n",
              "18   None   3\n",
              "19   None   3\n",
              "20      4   4\n",
              "21   None   4\n",
              "22   None   4\n",
              "23   None   4\n",
              "24   None   4\n",
              "25   None   5\n",
              "26   None   5\n",
              "27   None   5\n",
              "28   None   5\n",
              "29   None   5\n",
              "30      6   6\n",
              "31      6   6\n",
              "32   None   6\n",
              "33   None   6\n",
              "34   None   6\n",
              "35   None   7\n",
              "36   None   7\n",
              "37      7   7\n",
              "38   None   7\n",
              "39      7   7\n",
              "40   None   8\n",
              "41   None   8\n",
              "42   None   8\n",
              "43   None   8\n",
              "44      9   9\n",
              "45   None   9\n",
              "46   None   9\n",
              "47   None   9\n",
              "48   None   9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwBNwplZM3UI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "6c371c1a-a260-4563-af1c-a16673b792e9"
      },
      "source": [
        "print(im_bug[\"filter\"].value_counts().sort_index())\n",
        "print(im_bug[\"org\"].value_counts().sort_index())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    1\n",
            "1    1\n",
            "3    1\n",
            "4    1\n",
            "6    2\n",
            "7    2\n",
            "9    1\n",
            "Name: filter, dtype: int64\n",
            "0    5\n",
            "1    5\n",
            "2    5\n",
            "3    5\n",
            "4    5\n",
            "5    5\n",
            "6    5\n",
            "7    5\n",
            "8    4\n",
            "9    5\n",
            "Name: org, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWbPrxZRNBZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a928d291-8b22-4984-961d-79ed1e1a9d4f"
      },
      "source": [
        "type(im_all[\"fig\"])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lncXwCAINPAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#im_bug[\"filter\"].value_counts().sort_index().merge(im_bug[\"org\"].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obbro2n3ONLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "outputId": "c8833f67-caa3-487d-f62e-582aa8806126"
      },
      "source": [
        "df4_scale.head(20)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>('block1_conv1', 0)</th>\n",
              "      <th>('block1_conv1', 1)</th>\n",
              "      <th>('block1_conv1', 2)</th>\n",
              "      <th>('block1_conv1', 3)</th>\n",
              "      <th>('block1_pool1', 0)</th>\n",
              "      <th>('block1_pool1', 1)</th>\n",
              "      <th>('block1_pool1', 2)</th>\n",
              "      <th>('block1_pool1', 3)</th>\n",
              "      <th>('block2_conv1', 0)</th>\n",
              "      <th>('block2_conv1', 1)</th>\n",
              "      <th>('block2_conv1', 2)</th>\n",
              "      <th>('block2_conv1', 3)</th>\n",
              "      <th>('block2_conv1', 4)</th>\n",
              "      <th>('block2_conv1', 5)</th>\n",
              "      <th>('block2_conv1', 6)</th>\n",
              "      <th>('block2_conv1', 7)</th>\n",
              "      <th>('block2_conv1', 8)</th>\n",
              "      <th>('block2_conv1', 9)</th>\n",
              "      <th>('block2_conv1', 10)</th>\n",
              "      <th>('block2_conv1', 11)</th>\n",
              "      <th>('block2_pool1', 0)</th>\n",
              "      <th>('block2_pool1', 1)</th>\n",
              "      <th>('block2_pool1', 2)</th>\n",
              "      <th>('block2_pool1', 3)</th>\n",
              "      <th>('block2_pool1', 4)</th>\n",
              "      <th>('block2_pool1', 5)</th>\n",
              "      <th>('block2_pool1', 6)</th>\n",
              "      <th>('block2_pool1', 7)</th>\n",
              "      <th>('block2_pool1', 8)</th>\n",
              "      <th>('block2_pool1', 9)</th>\n",
              "      <th>('block2_pool1', 10)</th>\n",
              "      <th>('block2_pool1', 11)</th>\n",
              "      <th>('before_softmax', 0)</th>\n",
              "      <th>('before_softmax', 1)</th>\n",
              "      <th>('before_softmax', 2)</th>\n",
              "      <th>('before_softmax', 3)</th>\n",
              "      <th>('before_softmax', 4)</th>\n",
              "      <th>('before_softmax', 5)</th>\n",
              "      <th>('before_softmax', 6)</th>\n",
              "      <th>('before_softmax', 7)</th>\n",
              "      <th>('before_softmax', 8)</th>\n",
              "      <th>('before_softmax', 9)</th>\n",
              "      <th>('predictions', 0)</th>\n",
              "      <th>('predictions', 1)</th>\n",
              "      <th>('predictions', 2)</th>\n",
              "      <th>('predictions', 3)</th>\n",
              "      <th>('predictions', 4)</th>\n",
              "      <th>('predictions', 5)</th>\n",
              "      <th>('predictions', 6)</th>\n",
              "      <th>('predictions', 7)</th>\n",
              "      <th>('predictions', 8)</th>\n",
              "      <th>('predictions', 9)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_1_0</th>\n",
              "      <td>0_1_0</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.266</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.320</td>\n",
              "      <td>0.278</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.018</td>\n",
              "      <td>0.197</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.195</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.164</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.387</td>\n",
              "      <td>0.475</td>\n",
              "      <td>0.427</td>\n",
              "      <td>0.439</td>\n",
              "      <td>0.460</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_1_1</th>\n",
              "      <td>0_1_1</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.196</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.260</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.337</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.266</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.245</td>\n",
              "      <td>0.816</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.972</td>\n",
              "      <td>0.663</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.601</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.408</td>\n",
              "      <td>0.382</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.704</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_2_0</th>\n",
              "      <td>0_2_0</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.199</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.256</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.144</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.724</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.616</td>\n",
              "      <td>0.607</td>\n",
              "      <td>0.239</td>\n",
              "      <td>0.735</td>\n",
              "      <td>0.427</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_2_1</th>\n",
              "      <td>0_2_1</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.205</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.198</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.321</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.341</td>\n",
              "      <td>0.163</td>\n",
              "      <td>0.303</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.275</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.229</td>\n",
              "      <td>0.651</td>\n",
              "      <td>0.027</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.588</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.957</td>\n",
              "      <td>0.492</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.596</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.008</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_3_0</th>\n",
              "      <td>0_3_0</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.020</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.292</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.379</td>\n",
              "      <td>0.337</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.212</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.309</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.327</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.146</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.450</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.393</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.451</td>\n",
              "      <td>0.773</td>\n",
              "      <td>0.521</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.009</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_3_1</th>\n",
              "      <td>0_3_1</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.134</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.235</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.194</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.806</td>\n",
              "      <td>0.407</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.783</td>\n",
              "      <td>0.504</td>\n",
              "      <td>0.590</td>\n",
              "      <td>0.713</td>\n",
              "      <td>0.652</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.081</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_3_2</th>\n",
              "      <td>0_3_2</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.534</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.568</td>\n",
              "      <td>0.508</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.114</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.216</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.281</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.303</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.291</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.211</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.188</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.266</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_4_0</th>\n",
              "      <td>0_4_0</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.279</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.377</td>\n",
              "      <td>0.328</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.171</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.029</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.131</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.226</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.248</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.326</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.261</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.174</td>\n",
              "      <td>0.209</td>\n",
              "      <td>0.160</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.596</td>\n",
              "      <td>0.214</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.418</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.611</td>\n",
              "      <td>0.319</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_4_1</th>\n",
              "      <td>0_4_1</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.170</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.167</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.197</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.136</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.234</td>\n",
              "      <td>0.228</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.347</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.279</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.127</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.220</td>\n",
              "      <td>0.242</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.908</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.869</td>\n",
              "      <td>0.755</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.574</td>\n",
              "      <td>0.331</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.120</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_4_2</th>\n",
              "      <td>0_4_2</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.554</td>\n",
              "      <td>0.494</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.594</td>\n",
              "      <td>0.528</td>\n",
              "      <td>0.166</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.243</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.269</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.244</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.152</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.333</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.309</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.182</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.910</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.609</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.187</td>\n",
              "      <td>0.199</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_5_0</th>\n",
              "      <td>0_5_0</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.331</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.384</td>\n",
              "      <td>0.336</td>\n",
              "      <td>0.122</td>\n",
              "      <td>0.047</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.229</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.143</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.131</td>\n",
              "      <td>0.711</td>\n",
              "      <td>0.456</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.449</td>\n",
              "      <td>0.368</td>\n",
              "      <td>0.446</td>\n",
              "      <td>0.779</td>\n",
              "      <td>0.381</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_1_0</th>\n",
              "      <td>1_1_0</td>\n",
              "      <td>0.059</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.058</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.069</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.082</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.065</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.219</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.201</td>\n",
              "      <td>0.289</td>\n",
              "      <td>0.243</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_1_1</th>\n",
              "      <td>1_1_1</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.522</td>\n",
              "      <td>0.465</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.548</td>\n",
              "      <td>0.492</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.042</td>\n",
              "      <td>0.231</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.251</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.233</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.078</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.290</td>\n",
              "      <td>0.023</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.306</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.106</td>\n",
              "      <td>0.488</td>\n",
              "      <td>0.708</td>\n",
              "      <td>0.677</td>\n",
              "      <td>0.307</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.086</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.311</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_2_0</th>\n",
              "      <td>1_2_0</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.048</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.100</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.044</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.155</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.472</td>\n",
              "      <td>0.337</td>\n",
              "      <td>0.316</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.157</td>\n",
              "      <td>0.476</td>\n",
              "      <td>0.387</td>\n",
              "      <td>0.268</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_2_1</th>\n",
              "      <td>1_2_1</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.533</td>\n",
              "      <td>0.474</td>\n",
              "      <td>0.124</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.562</td>\n",
              "      <td>0.507</td>\n",
              "      <td>0.147</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.240</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.231</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.108</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.208</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.290</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.296</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.203</td>\n",
              "      <td>0.146</td>\n",
              "      <td>0.098</td>\n",
              "      <td>0.492</td>\n",
              "      <td>0.929</td>\n",
              "      <td>0.706</td>\n",
              "      <td>0.385</td>\n",
              "      <td>0.282</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.006</td>\n",
              "      <td>0.415</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.261</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_3_0</th>\n",
              "      <td>1_3_0</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.052</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.041</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.066</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.061</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.149</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.088</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.173</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.119</td>\n",
              "      <td>0.110</td>\n",
              "      <td>0.138</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.828</td>\n",
              "      <td>0.581</td>\n",
              "      <td>0.302</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.297</td>\n",
              "      <td>0.191</td>\n",
              "      <td>0.285</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.001</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.283</td>\n",
              "      <td>0.046</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_3_1</th>\n",
              "      <td>1_3_1</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.531</td>\n",
              "      <td>0.473</td>\n",
              "      <td>0.121</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.562</td>\n",
              "      <td>0.503</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.230</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.253</td>\n",
              "      <td>0.050</td>\n",
              "      <td>0.237</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.218</td>\n",
              "      <td>0.068</td>\n",
              "      <td>0.299</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.003</td>\n",
              "      <td>0.308</td>\n",
              "      <td>0.089</td>\n",
              "      <td>0.327</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.207</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.361</td>\n",
              "      <td>0.787</td>\n",
              "      <td>0.619</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.252</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.238</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.056</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_4_0</th>\n",
              "      <td>1_4_0</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.158</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.022</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.030</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.111</td>\n",
              "      <td>0.139</td>\n",
              "      <td>0.043</td>\n",
              "      <td>0.159</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.062</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.092</td>\n",
              "      <td>0.169</td>\n",
              "      <td>0.225</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.263</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.083</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.115</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.161</td>\n",
              "      <td>0.247</td>\n",
              "      <td>0.786</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.687</td>\n",
              "      <td>0.293</td>\n",
              "      <td>0.841</td>\n",
              "      <td>0.416</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.005</td>\n",
              "      <td>0.228</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.116</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.335</td>\n",
              "      <td>0.017</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_5_0</th>\n",
              "      <td>1_5_0</td>\n",
              "      <td>0.071</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.026</td>\n",
              "      <td>0.033</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.049</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.076</td>\n",
              "      <td>0.073</td>\n",
              "      <td>0.084</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.040</td>\n",
              "      <td>0.094</td>\n",
              "      <td>0.091</td>\n",
              "      <td>0.072</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.133</td>\n",
              "      <td>0.138</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.142</td>\n",
              "      <td>0.087</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.150</td>\n",
              "      <td>0.113</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.093</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.634</td>\n",
              "      <td>0.697</td>\n",
              "      <td>0.294</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.238</td>\n",
              "      <td>0.464</td>\n",
              "      <td>0.477</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.155</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_5_1</th>\n",
              "      <td>1_5_1</td>\n",
              "      <td>0.096</td>\n",
              "      <td>0.016</td>\n",
              "      <td>0.526</td>\n",
              "      <td>0.468</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.556</td>\n",
              "      <td>0.498</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.039</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.001</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.045</td>\n",
              "      <td>0.227</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.140</td>\n",
              "      <td>0.112</td>\n",
              "      <td>0.074</td>\n",
              "      <td>0.215</td>\n",
              "      <td>0.067</td>\n",
              "      <td>0.300</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.002</td>\n",
              "      <td>0.298</td>\n",
              "      <td>0.077</td>\n",
              "      <td>0.308</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.189</td>\n",
              "      <td>0.148</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.336</td>\n",
              "      <td>0.735</td>\n",
              "      <td>0.557</td>\n",
              "      <td>0.370</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.221</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.271</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ('block1_conv1', 0)  ...  ('predictions', 8)  ('predictions', 9)\n",
              "0_1_0      0_1_0                0.071  ...               0.000               0.000\n",
              "0_1_1      0_1_1                0.124  ...               0.001               0.000\n",
              "0_2_0      0_2_0                0.063  ...               0.002               0.000\n",
              "0_2_1      0_2_1                0.114  ...               0.008               0.000\n",
              "0_3_0      0_3_0                0.079  ...               0.009               0.000\n",
              "0_3_1      0_3_1                0.099  ...               0.024               0.011\n",
              "0_3_2      0_3_2                0.094  ...               1.000               0.000\n",
              "0_4_0      0_4_0                0.077  ...               0.000               0.000\n",
              "0_4_1      0_4_1                0.110  ...               0.001               0.000\n",
              "0_4_2      0_4_2                0.096  ...               1.000               0.000\n",
              "0_5_0      0_5_0                0.080  ...               0.006               0.000\n",
              "1_1_0      1_1_0                0.059  ...               0.003               0.000\n",
              "1_1_1      1_1_1                0.092  ...               1.000               0.000\n",
              "1_2_0      1_2_0                0.061  ...               0.001               0.000\n",
              "1_2_1      1_2_1                0.099  ...               1.000               0.000\n",
              "1_3_0      1_3_0                0.090  ...               0.005               0.000\n",
              "1_3_1      1_3_1                0.097  ...               1.000               0.000\n",
              "1_4_0      1_4_0                0.092  ...               0.013               0.000\n",
              "1_5_0      1_5_0                0.071  ...               0.039               0.008\n",
              "1_5_1      1_5_1                0.096  ...               1.000               0.000\n",
              "\n",
              "[20 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5HVngqRZ4GY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}