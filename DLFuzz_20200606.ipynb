{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLFuzz_20200606.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1FeF4gUkZlQAs_FORRdKt4H30KAVWIQRE",
      "authorship_tag": "ABX9TyNevftAUGyZ0oyKjlnaS69P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isshii/de4test/blob/master/DLFuzz_20200606.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utcvtc4YvE90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://qiita.com/kurilab/items/f6f4374d7b1980060de7"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGmkOtEOxh1P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e9900afa-6a96-4082-bbde-fe5ef03e35b9"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/DLFuzz/DLFuzz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/DLFuzz/DLFuzz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogCfVGnhxvNy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "5ef33aa6-094f-4e46-acb0-47e5cc8f0a89"
      },
      "source": [
        "#初回のみ\n",
        "#!git clone https://github.com/turned2670/DLFuzz.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DLFuzz'...\n",
            "remote: Enumerating objects: 182, done.\u001b[K\n",
            "remote: Total 182 (delta 0), reused 0 (delta 0), pack-reused 182\u001b[K\n",
            "Receiving objects: 100% (182/182), 5.47 MiB | 17.23 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9T3iruWyJ9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git config --global user.email \"n.koyama777@gmail.com\"\n",
        "!git config --global user.name \"inty1129\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIvAEqEmSj8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e6e04249-5551-4904-c738-772e7c73830c"
      },
      "source": [
        "!pip install imageio"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC83JcQVyc1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from keras import backend as K\n",
        "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "\n",
        "model_layer_weights_top_k = []\n",
        "\n",
        "def preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(28, 28), grayscale=True)\n",
        "    input_img_data = image.img_to_array(img)\n",
        "    input_img_data = input_img_data.reshape(1, 28, 28, 1)\n",
        "\n",
        "    input_img_data = input_img_data.astype('float32')\n",
        "    input_img_data /= 255\n",
        "    # input_img_data = preprocess_input(input_img_data)  # final input shape = (1,224,224,3)\n",
        "    return input_img_data\n",
        "\n",
        "\n",
        "def deprocess_image(x):\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x.reshape(x.shape[1], x.shape[2])  # original shape (1,img_rows, img_cols,1)\n",
        "\n",
        "\n",
        "def decode_label(pred):\n",
        "    return decode_predictions(pred)[0][0][1]\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "\n",
        "def constraint_occl(gradients, start_point, rect_shape):\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "    start_point[1]:start_point[1] + rect_shape[1]] = gradients[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "                                                     start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def constraint_light(gradients):\n",
        "    new_grads = np.ones_like(gradients)\n",
        "    grad_mean = 1e4 * np.mean(gradients)\n",
        "    return grad_mean * new_grads\n",
        "\n",
        "\n",
        "def constraint_black(gradients, rect_shape=(10, 10)):\n",
        "    start_point = (\n",
        "        random.randint(0, gradients.shape[1] - rect_shape[0]), random.randint(0, gradients.shape[2] - rect_shape[1]))\n",
        "    new_grads = np.zeros_like(gradients)\n",
        "    patch = gradients[:, start_point[0]:start_point[0] + rect_shape[0], start_point[1]:start_point[1] + rect_shape[1]]\n",
        "    if np.mean(patch) < 0:\n",
        "        new_grads[:, start_point[0]:start_point[0] + rect_shape[0],\n",
        "        start_point[1]:start_point[1] + rect_shape[1]] = -np.ones_like(patch)\n",
        "    return new_grads\n",
        "\n",
        "\n",
        "def init_coverage_tables(model1, model2, model3):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    model_layer_dict2 = defaultdict(bool)\n",
        "    model_layer_dict3 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    init_dict(model2, model_layer_dict2)\n",
        "    init_dict(model3, model_layer_dict3)\n",
        "    return model_layer_dict1, model_layer_dict2, model_layer_dict3\n",
        "\n",
        "def init_coverage_tables(model1):\n",
        "    model_layer_dict1 = defaultdict(bool)\n",
        "    init_dict(model1, model_layer_dict1)\n",
        "    return model_layer_dict1\n",
        "\n",
        "\n",
        "def init_dict(model, model_layer_dict):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_dict[(layer.name, index)] = False\n",
        "\n",
        "def init_coverage_times(model):\n",
        "    model_layer_times = defaultdict(int)\n",
        "    init_times(model,model_layer_times)\n",
        "    return model_layer_times\n",
        "\n",
        "def init_coverage_value(model):\n",
        "    model_layer_value = defaultdict(float)\n",
        "    init_times(model, model_layer_value)\n",
        "    return model_layer_value\n",
        "\n",
        "def init_times(model,model_layer_times):\n",
        "    for layer in model.layers:\n",
        "        if 'flatten' in layer.name or 'input' in layer.name:\n",
        "            continue\n",
        "        for index in range(layer.output_shape[-1]):\n",
        "            model_layer_times[(layer.name, index)] = 0\n",
        "\n",
        "def neuron_to_cover(model_layer_dict):\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_dict.items() if not v]\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "    else:\n",
        "        layer_name, index = random.choice(model_layer_dict.keys())\n",
        "    return layer_name, index\n",
        "\n",
        "\n",
        "def neuron_to_cover(not_covered,model_layer_dict):\n",
        "    if not_covered:\n",
        "        layer_name, index = random.choice(not_covered)\n",
        "        not_covered.remove((layer_name, index))\n",
        "    else:\n",
        "        layer_name, index = random.choice(model_layer_dict.keys())\n",
        "    return layer_name, index\n",
        "\n",
        "def random_strategy(model,model_layer_times, neuron_to_cover_num):\n",
        "    loss_neuron = []\n",
        "    not_covered = [(layer_name, index) for (layer_name, index), v in model_layer_times.items() if v == 0]\n",
        "\n",
        "    #for _ in xrange(neuron_to_cover_num):\n",
        "    for _ in range(neuron_to_cover_num):\n",
        "        layer_name, index = neuron_to_cover(not_covered, model_layer_times)\n",
        "        loss00_neuron = K.mean(model.get_layer(layer_name).output[..., index])\n",
        "        # if loss_neuron == 0:\n",
        "        #     loss_neuron = loss00_neuron\n",
        "        # else:\n",
        "        #     loss_neuron += loss00_neuron\n",
        "        # loss_neuron += loss1_neuron\n",
        "        loss_neuron.append(loss00_neuron)\n",
        "    return loss_neuron\n",
        "\n",
        "def neuron_select_high_weight(model, layer_names, top_k):\n",
        "    global model_layer_weights_top_k\n",
        "    model_layer_weights_dict = {}\n",
        "    for layer_name in layer_names:\n",
        "        weights = model.get_layer(layer_name).get_weights()\n",
        "        if len(weights) <= 0:\n",
        "            continue\n",
        "        w = np.asarray(weights[0])  # 0 is weights, 1 is biases\n",
        "        w = w.reshape(w.shape)\n",
        "        for index in range(model.get_layer(layer_name).output_shape[-1]):\n",
        "            index_w = np.mean(w[..., index])\n",
        "            if index_w <= 0:\n",
        "                continue\n",
        "            model_layer_weights_dict[(layer_name,index)]=index_w\n",
        "    # notice!\n",
        "    model_layer_weights_list = sorted(model_layer_weights_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    k = 0\n",
        "    for (layer_name, index),weight in model_layer_weights_list:\n",
        "        if k >= top_k:\n",
        "            break\n",
        "        model_layer_weights_top_k.append([layer_name,index])\n",
        "        k += 1\n",
        "\n",
        "\n",
        "def neuron_selection(model, model_layer_times, model_layer_value, neuron_select_strategy, neuron_to_cover_num, threshold):\n",
        "    if neuron_select_strategy == 'None':\n",
        "        return random_strategy(model, model_layer_times, neuron_to_cover_num)\n",
        "    #######エラー発生\n",
        "    #num_strategy = len([x for x in neuron_select_strategy if x in ['0', '1', '2', '3']])\n",
        "    num_strategy = params[0][0]\n",
        "    neuron_to_cover_num_each = neuron_to_cover_num / num_strategy\n",
        "\n",
        "    loss_neuron = []\n",
        "    # initialization for strategies\n",
        "    if ('0' in list(neuron_select_strategy)) or ('1' in list(neuron_select_strategy)):\n",
        "        i = 0\n",
        "        neurons_covered_times = []\n",
        "        neurons_key_pos = {}\n",
        "        for (layer_name, index), time in model_layer_times.items():\n",
        "            neurons_covered_times.append(time)\n",
        "            neurons_key_pos[i] = (layer_name, index)\n",
        "            i += 1\n",
        "        neurons_covered_times = np.asarray(neurons_covered_times)\n",
        "        times_total = sum(neurons_covered_times)\n",
        "\n",
        "    # select neurons covered often\n",
        "    if '0' in list(neuron_select_strategy):\n",
        "        if times_total == 0:\n",
        "            return random_strategy(model, model_layer_times, 1)#The beginning of no neurons covered\n",
        "        neurons_covered_percentage = neurons_covered_times / float(times_total)\n",
        "        # num_neuron0 = np.random.choice(range(len(neurons_covered_times)), p=neurons_covered_percentage)\n",
        "        num_neuron0 = np.random.choice(range(len(neurons_covered_times)), neuron_to_cover_num_each, replace=False, p=neurons_covered_percentage)\n",
        "        for num in num_neuron0:\n",
        "            layer_name0, index0 = neurons_key_pos[num]\n",
        "            loss0_neuron = K.mean(model.get_layer(layer_name0).output[..., index0])\n",
        "            loss_neuron.append(loss0_neuron)\n",
        "\n",
        "    # select neurons covered rarely\n",
        "    if '1' in list(neuron_select_strategy):\n",
        "        if times_total == 0:\n",
        "            return random_strategy(model, model_layer_times, 1)\n",
        "        neurons_covered_times_inverse = np.subtract(max(neurons_covered_times), neurons_covered_times)\n",
        "        neurons_covered_percentage_inverse = neurons_covered_times_inverse / float(sum(neurons_covered_times_inverse))\n",
        "        # num_neuron1 = np.random.choice(range(len(neurons_covered_times)), p=neurons_covered_percentage_inverse)\n",
        "        num_neuron1 = np.random.choice(range(len(neurons_covered_times)), neuron_to_cover_num_each, replace=False,\n",
        "                                       p=neurons_covered_percentage_inverse)\n",
        "        for num in num_neuron1:\n",
        "            layer_name1, index1 = neurons_key_pos[num]\n",
        "            loss1_neuron = K.mean(model.get_layer(layer_name1).output[..., index1])\n",
        "            loss_neuron.append(loss1_neuron)\n",
        "\n",
        "    # select neurons with largest weights (feature maps with largest filter weights)\n",
        "    if '2' in list(neuron_select_strategy):\n",
        "        layer_names = [layer.name for layer in model.layers if\n",
        "                       'flatten' not in layer.name and 'input' not in layer.name]\n",
        "        k = 0.1\n",
        "        top_k = k * len(model_layer_times)  # number of neurons to be selected within\n",
        "        global model_layer_weights_top_k\n",
        "        if len(model_layer_weights_top_k) == 0:\n",
        "            neuron_select_high_weight(model, layer_names, top_k)  # Set the value\n",
        "\n",
        "        num_neuron2 = np.random.choice(range(len(model_layer_weights_top_k)), neuron_to_cover_num_each, replace=False)\n",
        "        for i in num_neuron2:\n",
        "            # i = np.random.choice(range(len(model_layer_weights_top_k)))\n",
        "            layer_name2 = model_layer_weights_top_k[i][0]\n",
        "            index2 = model_layer_weights_top_k[i][1]\n",
        "            loss2_neuron = K.mean(model.get_layer(layer_name2).output[..., index2])\n",
        "            loss_neuron.append(loss2_neuron)\n",
        "\n",
        "    if '3' in list(neuron_select_strategy):\n",
        "        above_threshold = []\n",
        "        below_threshold = []\n",
        "        above_num = neuron_to_cover_num_each / 2\n",
        "        below_num = neuron_to_cover_num_each - above_num\n",
        "        above_i = 0\n",
        "        below_i = 0\n",
        "        for (layer_name, index), value in model_layer_value.items():\n",
        "            if threshold + 0.25 > value > threshold and layer_name != 'fc1' and layer_name != 'fc2' and \\\n",
        "                    layer_name != 'predictions' and layer_name != 'fc1000' and layer_name != 'before_softmax' \\\n",
        "                    and above_i < above_num:\n",
        "                above_threshold.append([layer_name, index])\n",
        "                above_i += 1\n",
        "                # print(layer_name,index,value)\n",
        "                # above_threshold_dict[(layer_name, index)]=value\n",
        "            elif threshold > value > threshold - 0.2 and layer_name != 'fc1' and layer_name != 'fc2' and \\\n",
        "                    layer_name != 'predictions' and layer_name != 'fc1000' and layer_name != 'before_softmax' \\\n",
        "                    and below_i < below_num:\n",
        "                below_threshold.append([layer_name, index])\n",
        "                below_i += 1\n",
        "        #\n",
        "        # loss3_neuron_above = 0\n",
        "        # loss3_neuron_below = 0\n",
        "        loss_neuron = []\n",
        "        if len(above_threshold) > 0:\n",
        "            for above_item in range(len(above_threshold)):\n",
        "                loss_neuron.append(K.mean(\n",
        "                    model.get_layer(above_threshold[above_item][0]).output[..., above_threshold[above_item][1]]))\n",
        "\n",
        "        if len(below_threshold) > 0:\n",
        "            for below_item in range(len(below_threshold)):\n",
        "                loss_neuron.append(-K.mean(\n",
        "                    model.get_layer(below_threshold[below_item][0]).output[..., below_threshold[below_item][1]]))\n",
        "\n",
        "        # loss_neuron += loss3_neuron_below - loss3_neuron_above\n",
        "\n",
        "        # for (layer_name, index), value in model_layer_value.items():\n",
        "        #     if 0.5 > value > 0.25:\n",
        "        #         above_threshold.append([layer_name, index])\n",
        "        #     elif 0.25 > value > 0.2:\n",
        "        #         below_threshold.append([layer_name, index])\n",
        "        # loss3_neuron_above = 0\n",
        "        # loss3_neuron_below = 0\n",
        "        # if len(above_threshold)>0:\n",
        "        #     above_i = np.random.choice(range(len(above_threshold)))\n",
        "        #     loss3_neuron_above = K.mean(model.get_layer(above_threshold[above_i][0]).output[..., above_threshold[above_i][1]])\n",
        "        # if len(below_threshold)>0:\n",
        "        #     below_i = np.random.choice(range(len(below_threshold)))\n",
        "        #     loss3_neuron_below = K.mean(model.get_layer(below_threshold[below_i][0]).output[..., below_threshold[below_i][1]])\n",
        "        # loss_neuron += loss3_neuron_below - loss3_neuron_above\n",
        "        if loss_neuron == 0:\n",
        "            return random_strategy(model, model_layer_times, 1)  # The beginning of no neurons covered\n",
        "\n",
        "    return loss_neuron\n",
        "\n",
        "def neuron_scale(loss_neuron):\n",
        "    loss_neuron_new = []\n",
        "    loss_sum = K.sum(loss_neuron)\n",
        "    for loss_each in loss_neuron:\n",
        "        loss_each /= loss_sum\n",
        "        loss_neuron_new.append(loss_each)\n",
        "    return loss_neuron_new\n",
        "\n",
        "def neuron_scale_maxmin(loss_neuron):\n",
        "    max_loss = K.max(loss_neuron)\n",
        "    min_loss = K.min(loss_neuron)\n",
        "    base = max_loss - min_loss\n",
        "    loss_neuron_new = []\n",
        "    for loss_each in loss_neuron:\n",
        "        loss_each_new = (loss_each - min_loss) / base\n",
        "        loss_neuron_new.append(loss_each_new)\n",
        "    return loss_neuron_new\n",
        "\n",
        "def neuron_covered(model_layer_times):\n",
        "    covered_neurons = len([v for v in model_layer_times.values() if v > 0])\n",
        "    total_neurons = len(model_layer_times)\n",
        "    return covered_neurons, total_neurons, covered_neurons / float(total_neurons)\n",
        "\n",
        "\n",
        "def scale(intermediate_layer_output, rmax=1, rmin=0):\n",
        "    X_std = (intermediate_layer_output - intermediate_layer_output.min()) / (\n",
        "        intermediate_layer_output.max() - intermediate_layer_output.min())\n",
        "    X_scaled = X_std * (rmax - rmin) + rmin\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "def update_coverage(input_data, model, model_layer_times, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        # xrange(scaled.shape[-1])\n",
        "        #for num_neuron in xrange(scaled.shape[-1]):\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold: #and model_layer_dict[(layer_names[i], num_neuron)] == 0:\n",
        "                model_layer_times[(layer_names[i], num_neuron)] += 1\n",
        "\n",
        "    return intermediate_layer_outputs\n",
        "\n",
        "def update_coverage_value(input_data, model, model_layer_value):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        # xrange(scaled.shape[-1])\n",
        "        #for num_neuron in xrange(scaled.shape[-1]):\n",
        "        for num_neuron in range(scaled.shape[-1]):\n",
        "            model_layer_value[(layer_names[i], num_neuron)] = np.mean(scaled[..., num_neuron])\n",
        "\n",
        "    return intermediate_layer_outputs\n",
        "\n",
        "'''\n",
        "def update_coverage(input_data, model, model_layer_dict, threshold=0):\n",
        "    layer_names = [layer.name for layer in model.layers if\n",
        "                   'flatten' not in layer.name and 'input' not in layer.name]\n",
        "\n",
        "    intermediate_layer_model = Model(inputs=model.input,\n",
        "                                     outputs=[model.get_layer(layer_name).output for layer_name in layer_names])\n",
        "    \n",
        "    intermediate_layer_outputs = intermediate_layer_model.predict(input_data)\n",
        "\n",
        "    for i, intermediate_layer_output in enumerate(intermediate_layer_outputs):\n",
        "        scaled = scale(intermediate_layer_output[0])\n",
        "        # xrange(scaled.shape[-1])\n",
        "        for num_neuron in xrange(scaled.shape[-1]):\n",
        "            if np.mean(scaled[..., num_neuron]) > threshold and not model_layer_dict[(layer_names[i], num_neuron)]:\n",
        "                model_layer_dict[(layer_names[i], num_neuron)] = True\n",
        "\n",
        "    return intermediate_layer_outputs\n",
        "'''\n",
        "\n",
        "def full_coverage(model_layer_dict):\n",
        "    if False in model_layer_dict.values():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def fired(model, layer_name, index, input_data, threshold=0):\n",
        "    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
        "    intermediate_layer_output = intermediate_layer_model.predict(input_data)[0]\n",
        "    scaled = scale(intermediate_layer_output)\n",
        "    if np.mean(scaled[..., index]) > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def diverged(predictions1, predictions2, predictions3, target):\n",
        "    #     if predictions2 == predictions3 == target and predictions1 != target:\n",
        "    if not predictions1 == predictions2 == predictions3:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def get_signature():\n",
        "    now = datetime.now()\n",
        "    past = datetime(2015, 6, 6, 0, 0, 0, 0)\n",
        "    timespan = now - past\n",
        "    time_sig = int(timespan.total_seconds() * 1000)\n",
        "\n",
        "    return str(time_sig)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wq5TQLYZ98h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6c69a5f-0785-4e7d-cdae-635269142014"
      },
      "source": [
        "neuron_select_strategy"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3Q7ba5DQXpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#num_strategy = len([x for x in neuron_select_strategy if x in ['0', '1', '2', '3']])\n",
        "num_strategy = 2\n",
        "neuron_to_cover_num_each = neuron_to_cover_num / num_strategy"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rkYukaERoQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neuron_select_strategy = [0]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmmCD-uCajaT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56fc8cef-569e-4b20-82ef-d9288746eab5"
      },
      "source": [
        "type(neuron_select_strategy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpsy6fA8bWt9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6d1e648-50ad-4b07-df6d-c7419773dbf8"
      },
      "source": [
        "neuron_to_cover_num"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhMWviIUQa2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a85fad12-b79d-442b-a302-338be8009962"
      },
      "source": [
        "num_strategy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrqzeFkBdP6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73304348-9b77-44ea-8a54-742877784d72"
      },
      "source": [
        "params[0][0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Di3Y0VIGmsB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "248d60e2-1381-4d65-ee50-b145750e721d"
      },
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/DLFuzz/DLFuzz/MNIST"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/DLFuzz/DLFuzz/MNIST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LofVVFzCEjRa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95b6e599-c932-4901-9211-68a7f19c789c"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from __future__ import print_function\n",
        "from keras.layers import Input\n",
        "#https://intellectual-curiosity.tokyo/2019/06/22/quiver_engine%E3%81%A7%E3%80%8Cimporterror-cannot-import-name-imsave-from-scipy-misc%E3%80%8D%E3%81%AE%E5%AF%BE%E5%BF%9C/\n",
        "#from scipy.misc import imsave\n",
        "import imageio\n",
        "#from utils_tmp import *\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "\n",
        "from Model1 import Model1\n",
        "from Model2 import Model2\n",
        "from Model3 import Model3\n",
        "\n",
        "params = [[2], 0.5, 5, \"20200607\", 5, \"model1\"]\n",
        "\n",
        "def load_data(path=\"../MNIST_data/mnist.npz\"):\n",
        "    f = np.load(path)\n",
        "    x_train, y_train = f['x_train'], f['y_train']\n",
        "    x_test, y_test = f['x_test'], f['y_test']\n",
        "    f.close()\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "# define input tensor as a placeholder\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "# load multiple models sharing same input tensor\n",
        "K.set_learning_phase(0)\n",
        "\n",
        "#https://yukun.info/python-command-line-arguments/\n",
        "#model_name = sys.argv[6]\n",
        "model_name = params[5]\n",
        "\n",
        "if model_name == 'model1':\n",
        "    model1 = Model1(input_tensor=input_tensor)\n",
        "elif model_name == 'model2':\n",
        "    model1 = Model2(input_tensor=input_tensor)\n",
        "elif model_name == 'model3':\n",
        "    model1 = Model3(input_tensor=input_tensor)\n",
        "else:\n",
        "    print('please specify model name')\n",
        "    os._exit(0)\n",
        "\n",
        "print(model1.name)\n",
        "\n",
        "# model_layer_dict1 = init_coverage_tables(model1)\n",
        "model_layer_times1 = init_coverage_times(model1)  # times of each neuron covered\n",
        "model_layer_times2 = init_coverage_times(model1)  # update when new image and adversarial images found\n",
        "model_layer_value1 = init_coverage_value(model1)\n",
        "# start gen inputs\n",
        "# img_paths = image.list_pictures('../seeds_20', ext='JPEG')\n",
        "\n",
        "img_dir = './seeds_50'\n",
        "img_paths = os.listdir(img_dir)\n",
        "img_num = len(img_paths)\n",
        "\n",
        "# e.g.[0,1,2] None for neurons not covered, 0 for covered often, 1 for covered rarely, 2 for high weights\n",
        "#neuron_select_strategy = sys.argv[1]\n",
        "#threshold = float(sys.argv[2])\n",
        "#neuron_to_cover_num = int(sys.argv[3])\n",
        "#subdir = sys.argv[4]\n",
        "#iteration_times = int(sys.argv[5])\n",
        "neuron_select_strategy = params[0]\n",
        "threshold = float(params[1])\n",
        "neuron_to_cover_num = int(params[2])\n",
        "subdir = params[3]\n",
        "iteration_times = int(params[4])\n",
        "\n",
        "\n",
        "neuron_to_cover_weight = 0.5\n",
        "predict_weight = 0.5\n",
        "learning_step = 0.02\n",
        "\n",
        "save_dir = '/content/drive/My Drive/Colab Notebooks/DLFuzz/DLFuzz/MNIST/generated_inputs/' + subdir + '/'\n",
        "\n",
        "if os.path.exists(save_dir):\n",
        "    for i in os.listdir(save_dir):\n",
        "        path_file = os.path.join(save_dir, i)\n",
        "        if os.path.isfile(path_file):\n",
        "            os.remove(path_file)\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# start = time.clock()\n",
        "total_time = 0\n",
        "total_norm = 0\n",
        "adversial_num = 0\n",
        "\n",
        "total_perturb_adversial = 0\n",
        "\n",
        "#https://www.sejuku.net/blog/69291\n",
        "#python3系ではxrange不可。rangeのみ\n",
        "for i in range(img_num):\n",
        "\n",
        "    start_time = time.clock()\n",
        "\n",
        "    img_list = []\n",
        "\n",
        "    img_path = os.path.join(img_dir,img_paths[i])\n",
        "\n",
        "    img_name = img_paths[i].split('.')[0]\n",
        "\n",
        "    mannual_label = int(img_name.split('_')[1])\n",
        "\n",
        "    print(img_path)\n",
        "\n",
        "    tmp_img = preprocess_image(img_path)\n",
        "\n",
        "    orig_img = tmp_img.copy()\n",
        "\n",
        "    img_list.append(tmp_img)\n",
        "\n",
        "    update_coverage(tmp_img, model1, model_layer_times2, threshold)\n",
        "\n",
        "    while len(img_list) > 0:\n",
        "\n",
        "        gen_img = img_list[0]\n",
        "\n",
        "        img_list.remove(gen_img)\n",
        "\n",
        "        # first check if input already induces differences\n",
        "        pred1 = model1.predict(gen_img)\n",
        "        label1 = np.argmax(pred1[0])\n",
        "\n",
        "        label_top5 = np.argsort(pred1[0])[-5:]\n",
        "\n",
        "        update_coverage_value(gen_img, model1, model_layer_value1)\n",
        "        update_coverage(gen_img, model1, model_layer_times1, threshold)\n",
        "\n",
        "        orig_label = label1\n",
        "        orig_pred = pred1\n",
        "\n",
        "        loss_1 = K.mean(model1.get_layer('before_softmax').output[..., orig_label])\n",
        "        loss_2 = K.mean(model1.get_layer('before_softmax').output[..., label_top5[-2]])\n",
        "        loss_3 = K.mean(model1.get_layer('before_softmax').output[..., label_top5[-3]])\n",
        "        loss_4 = K.mean(model1.get_layer('before_softmax').output[..., label_top5[-4]])\n",
        "        loss_5 = K.mean(model1.get_layer('before_softmax').output[..., label_top5[-5]])\n",
        "\n",
        "        layer_output = (predict_weight * (loss_2 + loss_3 + loss_4 + loss_5) - loss_1)\n",
        "\n",
        "        # neuron coverage loss\n",
        "        loss_neuron = neuron_selection(model1, model_layer_times1, model_layer_value1, neuron_select_strategy,\n",
        "                                       neuron_to_cover_num, threshold)\n",
        "        # loss_neuron = neuron_scale(loss_neuron) # useless, and negative result\n",
        "\n",
        "        # extreme value means the activation value for a neuron can be as high as possible ...\n",
        "        EXTREME_VALUE = False\n",
        "        if EXTREME_VALUE:\n",
        "            neuron_to_cover_weight = 2\n",
        "\n",
        "        layer_output += neuron_to_cover_weight * K.sum(loss_neuron)\n",
        "\n",
        "        # for adversarial image generation\n",
        "        final_loss = K.mean(layer_output)\n",
        "\n",
        "        # we compute the gradient of the input picture wrt this loss\n",
        "        grads = normalize(K.gradients(final_loss, input_tensor)[0])\n",
        "\n",
        "        grads_tensor_list = [loss_1, loss_2, loss_3, loss_4, loss_5]\n",
        "        grads_tensor_list.extend(loss_neuron)\n",
        "        grads_tensor_list.append(grads)\n",
        "        # this function returns the loss and grads given the input picture\n",
        "\n",
        "        iterate = K.function([input_tensor], grads_tensor_list)\n",
        "\n",
        "        # we run gradient ascent for 3 steps\n",
        "        #for iters in xrange(iteration_times):\n",
        "        for iters in range(iteration_times):\n",
        "\n",
        "            loss_neuron_list = iterate([gen_img])\n",
        "\n",
        "            perturb = loss_neuron_list[-1] * learning_step\n",
        "\n",
        "            gen_img += perturb\n",
        "\n",
        "            # previous accumulated neuron coverage\n",
        "            previous_coverage = neuron_covered(model_layer_times1)[2]\n",
        "\n",
        "            pred1 = model1.predict(gen_img)\n",
        "            label1 = np.argmax(pred1[0])\n",
        "\n",
        "            update_coverage(gen_img, model1, model_layer_times1, threshold) # for seed selection\n",
        "\n",
        "            current_coverage = neuron_covered(model_layer_times1)[2]\n",
        "\n",
        "            diff_img = gen_img - orig_img\n",
        "\n",
        "            L2_norm = np.linalg.norm(diff_img)\n",
        "\n",
        "            orig_L2_norm = np.linalg.norm(orig_img)\n",
        "\n",
        "            perturb_adversial = L2_norm / orig_L2_norm\n",
        "\n",
        "            if current_coverage - previous_coverage > 0.01 / (i + 1) and perturb_adversial < 0.02:\n",
        "                img_list.append(gen_img)\n",
        "                # print('coverage diff = ', current_coverage - previous_coverage, 'perturb_adversial = ', perturb_adversial)\n",
        "\n",
        "            if label1 != orig_label:\n",
        "                update_coverage(gen_img, model1, model_layer_times2, threshold)\n",
        "\n",
        "                total_norm += L2_norm\n",
        "\n",
        "                total_perturb_adversial += perturb_adversial\n",
        "\n",
        "                # print('L2 norm : ' + str(L2_norm))\n",
        "                # print('ratio perturb = ', perturb_adversial)\n",
        "\n",
        "                gen_img_tmp = gen_img.copy()\n",
        "\n",
        "                gen_img_deprocessed = deprocess_image(gen_img_tmp)\n",
        "\n",
        "                save_img = save_dir + img_name + '_' + str(get_signature()) + '.png'\n",
        "\n",
        "                #imsave(save_img, gen_img_deprocessed)\n",
        "                imageio.imwrite(save_img, gen_img_deprocessed)\n",
        "\n",
        "                adversial_num += 1\n",
        "\n",
        "    end_time = time.clock()\n",
        "\n",
        "    print('covered neurons percentage %d neurons %.3f'\n",
        "          % (len(model_layer_times2), neuron_covered(model_layer_times2)[2]))\n",
        "\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    print('used time : ' + str(duration))\n",
        "\n",
        "    total_time += duration\n",
        "\n",
        "print('covered neurons percentage %d neurons %.3f'\n",
        "      % (len(model_layer_times2), neuron_covered(model_layer_times2)[2]))\n",
        "\n",
        "print('total_time = ' + str(total_time))\n",
        "print('average_norm = ' + str(total_norm / adversial_num))\n",
        "print('adversial num = ' + str(adversial_num))\n",
        "print('average perb adversial = ' + str(total_perturb_adversial / adversial_num))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model1 loaded\n",
            "model_5\n",
            "./seeds_50/10452_6.png\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
            "  warnings.warn('grayscale is deprecated. Please use '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "covered neurons percentage 52 neurons 0.154\n",
            "used time : 0.6240109999999994\n",
            "./seeds_50/14189_8.png\n",
            "covered neurons percentage 52 neurons 0.250\n",
            "used time : 0.6967540000000003\n",
            "./seeds_50/14245_7.png\n",
            "covered neurons percentage 52 neurons 0.308\n",
            "used time : 0.700372999999999\n",
            "./seeds_50/15217_2.png\n",
            "covered neurons percentage 52 neurons 0.308\n",
            "used time : 0.5453729999999997\n",
            "./seeds_50/15463_6.png\n",
            "covered neurons percentage 52 neurons 0.308\n",
            "used time : 0.6422840000000001\n",
            "./seeds_50/1657_8.png\n",
            "covered neurons percentage 52 neurons 0.308\n",
            "used time : 0.6795519999999993\n",
            "./seeds_50/18529_9.png\n",
            "covered neurons percentage 52 neurons 0.327\n",
            "used time : 0.6270340000000001\n",
            "./seeds_50/20608_0.png\n",
            "covered neurons percentage 52 neurons 0.365\n",
            "used time : 0.6056000000000008\n",
            "./seeds_50/20663_6.png\n",
            "covered neurons percentage 52 neurons 0.365\n",
            "used time : 0.7406410000000001\n",
            "./seeds_50/206_0.png\n",
            "covered neurons percentage 52 neurons 0.365\n",
            "used time : 0.7391709999999989\n",
            "./seeds_50/21535_1.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.6767250000000011\n",
            "./seeds_50/21638_2.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.6873039999999992\n",
            "./seeds_50/22386_2.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.6580709999999996\n",
            "./seeds_50/22880_7.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.7232300000000009\n",
            "./seeds_50/25128_7.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.6720459999999981\n",
            "./seeds_50/25755_0.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.6692210000000003\n",
            "./seeds_50/25970_1.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.6761139999999983\n",
            "./seeds_50/26619_0.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.5995290000000004\n",
            "./seeds_50/26802_1.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.720479000000001\n",
            "./seeds_50/27700_2.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.6795399999999994\n",
            "./seeds_50/30545_8.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.7959320000000005\n",
            "./seeds_50/30858_5.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.7035170000000015\n",
            "./seeds_50/3194_4.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.7482490000000013\n",
            "./seeds_50/32023_5.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.6896590000000025\n",
            "./seeds_50/32063_1.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.9034390000000023\n",
            "./seeds_50/32745_9.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.7419229999999999\n",
            "./seeds_50/34828_3.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.801518999999999\n",
            "./seeds_50/35254_4.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.6964860000000002\n",
            "./seeds_50/35604_8.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.8343179999999997\n",
            "./seeds_50/35867_5.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.7317089999999986\n",
            "./seeds_50/40469_9.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.7922830000000012\n",
            "./seeds_50/42070_3.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.8715860000000006\n",
            "./seeds_50/43279_7.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.7553409999999978\n",
            "./seeds_50/4363_7.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.7974609999999984\n",
            "./seeds_50/4413_5.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.7723719999999972\n",
            "./seeds_50/4520_6.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.8812810000000013\n",
            "./seeds_50/47707_5.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.7641069999999957\n",
            "./seeds_50/4998_1.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.8546509999999969\n",
            "./seeds_50/50927_9.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 1.072657999999997\n",
            "./seeds_50/52134_3.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.8658889999999957\n",
            "./seeds_50/53167_3.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.8288440000000037\n",
            "./seeds_50/54713_9.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.8965480000000028\n",
            "./seeds_50/54758_2.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.9251220000000018\n",
            "./seeds_50/57026_4.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.8869120000000024\n",
            "./seeds_50/58144_3.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.8351600000000019\n",
            "./seeds_50/59000_6.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.918130000000005\n",
            "./seeds_50/59013_8.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.9346450000000033\n",
            "./seeds_50/59599_4.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.9319419999999994\n",
            "./seeds_50/729_4.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.9548950000000005\n",
            "./seeds_50/8531_0.png\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "used time : 0.8547560000000018\n",
            "covered neurons percentage 52 neurons 0.385\n",
            "total_time = 38.404386\n",
            "average_norm = 2.0665516491119678\n",
            "adversial num = 130\n",
            "average perb adversial = 0.239876777277543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzMs2JsbI5Y-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "7c03472d-ef6c-4013-8c0c-8f63a51979b8"
      },
      "source": [
        "sys.argv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py',\n",
              " '-f',\n",
              " '/root/.local/share/jupyter/runtime/kernel-344751ca-83a5-4502-b386-43196df7da05.json']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y24-QOxdJUts",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "c94288e8-0987-4b10-9702-3b23f2a387bb"
      },
      "source": [
        "!python gen_diff.py [2] 0.5 5 0602 5 model1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2020-06-07 00:20:59.395363: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Traceback (most recent call last):\n",
            "  File \"gen_diff.py\", line 6, in <module>\n",
            "    from scipy.misc import imsave\n",
            "ImportError: cannot import name 'imsave'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5tqqybUJsHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = [[2], 0.5, 5, 602, 5, \"model1\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKYx33ykMLVT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "640b4b83-bee4-4f3a-f4a3-380584e96efc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgoZOYOVMZly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}